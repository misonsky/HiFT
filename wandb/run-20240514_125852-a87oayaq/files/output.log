  0%|          | 0/3218 [00:00<?, ?it/s]/mounts/work/lyk/anaconda3/envs/torch2/lib/python3.8/site-packages/deepspeed/runtime/utils.py:274: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/conda/conda-bld/pytorch_1699449229234/work/torch/csrc/tensor/python_tensor.cpp:83.)
  overflow_gpu = get_accelerator().ByteTensor([overflow])
  0%|          | 1/3218 [00:01<1:17:56,  1.45s/it]
[2024-05-14 12:59:00,499] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 0
[2024-05-14 12:59:00,499] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2024-05-14 12:59:00,500] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
{'loss': 0.0, 'learning_rate': 0.0, 'epoch': 0.0}
[2024-05-14 12:59:01,877] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1
[2024-05-14 12:59:01,877] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0

  0%|          | 3/3218 [00:03<56:40,  1.06s/it]
[2024-05-14 12:59:02,599] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 2
[2024-05-14 12:59:02,600] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2024-05-14 12:59:02,600] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
[2024-05-14 12:59:03,609] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 3
[2024-05-14 12:59:03,610] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 8192.0 to 4096.0

  0%|          | 4/3218 [00:04<57:32,  1.07s/it]
[2024-05-14 12:59:05,159] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 4
[2024-05-14 12:59:05,159] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 4096.0 to 2048.0

  0%|          | 5/3218 [00:06<1:06:48,  1.25s/it]
[2024-05-14 12:59:06,912] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 5
[2024-05-14 12:59:06,912] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 2048.0 to 1024.0

  0%|          | 7/3218 [00:09<1:13:37,  1.38s/it]
[2024-05-14 12:59:08,197] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 6
[2024-05-14 12:59:08,197] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 1024.0 to 512.0

  0%|          | 8/3218 [00:11<1:24:07,  1.57s/it]
[2024-05-14 12:59:10,174] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 7
[2024-05-14 12:59:10,174] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 512.0 to 256.0
[2024-05-14 12:59:10,180] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 512.0, reducing to 256.0
[2024-05-14 12:59:11,501] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 8
[2024-05-14 12:59:11,502] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 256.0 to 128.0

  0%|          | 10/3218 [00:13<1:16:57,  1.44s/it]
[2024-05-14 12:59:12,831] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 9
[2024-05-14 12:59:12,831] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 128.0 to 64.0
[2024-05-14 12:59:12,832] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 128.0, reducing to 64.0

  0%|          | 11/3218 [00:15<1:21:00,  1.52s/it]
[2024-05-14 12:59:14,494] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 10
[2024-05-14 12:59:14,494] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 64.0 to 32.0
[2024-05-14 12:59:14,494] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 64.0, reducing to 32.0
[2024-05-14 12:59:15,789] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 11
[2024-05-14 12:59:15,789] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 32.0 to 16.0

  0%|          | 12/3218 [00:16<1:17:07,  1.44s/it]
[2024-05-14 12:59:17,460] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 12
[2024-05-14 12:59:17,460] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 16.0 to 8.0

  0%|          | 14/3218 [00:19<1:21:08,  1.52s/it]
[2024-05-14 12:59:19,004] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 13
[2024-05-14 12:59:19,004] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 8.0 to 4.0

  0%|          | 15/3218 [00:21<1:21:57,  1.54s/it]
[2024-05-14 12:59:20,576] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 14
[2024-05-14 12:59:20,577] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 4.0 to 2.0
[2024-05-14 12:59:20,577] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 4.0, reducing to 2.0
[2024-05-14 12:59:21,880] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 15
[2024-05-14 12:59:21,880] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 2.0 to 1.0

  1%|          | 17/3218 [00:24<1:13:57,  1.39s/it]
[2024-05-14 12:59:23,083] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 16
[2024-05-14 12:59:23,083] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 1.0 to 0.5

  1%|          | 18/3218 [00:25<1:09:19,  1.30s/it]
[2024-05-14 12:59:24,176] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 17
[2024-05-14 12:59:24,176] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.5 to 0.25
[2024-05-14 12:59:24,177] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.5, reducing to 0.25
[2024-05-14 12:59:25,395] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 18
[2024-05-14 12:59:25,396] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  1%|          | 20/3218 [00:27<1:02:20,  1.17s/it]
[2024-05-14 12:59:26,323] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 19
[2024-05-14 12:59:26,324] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:59:26,324] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
{'loss': 0.0, 'learning_rate': 0.0, 'epoch': 0.01}
[2024-05-14 12:59:27,682] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 20
[2024-05-14 12:59:27,682] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  1%|          | 22/3218 [00:29<1:04:25,  1.21s/it]
[2024-05-14 12:59:28,847] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 21
[2024-05-14 12:59:28,847] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  1%|          | 23/3218 [00:31<1:05:43,  1.23s/it]
[2024-05-14 12:59:30,128] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 22
[2024-05-14 12:59:30,129] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:59:30,130] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 12:59:31,508] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 23
[2024-05-14 12:59:31,509] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  1%|          | 25/3218 [00:33<1:09:13,  1.30s/it]
[2024-05-14 12:59:32,858] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 24
[2024-05-14 12:59:32,859] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:59:32,859] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 12:59:33,959] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 25
[2024-05-14 12:59:33,960] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  1%|          | 27/3218 [00:36<1:05:25,  1.23s/it]
[2024-05-14 12:59:35,167] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 26
[2024-05-14 12:59:35,168] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  1%|          | 28/3218 [00:37<1:07:08,  1.26s/it]
[2024-05-14 12:59:36,509] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 27
[2024-05-14 12:59:36,510] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:59:36,510] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 12:59:37,775] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 28
[2024-05-14 12:59:37,776] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  1%|          | 30/3218 [00:39<1:05:29,  1.23s/it]
[2024-05-14 12:59:38,942] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 29
[2024-05-14 12:59:38,943] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:59:38,943] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
{'loss': 0.0, 'learning_rate': 0.0, 'epoch': 0.01}
[2024-05-14 12:59:40,084] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 30
[2024-05-14 12:59:40,085] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  1%|          | 32/3218 [00:42<1:01:31,  1.16s/it]
[2024-05-14 12:59:41,132] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 31
[2024-05-14 12:59:41,133] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:59:41,133] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 12:59:42,074] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 32
[2024-05-14 12:59:42,075] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  1%|          | 34/3218 [00:44<55:54,  1.05s/it]
[2024-05-14 12:59:43,016] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 33
[2024-05-14 12:59:43,017] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:59:43,017] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 12:59:43,939] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 34
[2024-05-14 12:59:43,939] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  1%|          | 35/3218 [00:44<52:55,  1.00it/s]
[2024-05-14 12:59:45,975] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 35
[2024-05-14 12:59:45,976] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  1%|          | 37/3218 [00:48<1:10:36,  1.33s/it]
[2024-05-14 12:59:47,318] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 36
[2024-05-14 12:59:47,318] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  1%|          | 38/3218 [00:49<1:02:29,  1.18s/it]
[2024-05-14 12:59:48,262] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 37
[2024-05-14 12:59:48,262] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:59:48,262] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 12:59:49,705] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 38
[2024-05-14 12:59:49,705] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  1%|          | 40/3218 [00:52<1:14:23,  1.40s/it]
[2024-05-14 12:59:51,288] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 39
[2024-05-14 12:59:51,288] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:59:51,289] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25

  1%|▏         | 41/3218 [00:53<1:14:58,  1.42s/it]
[2024-05-14 12:59:52,752] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 40
[2024-05-14 12:59:52,752] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  1%|▏         | 42/3218 [00:55<1:23:32,  1.58s/it]
[2024-05-14 12:59:54,673] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 41
[2024-05-14 12:59:54,673] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  1%|▏         | 43/3218 [00:58<1:35:50,  1.81s/it]
[2024-05-14 12:59:57,031] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 42
[2024-05-14 12:59:57,032] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  1%|▏         | 44/3218 [00:59<1:34:30,  1.79s/it]
[2024-05-14 12:59:58,791] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 43
[2024-05-14 12:59:58,791] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:59:58,792] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25

  1%|▏         | 45/3218 [01:01<1:27:11,  1.65s/it]
[2024-05-14 13:00:00,145] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:00:00,145] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 13:00:01,981] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 45
[2024-05-14 13:00:01,981] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  1%|▏         | 46/3218 [01:03<1:31:06,  1.72s/it]
[2024-05-14 13:00:03,571] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 46
[2024-05-14 13:00:03,572] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  1%|▏         | 48/3218 [01:05<1:23:17,  1.58s/it]
[2024-05-14 13:00:04,917] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 47
[2024-05-14 13:00:04,917] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:00:04,918] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 13:00:05,988] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 48
[2024-05-14 13:00:05,989] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  2%|▏         | 49/3218 [01:07<1:14:56,  1.42s/it]
[2024-05-14 13:00:07,456] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 49
[2024-05-14 13:00:07,456] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:00:07,456] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25

  2%|▏         | 51/3218 [01:09<1:14:32,  1.41s/it]
[2024-05-14 13:00:08,803] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 50
[2024-05-14 13:00:08,803] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:00:08,803] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 13:00:10,113] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 51
[2024-05-14 13:00:10,114] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  2%|▏         | 52/3218 [01:11<1:12:28,  1.37s/it]
[2024-05-14 13:00:11,399] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 52
[2024-05-14 13:00:11,400] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  2%|▏         | 54/3218 [01:13<1:12:50,  1.38s/it]
[2024-05-14 13:00:12,821] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 53
[2024-05-14 13:00:12,822] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  2%|▏         | 56/3218 [01:16<1:07:59,  1.29s/it]
[2024-05-14 13:00:14,274] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 54
[2024-05-14 13:00:14,275] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:00:14,275] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 13:00:15,332] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 55
[2024-05-14 13:00:15,333] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  2%|▏         | 57/3218 [01:17<1:06:36,  1.26s/it]
[2024-05-14 13:00:16,553] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 56
[2024-05-14 13:00:16,554] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:00:16,554] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 13:00:17,756] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 57
[2024-05-14 13:00:17,757] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  2%|▏         | 59/3218 [01:20<1:09:33,  1.32s/it]
[2024-05-14 13:00:19,243] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 58
[2024-05-14 13:00:19,243] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  2%|▏         | 60/3218 [01:21<1:05:41,  1.25s/it]
[2024-05-14 13:00:20,322] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 59
[2024-05-14 13:00:20,323] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:00:20,323] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
{'loss': 0.0, 'learning_rate': 0.0, 'epoch': 0.02}
[2024-05-14 13:00:21,476] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 60
[2024-05-14 13:00:21,477] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  2%|▏         | 62/3218 [01:23<1:04:24,  1.22s/it]
[2024-05-14 13:00:22,696] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 61
[2024-05-14 13:00:22,697] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:00:22,697] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 13:00:23,984] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 62
[2024-05-14 13:00:23,984] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  2%|▏         | 64/3218 [01:26<1:05:06,  1.24s/it]
[2024-05-14 13:00:25,226] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 63
[2024-05-14 13:00:25,226] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  2%|▏         | 65/3218 [01:27<1:03:25,  1.21s/it]
[2024-05-14 13:00:26,353] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 64
[2024-05-14 13:00:26,353] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:00:26,353] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 13:00:27,612] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 65
[2024-05-14 13:00:27,612] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  2%|▏         | 67/3218 [01:29<1:01:37,  1.17s/it]
[2024-05-14 13:00:28,689] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 66
[2024-05-14 13:00:28,690] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:00:28,690] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 13:00:29,907] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 67
[2024-05-14 13:00:29,908] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  2%|▏         | 69/3218 [01:31<58:58,  1.12s/it]
[2024-05-14 13:00:30,893] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 68
[2024-05-14 13:00:30,893] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:00:30,893] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 13:00:31,994] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 69
[2024-05-14 13:00:31,995] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:00:31,995] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25

  2%|▏         | 70/3218 [01:33<58:09,  1.11s/it]
[2024-05-14 13:00:33,841] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 70
[2024-05-14 13:00:33,841] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  2%|▏         | 71/3218 [01:34<1:10:25,  1.34s/it]
[2024-05-14 13:00:35,660] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 71
[2024-05-14 13:00:35,661] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  2%|▏         | 73/3218 [01:37<1:03:39,  1.21s/it]
[2024-05-14 13:00:36,413] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 72
[2024-05-14 13:00:36,413] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:00:36,413] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 13:00:37,551] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 73
[2024-05-14 13:00:37,552] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  2%|▏         | 75/3218 [01:40<1:13:28,  1.40s/it]
[2024-05-14 13:00:39,343] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 74
[2024-05-14 13:00:39,344] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  2%|▏         | 76/3218 [01:42<1:20:17,  1.53s/it]
[2024-05-14 13:00:41,111] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 75
[2024-05-14 13:00:41,111] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  2%|▏         | 77/3218 [01:43<1:21:06,  1.55s/it]
[2024-05-14 13:00:42,767] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 76
[2024-05-14 13:00:42,768] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  2%|▏         | 78/3218 [01:45<1:20:10,  1.53s/it]
[2024-05-14 13:00:44,284] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 77
[2024-05-14 13:00:44,284] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:00:44,285] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 13:00:46,265] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 78
[2024-05-14 13:00:46,266] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  2%|▏         | 79/3218 [01:47<1:27:50,  1.68s/it]
[2024-05-14 13:00:47,808] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 79
[2024-05-14 13:00:47,809] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:00:47,809] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25

  3%|▎         | 81/3218 [01:50<1:24:50,  1.62s/it]
[2024-05-14 13:00:49,425] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 80
[2024-05-14 13:00:49,425] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  3%|▎         | 82/3218 [01:52<1:25:28,  1.64s/it]
[2024-05-14 13:00:51,038] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 81
[2024-05-14 13:00:51,038] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  3%|▎         | 83/3218 [01:53<1:23:54,  1.61s/it]
[2024-05-14 13:00:52,597] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 82
[2024-05-14 13:00:52,597] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:00:52,597] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 13:00:53,952] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 83
[2024-05-14 13:00:53,952] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  3%|▎         | 85/3218 [01:56<1:18:04,  1.50s/it]
[2024-05-14 13:00:55,382] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 84
[2024-05-14 13:00:55,383] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  3%|▎         | 86/3218 [01:57<1:17:24,  1.48s/it]
[2024-05-14 13:00:56,839] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 85
[2024-05-14 13:00:56,839] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:00:56,840] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 13:00:58,250] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 86
[2024-05-14 13:00:58,250] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  3%|▎         | 87/3218 [01:59<1:17:22,  1.48s/it]
[2024-05-14 13:00:59,552] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 87
[2024-05-14 13:00:59,552] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  3%|▎         | 89/3218 [02:01<1:13:08,  1.40s/it]
[2024-05-14 13:01:00,938] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 88
[2024-05-14 13:01:00,938] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:01:00,938] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 13:01:02,195] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 89
[2024-05-14 13:01:02,195] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  3%|▎         | 90/3218 [02:03<1:11:26,  1.37s/it]
{'loss': 0.0, 'learning_rate': 0.0, 'epoch': 0.03}
[2024-05-14 13:01:03,541] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 90
[2024-05-14 13:01:03,541] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  3%|▎         | 92/3218 [02:06<1:11:57,  1.38s/it]
[2024-05-14 13:01:04,988] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 91
[2024-05-14 13:01:04,988] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:01:04,989] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 13:01:06,310] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 92
[2024-05-14 13:01:06,311] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  3%|▎         | 93/3218 [02:07<1:11:04,  1.36s/it]
[2024-05-14 13:01:07,628] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 93
[2024-05-14 13:01:07,629] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  3%|▎         | 95/3218 [02:09<1:09:02,  1.33s/it]
[2024-05-14 13:01:08,904] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 94
[2024-05-14 13:01:08,905] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  3%|▎         | 96/3218 [02:11<1:12:07,  1.39s/it]
[2024-05-14 13:01:10,396] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 95
[2024-05-14 13:01:10,396] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:01:10,396] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 13:01:11,826] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 96
[2024-05-14 13:01:11,826] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  3%|▎         | 98/3218 [02:14<1:14:34,  1.43s/it]
[2024-05-14 13:01:13,364] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 97
[2024-05-14 13:01:13,365] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  3%|▎         | 99/3218 [02:15<1:14:34,  1.43s/it]
[2024-05-14 13:01:14,802] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 98
[2024-05-14 13:01:14,802] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:01:14,802] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 13:01:16,080] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 99
[2024-05-14 13:01:16,080] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:01:16,080] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
  3%|▎         | 100/3218 [02:17<1:12:23,  1.39s/it][INFO|trainer.py:3166] 2024-05-14 13:01:16,262 >> ***** Running Evaluation *****
[INFO|trainer.py:3168] 2024-05-14 13:01:16,262 >>   Num examples = 521
[INFO|trainer.py:3171] 2024-05-14 13:01:16,263 >>   Batch size = 8






 94%|█████████▍| 31/33 [00:10<00:00,  2.84it/s]
{'eval_loss': nan, 'eval_runtime': 12.3464, 'eval_samples_per_second': 42.198, 'eval_steps_per_second': 2.673, 'epoch': 0.03}
[2024-05-14 13:01:29,782] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 100
[2024-05-14 13:01:29,782] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  3%|▎         | 102/3218 [02:32<3:30:10,  4.05s/it]
[2024-05-14 13:01:31,382] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 101
[2024-05-14 13:01:31,383] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  3%|▎         | 103/3218 [02:33<2:46:54,  3.21s/it]
[2024-05-14 13:01:32,677] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 102
[2024-05-14 13:01:32,678] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:01:32,678] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 13:01:34,226] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 103
[2024-05-14 13:01:34,226] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  3%|▎         | 104/3218 [02:35<2:20:37,  2.71s/it]
[2024-05-14 13:01:35,358] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 104
[2024-05-14 13:01:35,358] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  3%|▎         | 106/3218 [02:37<1:44:14,  2.01s/it]
[2024-05-14 13:01:36,863] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 105
[2024-05-14 13:01:36,863] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:01:36,863] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 13:01:38,121] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 106
[2024-05-14 13:01:38,121] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  3%|▎         | 108/3218 [02:40<1:19:13,  1.53s/it]
[2024-05-14 13:01:39,178] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 107
[2024-05-14 13:01:39,179] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:01:39,179] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 13:01:40,357] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 108
[2024-05-14 13:01:40,357] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  3%|▎         | 109/3218 [02:41<1:16:13,  1.47s/it]
[2024-05-14 13:01:42,206] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 109
[2024-05-14 13:01:42,207] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  3%|▎         | 110/3218 [02:43<1:22:59,  1.60s/it]
{'loss': 0.0, 'learning_rate': 0.0, 'epoch': 0.03}
[2024-05-14 13:01:44,132] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 110
[2024-05-14 13:01:44,133] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  3%|▎         | 111/3218 [02:45<1:27:16,  1.69s/it]
[2024-05-14 13:01:45,982] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 111
[2024-05-14 13:01:45,983] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  3%|▎         | 112/3218 [02:47<1:29:51,  1.74s/it]
[2024-05-14 13:01:47,585] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 112
[2024-05-14 13:01:47,585] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  4%|▎         | 114/3218 [02:50<1:27:23,  1.69s/it]
[2024-05-14 13:01:49,232] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 113
[2024-05-14 13:01:49,232] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  4%|▎         | 115/3218 [02:52<1:28:14,  1.71s/it]
[2024-05-14 13:01:50,984] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 114
[2024-05-14 13:01:50,984] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  4%|▎         | 116/3218 [02:53<1:28:27,  1.71s/it]
[2024-05-14 13:01:52,710] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 115
[2024-05-14 13:01:52,710] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  4%|▎         | 117/3218 [02:55<1:32:32,  1.79s/it]
[2024-05-14 13:01:54,668] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 116
[2024-05-14 13:01:54,669] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  4%|▎         | 118/3218 [02:57<1:33:02,  1.80s/it]
[2024-05-14 13:01:56,503] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 117
[2024-05-14 13:01:56,504] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:01:56,504] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 13:01:58,004] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 118
[2024-05-14 13:01:58,004] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  4%|▎         | 119/3218 [02:59<1:27:53,  1.70s/it]
[2024-05-14 13:01:59,577] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 119
[2024-05-14 13:01:59,578] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:01:59,578] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25

  4%|▍         | 121/3218 [03:02<1:22:38,  1.60s/it]
[2024-05-14 13:02:01,021] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 120
[2024-05-14 13:02:01,021] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  4%|▍         | 122/3218 [03:03<1:21:16,  1.57s/it]
[2024-05-14 13:02:02,526] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 121
[2024-05-14 13:02:02,526] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:02:02,528] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 13:02:04,012] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 122
[2024-05-14 13:02:04,012] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  4%|▍         | 124/3218 [03:06<1:18:00,  1.51s/it]
[2024-05-14 13:02:05,458] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 123
[2024-05-14 13:02:05,459] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  4%|▍         | 125/3218 [03:07<1:13:49,  1.43s/it]
[2024-05-14 13:02:06,727] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 124
[2024-05-14 13:02:06,728] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:02:06,728] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 13:02:08,312] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 125
[2024-05-14 13:02:08,312] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  4%|▍         | 126/3218 [03:09<1:16:22,  1.48s/it]
[2024-05-14 13:02:09,753] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 126
[2024-05-14 13:02:09,753] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  4%|▍         | 128/3218 [03:12<1:14:25,  1.45s/it]
[2024-05-14 13:02:11,127] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 127
[2024-05-14 13:02:11,128] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  4%|▍         | 129/3218 [03:13<1:12:36,  1.41s/it]
[2024-05-14 13:02:12,460] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 128
[2024-05-14 13:02:12,461] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:02:12,461] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 13:02:13,643] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 129
[2024-05-14 13:02:13,643] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:02:13,643] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25

  4%|▍         | 131/3218 [03:16<1:10:53,  1.38s/it]
[2024-05-14 13:02:15,095] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 130
[2024-05-14 13:02:15,095] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  4%|▍         | 132/3218 [03:17<1:12:06,  1.40s/it]
[2024-05-14 13:02:16,550] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 131
[2024-05-14 13:02:16,551] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:02:16,551] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 13:02:17,869] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 132
[2024-05-14 13:02:17,869] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  4%|▍         | 134/3218 [03:20<1:14:24,  1.45s/it]
[2024-05-14 13:02:19,489] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 133
[2024-05-14 13:02:19,490] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  4%|▍         | 135/3218 [03:21<1:14:08,  1.44s/it]
[2024-05-14 13:02:20,924] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 134
[2024-05-14 13:02:20,925] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  4%|▍         | 136/3218 [03:23<1:16:19,  1.49s/it]
[2024-05-14 13:02:22,485] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 135
[2024-05-14 13:02:22,486] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:02:22,486] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 13:02:23,783] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 136
[2024-05-14 13:02:23,783] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  4%|▍         | 138/3218 [03:26<1:10:59,  1.38s/it]
[2024-05-14 13:02:25,041] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 137
[2024-05-14 13:02:25,042] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:02:25,042] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 13:02:26,203] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 138
[2024-05-14 13:02:26,203] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  4%|▍         | 140/3218 [03:28<1:02:15,  1.21s/it]
[2024-05-14 13:02:27,264] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 139
[2024-05-14 13:02:27,264] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:02:27,264] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25

  4%|▍         | 141/3218 [03:29<1:08:59,  1.35s/it]
[2024-05-14 13:02:28,870] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 140
[2024-05-14 13:02:28,870] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  4%|▍         | 143/3218 [03:32<1:03:52,  1.25s/it]
[2024-05-14 13:02:30,638] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 141
[2024-05-14 13:02:30,638] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:02:30,638] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 13:02:31,509] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 142
[2024-05-14 13:02:31,509] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  4%|▍         | 144/3218 [03:34<1:14:38,  1.46s/it]
[2024-05-14 13:02:33,316] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 143
[2024-05-14 13:02:33,316] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  5%|▍         | 145/3218 [03:35<1:17:46,  1.52s/it]
[2024-05-14 13:02:34,935] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 144
[2024-05-14 13:02:34,935] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  5%|▍         | 146/3218 [03:38<1:27:40,  1.71s/it]
[2024-05-14 13:02:37,060] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 145
[2024-05-14 13:02:37,061] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  5%|▍         | 147/3218 [03:39<1:26:10,  1.68s/it]
[2024-05-14 13:02:38,680] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 146
[2024-05-14 13:02:38,681] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:02:38,681] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 13:02:40,425] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 147
[2024-05-14 13:02:40,425] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  5%|▍         | 148/3218 [03:41<1:26:13,  1.69s/it]
[2024-05-14 13:02:41,931] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 148
[2024-05-14 13:02:41,931] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  5%|▍         | 149/3218 [03:42<1:23:18,  1.63s/it]
[2024-05-14 13:02:43,745] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 149
[2024-05-14 13:02:43,746] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:02:43,746] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25

  5%|▍         | 150/3218 [03:44<1:26:18,  1.69s/it]
[2024-05-14 13:02:45,751] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 150
[2024-05-14 13:02:45,752] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  5%|▍         | 151/3218 [03:46<1:31:03,  1.78s/it]
[2024-05-14 13:02:47,604] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 151
[2024-05-14 13:02:47,604] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  5%|▍         | 153/3218 [03:50<1:29:36,  1.75s/it]
[2024-05-14 13:02:49,230] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 152
[2024-05-14 13:02:49,231] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  5%|▍         | 154/3218 [03:52<1:30:42,  1.78s/it]
[2024-05-14 13:02:51,037] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 153
[2024-05-14 13:02:51,038] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  5%|▍         | 155/3218 [03:53<1:31:28,  1.79s/it]
[2024-05-14 13:02:52,831] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 154
[2024-05-14 13:02:52,832] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:02:52,832] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 13:02:54,443] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 155
[2024-05-14 13:02:54,444] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  5%|▍         | 156/3218 [03:55<1:27:14,  1.71s/it]
[2024-05-14 13:02:55,790] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 156
[2024-05-14 13:02:55,791] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  5%|▍         | 158/3218 [03:58<1:23:11,  1.63s/it]
[2024-05-14 13:02:57,445] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 157
[2024-05-14 13:02:57,445] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  5%|▍         | 159/3218 [04:00<1:21:04,  1.59s/it]
[2024-05-14 13:02:58,950] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 158
[2024-05-14 13:02:58,950] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:02:58,952] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 13:03:00,415] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 159
[2024-05-14 13:03:00,416] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  5%|▍         | 160/3218 [04:01<1:19:03,  1.55s/it]
{'loss': 0.0, 'learning_rate': 0.0, 'epoch': 0.05}
[2024-05-14 13:03:01,729] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 160
[2024-05-14 13:03:01,730] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  5%|▌         | 162/3218 [04:04<1:13:33,  1.44s/it]
[2024-05-14 13:03:03,095] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 161
[2024-05-14 13:03:03,096] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  5%|▌         | 163/3218 [04:06<1:22:03,  1.61s/it]
[2024-05-14 13:03:05,109] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 162
[2024-05-14 13:03:05,110] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:03:05,110] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 13:03:06,361] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 163
[2024-05-14 13:03:06,362] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  5%|▌         | 164/3218 [04:07<1:16:32,  1.50s/it]
[2024-05-14 13:03:07,704] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 164
[2024-05-14 13:03:07,705] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  5%|▌         | 166/3218 [04:09<1:10:21,  1.38s/it]
[2024-05-14 13:03:08,917] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 165
[2024-05-14 13:03:08,917] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:03:08,919] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 13:03:10,267] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 166
[2024-05-14 13:03:10,268] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  5%|▌         | 167/3218 [04:11<1:10:00,  1.38s/it]
[2024-05-14 13:03:11,800] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 167
[2024-05-14 13:03:11,801] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  5%|▌         | 169/3218 [04:14<1:13:30,  1.45s/it]
[2024-05-14 13:03:13,307] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 168
[2024-05-14 13:03:13,308] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  5%|▌         | 170/3218 [04:15<1:11:06,  1.40s/it]
[2024-05-14 13:03:14,607] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 169
[2024-05-14 13:03:14,607] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:03:14,607] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
{'loss': 0.0, 'learning_rate': 0.0, 'epoch': 0.05}
[2024-05-14 13:03:16,399] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 170
[2024-05-14 13:03:16,400] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  5%|▌         | 171/3218 [04:17<1:17:03,  1.52s/it]
[2024-05-14 13:03:17,622] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 171
[2024-05-14 13:03:17,622] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  5%|▌         | 173/3218 [04:20<1:15:09,  1.48s/it]
[2024-05-14 13:03:19,187] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 172
[2024-05-14 13:03:19,187] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:03:19,187] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 13:03:20,319] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 173
[2024-05-14 13:03:20,319] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  5%|▌         | 175/3218 [04:22<1:06:02,  1.30s/it]
[2024-05-14 13:03:21,519] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 174
[2024-05-14 13:03:21,519] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  5%|▌         | 176/3218 [04:23<1:08:45,  1.36s/it]
[2024-05-14 13:03:22,988] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 175
[2024-05-14 13:03:22,989] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  6%|▌         | 178/3218 [04:26<1:01:25,  1.21s/it]
[2024-05-14 13:03:24,583] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 176
[2024-05-14 13:03:24,583] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:03:24,583] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 13:03:25,413] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 177
[2024-05-14 13:03:25,413] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:03:25,413] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 13:03:26,444] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 178
[2024-05-14 13:03:26,444] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  6%|▌         | 179/3218 [04:27<1:01:03,  1.21s/it]
[2024-05-14 13:03:28,108] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 179
[2024-05-14 13:03:28,109] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:03:28,109] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25

  6%|▌         | 180/3218 [04:29<1:08:41,  1.36s/it]
[2024-05-14 13:03:29,862] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 180
[2024-05-14 13:03:29,862] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  6%|▌         | 182/3218 [04:32<1:17:01,  1.52s/it]
[2024-05-14 13:03:31,516] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 181
[2024-05-14 13:03:31,516] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:03:31,517] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 13:03:33,874] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 182
[2024-05-14 13:03:33,874] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  6%|▌         | 184/3218 [04:36<1:28:43,  1.75s/it]
[2024-05-14 13:03:35,571] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 183
[2024-05-14 13:03:35,572] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  6%|▌         | 185/3218 [04:38<1:25:10,  1.69s/it]
[2024-05-14 13:03:37,130] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 184
[2024-05-14 13:03:37,131] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  6%|▌         | 186/3218 [04:39<1:26:49,  1.72s/it]
[2024-05-14 13:03:38,902] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 185
[2024-05-14 13:03:38,903] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  6%|▌         | 187/3218 [04:41<1:29:53,  1.78s/it]
[2024-05-14 13:03:40,825] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 186
[2024-05-14 13:03:40,825] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  6%|▌         | 188/3218 [04:43<1:32:54,  1.84s/it]
[2024-05-14 13:03:42,816] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 187
[2024-05-14 13:03:42,816] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:03:42,816] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 13:03:44,240] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 188
[2024-05-14 13:03:44,240] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  6%|▌         | 189/3218 [04:45<1:26:13,  1.71s/it]
[2024-05-14 13:03:45,737] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 189
[2024-05-14 13:03:45,738] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:03:45,738] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25

  6%|▌         | 191/3218 [04:48<1:22:46,  1.64s/it]
[2024-05-14 13:03:47,341] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 190
[2024-05-14 13:03:47,342] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  6%|▌         | 192/3218 [04:49<1:20:15,  1.59s/it]
[2024-05-14 13:03:48,818] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 191
[2024-05-14 13:03:48,818] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:03:48,818] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 13:03:50,538] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 192
[2024-05-14 13:03:50,539] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  6%|▌         | 193/3218 [04:51<1:22:01,  1.63s/it]
[2024-05-14 13:03:51,912] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 193
[2024-05-14 13:03:51,913] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  6%|▌         | 195/3218 [04:54<1:16:13,  1.51s/it]
[2024-05-14 13:03:53,334] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 194
[2024-05-14 13:03:53,335] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:03:53,335] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 13:03:54,587] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 195
[2024-05-14 13:03:54,587] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  6%|▌         | 196/3218 [04:55<1:12:15,  1.43s/it]
[2024-05-14 13:03:56,105] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 196
[2024-05-14 13:03:56,106] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  6%|▌         | 198/3218 [04:58<1:14:16,  1.48s/it]
[2024-05-14 13:03:57,599] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 197
[2024-05-14 13:03:57,599] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  6%|▌         | 199/3218 [04:59<1:11:41,  1.42s/it]
[2024-05-14 13:03:58,903] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 198
[2024-05-14 13:03:58,904] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:03:58,904] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 13:04:00,252] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 199
[2024-05-14 13:04:00,253] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:04:00,261] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
  6%|▌         | 200/3218 [05:01<1:10:20,  1.40s/it][INFO|trainer.py:3166] 2024-05-14 13:04:00,442 >> ***** Running Evaluation *****
[INFO|trainer.py:3168] 2024-05-14 13:04:00,442 >>   Num examples = 521
[INFO|trainer.py:3171] 2024-05-14 13:04:00,442 >>   Batch size = 8






 94%|█████████▍| 31/33 [00:10<00:00,  2.82it/s]
{'eval_loss': nan, 'eval_runtime': 12.411, 'eval_samples_per_second': 41.979, 'eval_steps_per_second': 2.659, 'epoch': 0.06}
[INFO|trainer.py:2889] 2024-05-14 13:04:19,701 >> Saving model checkpoint to outputs/instruct_tuning/model/checkpoint-200
[INFO|configuration_utils.py:483] 2024-05-14 13:04:19,703 >> Configuration saved in outputs/instruct_tuning/model/checkpoint-200/config.json
[INFO|configuration_utils.py:594] 2024-05-14 13:04:19,705 >> Configuration saved in outputs/instruct_tuning/model/checkpoint-200/generation_config.json
[INFO|modeling_utils.py:2390] 2024-05-14 13:04:45,278 >> The model is bigger than the maximum size per checkpoint (5GB) and is going to be split in 3 checkpoint shards. You can find where each parameters has been saved in the index located at outputs/instruct_tuning/model/checkpoint-200/model.safetensors.index.json.
[INFO|tokenization_utils_base.py:2432] 2024-05-14 13:04:45,279 >> tokenizer config file saved in outputs/instruct_tuning/model/checkpoint-200/tokenizer_config.json
[INFO|tokenization_utils_base.py:2441] 2024-05-14 13:04:45,280 >> Special tokens file saved in outputs/instruct_tuning/model/checkpoint-200/special_tokens_map.json
[2024-05-14 13:04:46,037] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step200 is about to be saved!
[2024-05-14 13:04:46,061] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: outputs/instruct_tuning/model/checkpoint-200/global_step200/mp_rank_00_model_states.pt
[2024-05-14 13:04:46,061] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving outputs/instruct_tuning/model/checkpoint-200/global_step200/mp_rank_00_model_states.pt...
/mounts/work/lyk/anaconda3/envs/torch2/lib/python3.8/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
[2024-05-14 13:06:10,499] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved outputs/instruct_tuning/model/checkpoint-200/global_step200/mp_rank_00_model_states.pt.
  6%|▌         | 201/3218 [07:12<33:51:51, 40.41s/it]
[2024-05-14 13:06:10,501] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step200 is ready now!
[2024-05-14 13:06:11,673] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 200
[2024-05-14 13:06:11,674] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  6%|▋         | 202/3218 [07:14<24:04:10, 28.73s/it]
[2024-05-14 13:06:13,148] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 201
[2024-05-14 13:06:13,148] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:06:13,149] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 13:06:14,455] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 202
[2024-05-14 13:06:14,455] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  6%|▋         | 204/3218 [07:16<12:19:29, 14.72s/it]
[2024-05-14 13:06:15,697] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 203
[2024-05-14 13:06:15,698] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  6%|▋         | 205/3218 [07:18<9:02:22, 10.80s/it]
[2024-05-14 13:06:17,314] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 204
[2024-05-14 13:06:17,315] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:06:17,315] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 13:06:18,456] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 205
[2024-05-14 13:06:18,457] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  6%|▋         | 207/3218 [07:20<4:55:49,  5.89s/it]
[2024-05-14 13:06:19,714] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 206
[2024-05-14 13:06:19,715] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  6%|▋         | 208/3218 [07:21<3:45:09,  4.49s/it]
[2024-05-14 13:06:20,924] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 207
[2024-05-14 13:06:20,925] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:06:20,925] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 13:06:21,931] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 208
[2024-05-14 13:06:21,932] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  7%|▋         | 210/3218 [07:23<2:13:38,  2.67s/it]
[2024-05-14 13:06:22,825] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 209
[2024-05-14 13:06:22,825] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:06:22,826] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
{'loss': 0.0, 'learning_rate': 0.0, 'epoch': 0.07}
[2024-05-14 13:06:24,433] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 210
[2024-05-14 13:06:24,433] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  7%|▋         | 211/3218 [07:25<1:57:31,  2.34s/it]
[2024-05-14 13:06:26,137] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 211
[2024-05-14 13:06:26,137] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  7%|▋         | 213/3218 [07:27<1:25:56,  1.72s/it]
[2024-05-14 13:06:26,956] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 212
[2024-05-14 13:06:26,957] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:06:26,957] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 13:06:28,205] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 213
[2024-05-14 13:06:28,206] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  7%|▋         | 214/3218 [07:29<1:21:06,  1.62s/it]
[2024-05-14 13:06:29,895] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 214
[2024-05-14 13:06:29,895] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  7%|▋         | 216/3218 [07:32<1:21:11,  1.62s/it]
[2024-05-14 13:06:31,474] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 215
[2024-05-14 13:06:31,475] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  7%|▋         | 217/3218 [07:34<1:19:49,  1.60s/it]
[2024-05-14 13:06:33,008] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 216
[2024-05-14 13:06:33,008] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:06:33,008] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 13:06:34,619] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 217
[2024-05-14 13:06:34,620] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  7%|▋         | 218/3218 [07:35<1:20:11,  1.60s/it]
[2024-05-14 13:06:36,528] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 218
[2024-05-14 13:06:36,528] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  7%|▋         | 219/3218 [07:37<1:25:23,  1.71s/it]
[2024-05-14 13:06:38,192] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 219
[2024-05-14 13:06:38,193] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:06:38,193] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25

  7%|▋         | 220/3218 [07:39<1:24:27,  1.69s/it]
[2024-05-14 13:06:39,947] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 220
[2024-05-14 13:06:39,947] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  7%|▋         | 222/3218 [07:42<1:21:02,  1.62s/it]
[2024-05-14 13:06:41,404] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 221
[2024-05-14 13:06:41,404] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  7%|▋         | 223/3218 [07:44<1:20:59,  1.62s/it]
[2024-05-14 13:06:43,000] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 222
[2024-05-14 13:06:43,000] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:06:43,000] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 13:06:44,361] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 223
[2024-05-14 13:06:44,362] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  7%|▋         | 224/3218 [07:45<1:16:54,  1.54s/it]
[2024-05-14 13:06:46,029] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 224
[2024-05-14 13:06:46,030] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  7%|▋         | 226/3218 [07:48<1:15:55,  1.52s/it]
[2024-05-14 13:06:47,427] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 225
[2024-05-14 13:06:47,428] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  7%|▋         | 227/3218 [07:50<1:24:01,  1.69s/it]
[2024-05-14 13:06:49,462] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 226
[2024-05-14 13:06:49,463] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  7%|▋         | 228/3218 [07:51<1:19:12,  1.59s/it]
[2024-05-14 13:06:50,877] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 227
[2024-05-14 13:06:50,877] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:06:50,878] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 13:06:52,268] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 228
[2024-05-14 13:06:52,268] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  7%|▋         | 229/3218 [07:53<1:16:14,  1.53s/it]
[2024-05-14 13:06:54,159] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 229
[2024-05-14 13:06:54,160] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:06:54,160] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25

  7%|▋         | 231/3218 [07:56<1:17:53,  1.56s/it]
[2024-05-14 13:06:55,531] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 230
[2024-05-14 13:06:55,532] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  7%|▋         | 232/3218 [07:58<1:15:35,  1.52s/it]
[2024-05-14 13:06:56,977] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 231
[2024-05-14 13:06:56,978] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:06:56,978] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 13:06:58,795] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 232
[2024-05-14 13:06:58,796] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  7%|▋         | 233/3218 [07:59<1:21:07,  1.63s/it]
[2024-05-14 13:07:00,786] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 233
[2024-05-14 13:07:00,787] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  7%|▋         | 234/3218 [08:01<1:26:32,  1.74s/it]
[2024-05-14 13:07:02,519] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 234
[2024-05-14 13:07:02,519] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  7%|▋         | 235/3218 [08:03<1:26:03,  1.73s/it]
[2024-05-14 13:07:04,172] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 235
[2024-05-14 13:07:04,172] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  7%|▋         | 237/3218 [08:06<1:16:12,  1.53s/it]
[2024-05-14 13:07:05,323] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 236
[2024-05-14 13:07:05,323] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:07:05,323] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 13:07:06,514] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 237
[2024-05-14 13:07:06,514] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  7%|▋         | 239/3218 [08:08<1:08:32,  1.38s/it]
[2024-05-14 13:07:07,756] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 238
[2024-05-14 13:07:07,757] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:07:07,757] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 13:07:08,715] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 239
[2024-05-14 13:07:08,716] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  7%|▋         | 241/3218 [08:10<58:53,  1.19s/it]
{'loss': 0.0, 'learning_rate': 0.0, 'epoch': 0.07}
[2024-05-14 13:07:09,768] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 240
[2024-05-14 13:07:09,768] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  8%|▊         | 242/3218 [08:11<58:12,  1.17s/it]
[2024-05-14 13:07:10,929] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 241
[2024-05-14 13:07:10,929] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:07:10,929] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 13:07:12,198] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 242
[2024-05-14 13:07:12,198] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  8%|▊         | 244/3218 [08:14<57:10,  1.15s/it]
[2024-05-14 13:07:13,230] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 243
[2024-05-14 13:07:13,230] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:07:13,230] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 13:07:14,438] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 244
[2024-05-14 13:07:14,438] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  8%|▊         | 245/3218 [08:15<57:19,  1.16s/it]
[2024-05-14 13:07:16,106] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 245
[2024-05-14 13:07:16,106] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  8%|▊         | 247/3218 [08:18<1:11:18,  1.44s/it]
[2024-05-14 13:07:17,783] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 246
[2024-05-14 13:07:17,784] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  8%|▊         | 248/3218 [08:19<1:04:31,  1.30s/it]
[2024-05-14 13:07:18,963] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 247
[2024-05-14 13:07:18,964] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:07:18,964] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 13:07:20,559] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 248
[2024-05-14 13:07:20,560] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  8%|▊         | 249/3218 [08:21<1:11:14,  1.44s/it]
[2024-05-14 13:07:22,425] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 249
[2024-05-14 13:07:22,426] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:07:22,427] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25

  8%|▊         | 250/3218 [08:23<1:17:46,  1.57s/it]
[2024-05-14 13:07:24,572] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 250
[2024-05-14 13:07:24,573] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  8%|▊         | 251/3218 [08:25<1:26:31,  1.75s/it]
[2024-05-14 13:07:26,699] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 251
[2024-05-14 13:07:26,700] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  8%|▊         | 252/3218 [08:27<1:31:50,  1.86s/it]
[2024-05-14 13:07:28,400] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 252
[2024-05-14 13:07:28,400] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  8%|▊         | 253/3218 [08:29<1:28:46,  1.80s/it]
[2024-05-14 13:07:29,938] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 253
[2024-05-14 13:07:29,942] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  8%|▊         | 255/3218 [08:32<1:20:17,  1.63s/it]
[2024-05-14 13:07:31,336] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 254
[2024-05-14 13:07:31,336] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  8%|▊         | 256/3218 [08:34<1:21:22,  1.65s/it]
[2024-05-14 13:07:33,032] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 255
[2024-05-14 13:07:33,033] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:07:33,034] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 13:07:34,754] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 256
[2024-05-14 13:07:34,754] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  8%|▊         | 257/3218 [08:35<1:22:35,  1.67s/it]
[2024-05-14 13:07:36,058] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 257
[2024-05-14 13:07:36,058] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  8%|▊         | 259/3218 [08:38<1:13:12,  1.48s/it]
[2024-05-14 13:07:37,331] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 258
[2024-05-14 13:07:37,331] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:07:37,332] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 13:07:38,789] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 259
[2024-05-14 13:07:38,790] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  8%|▊         | 260/3218 [08:39<1:12:49,  1.48s/it]
{'loss': 0.0, 'learning_rate': 0.0, 'epoch': 0.08}
[2024-05-14 13:07:40,335] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 260
[2024-05-14 13:07:40,335] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  8%|▊         | 262/3218 [08:42<1:11:34,  1.45s/it]
[2024-05-14 13:07:41,700] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 261
[2024-05-14 13:07:41,701] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  8%|▊         | 263/3218 [08:44<1:17:39,  1.58s/it]
[2024-05-14 13:07:43,521] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 262
[2024-05-14 13:07:43,522] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  8%|▊         | 264/3218 [08:46<1:16:27,  1.55s/it]
[2024-05-14 13:07:45,091] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 263
[2024-05-14 13:07:45,092] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:07:45,095] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 13:07:46,437] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 264
[2024-05-14 13:07:46,437] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  8%|▊         | 265/3218 [08:47<1:13:41,  1.50s/it]
[2024-05-14 13:07:48,012] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 265
[2024-05-14 13:07:48,012] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  8%|▊         | 267/3218 [08:50<1:09:03,  1.40s/it]
[2024-05-14 13:07:49,123] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 266
[2024-05-14 13:07:49,124] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:07:49,133] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 13:07:50,446] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 267
[2024-05-14 13:07:50,446] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  8%|▊         | 268/3218 [08:51<1:07:35,  1.37s/it]
[2024-05-14 13:07:52,107] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 268
[2024-05-14 13:07:52,107] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  8%|▊         | 270/3218 [08:54<1:09:00,  1.40s/it]
[2024-05-14 13:07:53,391] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 269
[2024-05-14 13:07:53,391] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:07:53,393] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
{'loss': 0.0, 'learning_rate': 0.0, 'epoch': 0.08}
[2024-05-14 13:07:54,558] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 270
[2024-05-14 13:07:54,558] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  8%|▊         | 272/3218 [08:56<1:04:07,  1.31s/it]
[2024-05-14 13:07:55,792] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 271
[2024-05-14 13:07:55,793] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  8%|▊         | 273/3218 [08:58<1:07:34,  1.38s/it]
[2024-05-14 13:07:57,333] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 272
[2024-05-14 13:07:57,333] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:07:57,334] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 13:07:58,413] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 273
[2024-05-14 13:07:58,413] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  9%|▊         | 275/3218 [09:00<1:02:22,  1.27s/it]
[2024-05-14 13:07:59,625] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 274
[2024-05-14 13:07:59,625] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:07:59,627] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 13:08:00,718] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 275
[2024-05-14 13:08:00,718] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  9%|▊         | 276/3218 [09:01<1:00:31,  1.23s/it]
[2024-05-14 13:08:02,516] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 276
[2024-05-14 13:08:02,517] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  9%|▊         | 278/3218 [09:04<1:04:09,  1.31s/it]
[2024-05-14 13:08:03,691] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 277
[2024-05-14 13:08:03,691] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:08:03,692] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 13:08:04,800] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 278
[2024-05-14 13:08:04,801] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  9%|▊         | 280/3218 [09:06<56:57,  1.16s/it]
[2024-05-14 13:08:05,810] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 279
[2024-05-14 13:08:05,811] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:08:05,811] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25

  9%|▊         | 281/3218 [09:08<1:01:18,  1.25s/it]
[2024-05-14 13:08:07,284] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 280
[2024-05-14 13:08:07,284] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  9%|▉         | 282/3218 [09:10<1:13:34,  1.50s/it]
[2024-05-14 13:08:09,319] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 281
[2024-05-14 13:08:09,319] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:08:09,320] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 13:08:10,404] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 282
[2024-05-14 13:08:10,405] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  9%|▉         | 284/3218 [09:12<1:07:41,  1.38s/it]
[2024-05-14 13:08:11,753] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 283
[2024-05-14 13:08:11,753] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  9%|▉         | 285/3218 [09:14<1:09:11,  1.42s/it]
[2024-05-14 13:08:13,250] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 284
[2024-05-14 13:08:13,251] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  9%|▉         | 286/3218 [09:16<1:20:37,  1.65s/it]
[2024-05-14 13:08:15,386] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 285
[2024-05-14 13:08:15,386] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  9%|▉         | 287/3218 [09:18<1:20:31,  1.65s/it]
[2024-05-14 13:08:17,061] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 286
[2024-05-14 13:08:17,062] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:08:17,062] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 13:08:18,592] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 287
[2024-05-14 13:08:18,593] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  9%|▉         | 288/3218 [09:19<1:18:11,  1.60s/it]
[2024-05-14 13:08:20,214] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 288
[2024-05-14 13:08:20,214] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  9%|▉         | 290/3218 [09:22<1:17:51,  1.60s/it]
[2024-05-14 13:08:21,769] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 289
[2024-05-14 13:08:21,769] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:08:21,769] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25

  9%|▉         | 291/3218 [09:24<1:21:33,  1.67s/it]
[2024-05-14 13:08:23,563] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 290
[2024-05-14 13:08:23,564] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  9%|▉         | 292/3218 [09:26<1:20:37,  1.65s/it]
[2024-05-14 13:08:25,212] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 291
[2024-05-14 13:08:25,212] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:08:25,212] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 13:08:26,452] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 292
[2024-05-14 13:08:26,452] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  9%|▉         | 294/3218 [09:28<1:12:55,  1.50s/it]
[2024-05-14 13:08:27,878] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 293
[2024-05-14 13:08:27,878] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  9%|▉         | 295/3218 [09:30<1:10:11,  1.44s/it]
[2024-05-14 13:08:29,189] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 294
[2024-05-14 13:08:29,189] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  9%|▉         | 296/3218 [09:32<1:17:23,  1.59s/it]
[2024-05-14 13:08:31,096] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 295
[2024-05-14 13:08:31,097] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:08:31,097] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 13:08:32,398] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 296
[2024-05-14 13:08:32,399] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  9%|▉         | 297/3218 [09:33<1:12:35,  1.49s/it]
[2024-05-14 13:08:34,045] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 297
[2024-05-14 13:08:34,045] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  9%|▉         | 299/3218 [09:36<1:18:40,  1.62s/it]
[2024-05-14 13:08:35,810] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 298
[2024-05-14 13:08:35,810] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
  9%|▉         | 300/3218 [09:38<1:16:30,  1.57s/it][INFO|trainer.py:3166] 2024-05-14 13:08:37,469 >> ***** Running Evaluation *****
[INFO|trainer.py:3168] 2024-05-14 13:08:37,469 >>   Num examples = 521
[INFO|trainer.py:3171] 2024-05-14 13:08:37,469 >>   Batch size = 8
  6%|▌         | 2/33 [00:00<00:03,  8.42it/s]
[2024-05-14 13:08:37,314] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 299
[2024-05-14 13:08:37,315] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:08:37,316] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25






 88%|████████▊ | 29/33 [00:10<00:01,  2.82it/s]

  9%|▉         | 301/3218 [09:52<4:15:58,  5.27s/it]
[2024-05-14 13:08:51,130] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 300
[2024-05-14 13:08:51,130] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:08:51,136] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 13:08:52,592] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 301
[2024-05-14 13:08:52,593] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  9%|▉         | 303/3218 [09:54<2:38:29,  3.26s/it]
[2024-05-14 13:08:53,882] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 302
[2024-05-14 13:08:53,883] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  9%|▉         | 304/3218 [09:56<2:14:22,  2.77s/it]
[2024-05-14 13:08:55,492] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 303
[2024-05-14 13:08:55,492] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:08:55,492] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 13:08:56,798] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 304
[2024-05-14 13:08:56,799] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  9%|▉         | 305/3218 [09:57<1:53:12,  2.33s/it]
[2024-05-14 13:08:58,114] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 305
[2024-05-14 13:08:58,115] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 10%|▉         | 307/3218 [10:00<1:33:40,  1.93s/it]
[2024-05-14 13:08:59,795] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 306
[2024-05-14 13:08:59,796] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 10%|▉         | 308/3218 [10:02<1:25:27,  1.76s/it]
[2024-05-14 13:09:01,189] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 307
[2024-05-14 13:09:01,190] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:09:01,190] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 13:09:02,318] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 308
[2024-05-14 13:09:02,318] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 10%|▉         | 310/3218 [10:04<1:09:37,  1.44s/it]
[2024-05-14 13:09:03,434] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 309
[2024-05-14 13:09:03,434] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:09:03,434] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
{'loss': 0.0, 'learning_rate': 0.0, 'epoch': 0.1}
[2024-05-14 13:09:04,531] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 310
[2024-05-14 13:09:04,532] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 10%|▉         | 312/3218 [10:06<1:02:44,  1.30s/it]
[2024-05-14 13:09:05,742] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 311
[2024-05-14 13:09:05,742] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:09:05,742] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 13:09:06,806] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 312
[2024-05-14 13:09:06,807] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 10%|▉         | 314/3218 [10:09<58:31,  1.21s/it]
[2024-05-14 13:09:07,978] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 313
[2024-05-14 13:09:07,978] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:09:07,978] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 13:09:08,959] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 314
[2024-05-14 13:09:08,959] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25


 10%|▉         | 316/3218 [10:12<1:13:14,  1.51s/it]
[2024-05-14 13:09:11,392] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 315
[2024-05-14 13:09:11,392] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 10%|▉         | 317/3218 [10:14<1:24:29,  1.75s/it]
[2024-05-14 13:09:13,597] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 316
[2024-05-14 13:09:13,597] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:09:13,598] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 13:09:14,483] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 317
[2024-05-14 13:09:14,483] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 10%|▉         | 319/3218 [10:16<1:07:46,  1.40s/it]
[2024-05-14 13:09:15,638] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 318
[2024-05-14 13:09:15,639] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 10%|▉         | 320/3218 [10:18<1:15:41,  1.57s/it]
[2024-05-14 13:09:17,577] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 319
[2024-05-14 13:09:17,578] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:09:17,578] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25

 10%|▉         | 321/3218 [10:20<1:15:12,  1.56s/it]
[2024-05-14 13:09:19,134] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 320
[2024-05-14 13:09:19,134] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:09:19,135] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 13:09:20,860] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 321
[2024-05-14 13:09:20,861] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 10%|█         | 322/3218 [10:21<1:17:34,  1.61s/it]
[2024-05-14 13:09:22,821] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 322
[2024-05-14 13:09:22,822] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 10%|█         | 323/3218 [10:23<1:22:59,  1.72s/it]
[2024-05-14 13:09:24,332] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 323
[2024-05-14 13:09:24,332] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 10%|█         | 325/3218 [10:26<1:17:06,  1.60s/it]
[2024-05-14 13:09:25,837] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 324
[2024-05-14 13:09:25,837] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 10%|█         | 326/3218 [10:28<1:21:01,  1.68s/it]
[2024-05-14 13:09:27,676] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 325
[2024-05-14 13:09:27,677] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 10%|█         | 327/3218 [10:30<1:23:48,  1.74s/it]
[2024-05-14 13:09:29,560] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 326
[2024-05-14 13:09:29,561] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:09:29,562] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 13:09:30,976] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 327
[2024-05-14 13:09:30,976] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 10%|█         | 328/3218 [10:32<1:19:00,  1.64s/it]
[2024-05-14 13:09:32,483] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 328
[2024-05-14 13:09:32,483] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 10%|█         | 330/3218 [10:35<1:15:43,  1.57s/it]
[2024-05-14 13:09:34,009] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 329
[2024-05-14 13:09:34,009] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:09:34,009] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25

 10%|█         | 331/3218 [10:36<1:15:59,  1.58s/it]
[2024-05-14 13:09:35,549] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 330
[2024-05-14 13:09:35,550] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 10%|█         | 332/3218 [10:38<1:16:34,  1.59s/it]
[2024-05-14 13:09:37,223] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 331
[2024-05-14 13:09:37,224] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:09:37,224] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 13:09:38,612] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 332
[2024-05-14 13:09:38,612] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 10%|█         | 333/3218 [10:39<1:14:01,  1.54s/it]
[2024-05-14 13:09:40,076] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 333
[2024-05-14 13:09:40,076] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 10%|█         | 335/3218 [10:42<1:11:44,  1.49s/it]
[2024-05-14 13:09:41,482] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 334
[2024-05-14 13:09:41,482] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:09:41,482] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 13:09:42,869] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 335
[2024-05-14 13:09:42,870] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 10%|█         | 336/3218 [10:43<1:09:26,  1.45s/it]
[2024-05-14 13:09:44,222] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 336
[2024-05-14 13:09:44,223] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 11%|█         | 338/3218 [10:46<1:10:35,  1.47s/it]
[2024-05-14 13:09:45,816] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 337
[2024-05-14 13:09:45,817] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:09:45,817] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 13:09:47,122] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 338
[2024-05-14 13:09:47,122] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 11%|█         | 339/3218 [10:48<1:08:35,  1.43s/it]
[2024-05-14 13:09:48,733] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 339
[2024-05-14 13:09:48,734] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:09:48,734] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25

 11%|█         | 341/3218 [10:51<1:07:26,  1.41s/it]
[2024-05-14 13:09:49,986] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 340
[2024-05-14 13:09:49,987] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:09:49,987] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 13:09:51,051] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 341
[2024-05-14 13:09:51,052] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 11%|█         | 342/3218 [10:52<1:02:13,  1.30s/it]
[2024-05-14 13:09:52,333] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 342
[2024-05-14 13:09:52,333] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 11%|█         | 344/3218 [10:54<1:01:39,  1.29s/it]
[2024-05-14 13:09:53,584] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 343
[2024-05-14 13:09:53,584] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:09:53,584] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 13:09:54,669] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 344
[2024-05-14 13:09:54,669] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 11%|█         | 346/3218 [10:56<56:55,  1.19s/it]
[2024-05-14 13:09:55,783] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 345
[2024-05-14 13:09:55,783] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:09:55,783] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 13:09:56,999] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 346
[2024-05-14 13:09:57,000] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 11%|█         | 348/3218 [10:59<54:46,  1.15s/it]
[2024-05-14 13:09:58,019] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 347
[2024-05-14 13:09:58,020] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:09:58,020] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 13:09:59,030] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 348
[2024-05-14 13:09:59,031] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 11%|█         | 350/3218 [11:01<51:11,  1.07s/it]
[2024-05-14 13:10:00,078] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 349
[2024-05-14 13:10:00,079] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:10:00,079] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25

 11%|█         | 351/3218 [11:03<1:04:53,  1.36s/it]
[2024-05-14 13:10:02,049] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 350
[2024-05-14 13:10:02,049] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 11%|█         | 352/3218 [11:05<1:13:53,  1.55s/it]
[2024-05-14 13:10:04,020] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 351
[2024-05-14 13:10:04,021] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:10:04,021] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 13:10:04,907] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 352
[2024-05-14 13:10:04,908] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 11%|█         | 353/3218 [11:05<1:01:37,  1.29s/it]
[2024-05-14 13:10:06,269] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 353
[2024-05-14 13:10:06,269] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 11%|█         | 354/3218 [11:07<1:05:13,  1.37s/it]
[2024-05-14 13:10:08,213] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 354
[2024-05-14 13:10:08,213] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 11%|█         | 356/3218 [11:10<1:13:08,  1.53s/it]
[2024-05-14 13:10:09,750] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 355
[2024-05-14 13:10:09,750] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 11%|█         | 357/3218 [11:12<1:17:51,  1.63s/it]
[2024-05-14 13:10:11,562] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 356
[2024-05-14 13:10:11,562] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 11%|█         | 358/3218 [11:14<1:19:04,  1.66s/it]
[2024-05-14 13:10:13,340] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 357
[2024-05-14 13:10:13,341] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 11%|█         | 359/3218 [11:16<1:22:32,  1.73s/it]
[2024-05-14 13:10:15,263] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 358
[2024-05-14 13:10:15,263] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:10:15,264] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 13:10:16,985] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 359
[2024-05-14 13:10:16,986] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:10:16,987] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25

 11%|█         | 360/3218 [11:18<1:22:46,  1.74s/it]
[2024-05-14 13:10:18,761] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 360
[2024-05-14 13:10:18,761] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 11%|█         | 361/3218 [11:19<1:23:37,  1.76s/it]
[2024-05-14 13:10:20,893] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 361
[2024-05-14 13:10:20,893] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 11%|█         | 362/3218 [11:21<1:29:02,  1.87s/it]
[2024-05-14 13:10:22,420] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 362
[2024-05-14 13:10:22,421] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 11%|█▏        | 364/3218 [11:25<1:21:18,  1.71s/it]
[2024-05-14 13:10:24,019] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 363
[2024-05-14 13:10:24,020] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 11%|█▏        | 365/3218 [11:26<1:16:57,  1.62s/it]
[2024-05-14 13:10:25,414] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 364
[2024-05-14 13:10:25,414] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:10:25,418] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 13:10:27,059] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 365
[2024-05-14 13:10:27,060] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 11%|█▏        | 366/3218 [11:28<1:17:21,  1.63s/it]
[2024-05-14 13:10:28,532] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 366
[2024-05-14 13:10:28,533] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 11%|█▏        | 367/3218 [11:29<1:15:01,  1.58s/it]
[2024-05-14 13:10:30,370] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 367
[2024-05-14 13:10:30,370] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 11%|█▏        | 368/3218 [11:31<1:19:02,  1.66s/it]
[2024-05-14 13:10:32,436] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 368
[2024-05-14 13:10:32,437] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 11%|█▏        | 370/3218 [11:35<1:21:24,  1.72s/it]
[2024-05-14 13:10:34,000] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 369
[2024-05-14 13:10:34,001] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:10:34,001] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25

 12%|█▏        | 371/3218 [11:36<1:20:55,  1.71s/it]
[2024-05-14 13:10:35,711] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 370
[2024-05-14 13:10:35,712] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 12%|█▏        | 372/3218 [11:38<1:20:51,  1.70s/it]
[2024-05-14 13:10:37,398] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 371
[2024-05-14 13:10:37,398] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:10:37,399] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 13:10:38,841] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 372
[2024-05-14 13:10:38,842] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 12%|█▏        | 373/3218 [11:39<1:17:07,  1.63s/it]
[2024-05-14 13:10:40,232] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 373
[2024-05-14 13:10:40,232] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 12%|█▏        | 375/3218 [11:42<1:14:16,  1.57s/it]
[2024-05-14 13:10:41,836] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 374
[2024-05-14 13:10:41,836] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:10:41,836] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 13:10:43,263] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 375
[2024-05-14 13:10:43,263] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 12%|█▏        | 376/3218 [11:44<1:12:16,  1.53s/it]
[2024-05-14 13:10:44,880] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 376
[2024-05-14 13:10:44,881] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 12%|█▏        | 378/3218 [11:47<1:08:18,  1.44s/it]
[2024-05-14 13:10:46,037] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 377
[2024-05-14 13:10:46,038] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:10:46,038] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 13:10:47,197] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 378
[2024-05-14 13:10:47,198] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 12%|█▏        | 379/3218 [11:48<1:03:51,  1.35s/it]
[2024-05-14 13:10:48,393] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 379
[2024-05-14 13:10:48,393] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:10:48,393] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25

 12%|█▏        | 381/3218 [11:50<57:00,  1.21s/it]
[2024-05-14 13:10:49,385] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 380
[2024-05-14 13:10:49,385] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:10:49,385] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 13:10:50,427] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 381
[2024-05-14 13:10:50,427] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 12%|█▏        | 383/3218 [11:52<54:57,  1.16s/it]
[2024-05-14 13:10:51,597] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 382
[2024-05-14 13:10:51,597] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:10:51,597] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 13:10:52,800] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 383
[2024-05-14 13:10:52,800] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 12%|█▏        | 385/3218 [11:54<51:40,  1.09s/it]
[2024-05-14 13:10:53,779] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 384
[2024-05-14 13:10:53,779] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 12%|█▏        | 386/3218 [11:56<58:45,  1.25s/it]
[2024-05-14 13:10:55,326] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 385
[2024-05-14 13:10:55,327] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:10:55,327] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 13:10:57,233] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 386
[2024-05-14 13:10:57,234] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 12%|█▏        | 387/3218 [11:58<1:08:34,  1.45s/it]
[2024-05-14 13:10:58,332] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 387
[2024-05-14 13:10:58,332] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 12%|█▏        | 389/3218 [12:01<1:10:29,  1.50s/it]
[2024-05-14 13:11:00,056] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 388
[2024-05-14 13:11:00,057] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 12%|█▏        | 390/3218 [12:02<1:10:11,  1.49s/it]
[2024-05-14 13:11:01,628] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 389
[2024-05-14 13:11:01,629] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:11:01,629] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25

 12%|█▏        | 391/3218 [12:04<1:14:32,  1.58s/it]
[2024-05-14 13:11:03,375] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 390
[2024-05-14 13:11:03,375] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:11:03,375] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 13:11:04,851] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 391
[2024-05-14 13:11:04,851] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 12%|█▏        | 392/3218 [12:05<1:12:32,  1.54s/it]
[2024-05-14 13:11:06,409] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 392
[2024-05-14 13:11:06,410] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 12%|█▏        | 393/3218 [12:07<1:12:56,  1.55s/it]
[2024-05-14 13:11:08,272] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 393
[2024-05-14 13:11:08,274] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 12%|█▏        | 394/3218 [12:09<1:16:33,  1.63s/it]
[2024-05-14 13:11:10,190] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 394
[2024-05-14 13:11:10,190] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 12%|█▏        | 396/3218 [12:12<1:20:44,  1.72s/it]
[2024-05-14 13:11:11,880] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 395
[2024-05-14 13:11:11,881] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 12%|█▏        | 397/3218 [12:14<1:24:08,  1.79s/it]
[2024-05-14 13:11:13,794] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 396
[2024-05-14 13:11:13,794] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:11:13,795] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 13:11:15,308] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 397
[2024-05-14 13:11:15,308] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 12%|█▏        | 398/3218 [12:16<1:18:57,  1.68s/it]
[2024-05-14 13:11:17,139] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 398
[2024-05-14 13:11:17,139] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 12%|█▏        | 399/3218 [12:18<1:22:13,  1.75s/it]
[2024-05-14 13:11:18,792] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 399
[2024-05-14 13:11:18,792] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:11:18,793] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
 12%|█▏        | 400/3218 [12:19<1:20:10,  1.71s/it][INFO|trainer.py:3166] 2024-05-14 13:11:19,108 >> ***** Running Evaluation *****
[INFO|trainer.py:3168] 2024-05-14 13:11:19,108 >>   Num examples = 521
[INFO|trainer.py:3171] 2024-05-14 13:11:19,108 >>   Batch size = 8




 52%|█████▏    | 17/33 [00:06<00:05,  3.11it/s]
 12%|█▏        | 400/3218 [12:32<1:20:10,  1.71[INFO|trainer.py:2889] 2024-05-14 13:11:37,594 >> Saving model checkpoint to outputs/instruct_tuning/model/tmp-checkpoint-400
[INFO|configuration_utils.py:483] 2024-05-14 13:11:37,596 >> Configuration saved in outputs/instruct_tuning/model/tmp-checkpoint-400/config.json
[INFO|configuration_utils.py:594] 2024-05-14 13:11:37,597 >> Configuration saved in outputs/instruct_tuning/model/tmp-checkpoint-400/generation_config.json
[INFO|modeling_utils.py:2390] 2024-05-14 13:12:04,145 >> The model is bigger than the maximum size per checkpoint (5GB) and is going to be split in 3 checkpoint shards. You can find where each parameters has been saved in the index located at outputs/instruct_tuning/model/tmp-checkpoint-400/model.safetensors.index.json.
[INFO|tokenization_utils_base.py:2432] 2024-05-14 13:12:04,147 >> tokenizer config file saved in outputs/instruct_tuning/model/tmp-checkpoint-400/tokenizer_config.json
[INFO|tokenization_utils_base.py:2441] 2024-05-14 13:12:04,148 >> Special tokens file saved in outputs/instruct_tuning/model/tmp-checkpoint-400/special_tokens_map.json
[2024-05-14 13:12:04,938] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step400 is about to be saved!
[2024-05-14 13:12:04,945] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: outputs/instruct_tuning/model/tmp-checkpoint-400/global_step400/mp_rank_00_model_states.pt
[2024-05-14 13:12:04,945] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving outputs/instruct_tuning/model/tmp-checkpoint-400/global_step400/mp_rank_00_model_states.pt...
/mounts/work/lyk/anaconda3/envs/torch2/lib/python3.8/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
[2024-05-14 13:13:26,699] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved outputs/instruct_tuning/model/tmp-checkpoint-400/global_step400/mp_rank_00_model_states.pt.
[2024-05-14 13:13:26,700] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step400 is ready now!
[INFO|trainer.py:2979] 2024-05-14 13:13:26,709 >> Deleting older checkpoint [outputs/instruct_tuning/model/checkpoint-1000] due to args.save_total_limit
[2024-05-14 13:13:29,219] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 400
[2024-05-14 13:13:29,220] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:13:29,220] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
 12%|█▏        | 401/3218 [14:30<31:34:09, 40.34s/it]
[2024-05-14 13:13:30,958] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 401
[2024-05-14 13:13:30,959] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 12%|█▏        | 402/3218 [14:32<22:29:40, 28.76s/it]
[2024-05-14 13:13:32,470] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 402
[2024-05-14 13:13:32,470] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 13%|█▎        | 404/3218 [14:35<11:38:50, 14.90s/it]
[2024-05-14 13:13:34,137] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 403
[2024-05-14 13:13:34,138] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 13%|█▎        | 405/3218 [14:36<8:30:47, 10.89s/it]
[2024-05-14 13:13:35,680] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 404
[2024-05-14 13:13:35,681] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:13:35,681] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 13:13:37,149] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 405
[2024-05-14 13:13:37,149] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 13%|█▎        | 406/3218 [14:38<6:18:13,  8.07s/it]
[2024-05-14 13:13:38,419] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 406
[2024-05-14 13:13:38,419] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 13%|█▎        | 408/3218 [14:40<3:37:03,  4.63s/it]
[2024-05-14 13:13:39,787] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 407
[2024-05-14 13:13:39,787] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:13:39,787] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 13:13:40,965] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 408
[2024-05-14 13:13:40,965] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 13%|█▎        | 409/3218 [14:42<2:48:02,  3.59s/it]
[2024-05-14 13:13:42,487] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 409
[2024-05-14 13:13:42,487] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:13:42,487] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25

 13%|█▎        | 411/3218 [14:44<1:55:26,  2.47s/it]
[2024-05-14 13:13:43,767] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 410
[2024-05-14 13:13:43,767] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:13:43,768] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 13:13:45,009] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 411
[2024-05-14 13:13:45,009] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 13%|█▎        | 413/3218 [14:47<1:24:34,  1.81s/it]
[2024-05-14 13:13:46,152] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 412
[2024-05-14 13:13:46,153] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:13:46,154] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 13:13:47,162] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 413
[2024-05-14 13:13:47,162] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 13%|█▎        | 415/3218 [14:49<1:06:59,  1.43s/it]
[2024-05-14 13:13:48,276] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 414
[2024-05-14 13:13:48,276] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 13%|█▎        | 416/3218 [14:50<1:06:43,  1.43s/it]
[2024-05-14 13:13:49,680] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 415
[2024-05-14 13:13:49,680] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:13:49,681] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 13:13:51,154] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 416
[2024-05-14 13:13:51,155] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 13%|█▎        | 417/3218 [14:52<1:07:47,  1.45s/it]
[2024-05-14 13:13:52,400] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 417
[2024-05-14 13:13:52,400] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:13:52,400] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 13:13:53,541] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 418
[2024-05-14 13:13:53,542] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 13%|█▎        | 419/3218 [14:54<1:00:50,  1.30s/it]
[2024-05-14 13:13:54,542] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 419
[2024-05-14 13:13:54,542] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:13:54,542] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25

 13%|█▎        | 421/3218 [14:56<56:09,  1.20s/it]
[2024-05-14 13:13:55,774] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 420
[2024-05-14 13:13:55,774] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 13%|█▎        | 422/3218 [14:58<1:09:15,  1.49s/it]
[2024-05-14 13:13:57,822] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 421
[2024-05-14 13:13:57,823] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:13:57,823] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 13:13:58,665] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 422
[2024-05-14 13:13:58,665] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 13%|█▎        | 424/3218 [15:01<1:00:54,  1.31s/it]
[2024-05-14 13:13:59,993] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 423
[2024-05-14 13:13:59,993] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 13%|█▎        | 425/3218 [15:02<1:06:18,  1.42s/it]
[2024-05-14 13:14:01,683] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 424
[2024-05-14 13:14:01,683] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:14:01,683] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 13:14:03,306] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 425
[2024-05-14 13:14:03,306] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 13%|█▎        | 426/3218 [15:04<1:09:20,  1.49s/it]
[2024-05-14 13:14:04,938] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 426
[2024-05-14 13:14:04,939] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 13%|█▎        | 427/3218 [15:06<1:11:21,  1.53s/it]
[2024-05-14 13:14:06,686] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 427
[2024-05-14 13:14:06,686] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 13%|█▎        | 428/3218 [15:07<1:14:27,  1.60s/it]
[2024-05-14 13:14:08,484] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 428
[2024-05-14 13:14:08,485] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 13%|█▎        | 430/3218 [15:11<1:17:01,  1.66s/it]
[2024-05-14 13:14:10,152] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 429
[2024-05-14 13:14:10,152] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:14:10,153] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25

 13%|█▎        | 431/3218 [15:12<1:14:01,  1.59s/it]
[2024-05-14 13:14:11,627] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 430
[2024-05-14 13:14:11,628] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:14:11,628] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 13:14:13,177] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 431
[2024-05-14 13:14:13,178] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 13%|█▎        | 432/3218 [15:14<1:13:40,  1.59s/it]
[2024-05-14 13:14:14,511] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 432
[2024-05-14 13:14:14,512] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 13%|█▎        | 434/3218 [15:16<1:08:49,  1.48s/it]
[2024-05-14 13:14:15,927] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 433
[2024-05-14 13:14:15,928] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 14%|█▎        | 435/3218 [15:18<1:15:06,  1.62s/it]
[2024-05-14 13:14:17,818] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 434
[2024-05-14 13:14:17,819] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:14:17,819] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 13:14:19,133] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 435
[2024-05-14 13:14:19,134] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 14%|█▎        | 436/3218 [15:20<1:10:17,  1.52s/it]
[2024-05-14 13:14:20,638] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 436
[2024-05-14 13:14:20,638] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 14%|█▎        | 438/3218 [15:23<1:12:08,  1.56s/it]
[2024-05-14 13:14:22,310] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 437
[2024-05-14 13:14:22,311] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:14:22,311] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 13:14:23,527] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 438
[2024-05-14 13:14:23,528] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 14%|█▎        | 439/3218 [15:24<1:07:01,  1.45s/it]
[2024-05-14 13:14:24,973] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 439
[2024-05-14 13:14:24,974] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:14:24,974] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25

 14%|█▎        | 441/3218 [15:27<1:05:52,  1.42s/it]
[2024-05-14 13:14:26,340] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 440
[2024-05-14 13:14:26,341] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 14%|█▎        | 442/3218 [15:28<1:05:04,  1.41s/it]
[2024-05-14 13:14:27,700] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 441
[2024-05-14 13:14:27,700] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:14:27,700] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 13:14:28,953] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 442
[2024-05-14 13:14:28,953] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 14%|█▍        | 444/3218 [15:31<1:03:50,  1.38s/it]
[2024-05-14 13:14:30,374] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 443
[2024-05-14 13:14:30,374] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:14:30,374] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 13:14:31,504] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 444
[2024-05-14 13:14:31,504] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 14%|█▍        | 445/3218 [15:32<59:52,  1.30s/it]
[2024-05-14 13:14:32,898] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 445
[2024-05-14 13:14:32,898] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 14%|█▍        | 447/3218 [15:35<58:05,  1.26s/it]
[2024-05-14 13:14:34,006] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 446
[2024-05-14 13:14:34,006] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:14:34,007] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 13:14:35,231] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 447
[2024-05-14 13:14:35,232] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 14%|█▍        | 449/3218 [15:37<54:35,  1.18s/it]
[2024-05-14 13:14:36,269] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 448
[2024-05-14 13:14:36,269] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 14%|█▍        | 450/3218 [15:38<58:14,  1.26s/it]
[2024-05-14 13:14:37,688] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 449
[2024-05-14 13:14:37,688] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:14:37,688] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
{'loss': 0.0, 'learning_rate': 0.0, 'epoch': 0.14}
[2024-05-14 13:14:38,909] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 450
[2024-05-14 13:14:38,909] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 14%|█▍        | 452/3218 [15:41<56:13,  1.22s/it]
[2024-05-14 13:14:40,070] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 451
[2024-05-14 13:14:40,070] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:14:40,071] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 13:14:41,199] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 452
[2024-05-14 13:14:41,199] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 14%|█▍        | 453/3218 [15:42<55:13,  1.20s/it]
[2024-05-14 13:14:42,514] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 453
[2024-05-14 13:14:42,515] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 14%|█▍        | 455/3218 [15:44<56:15,  1.22s/it]
[2024-05-14 13:14:43,717] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 454
[2024-05-14 13:14:43,717] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:14:43,717] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 13:14:45,083] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 455
[2024-05-14 13:14:45,084] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 14%|█▍        | 456/3218 [15:46<58:15,  1.27s/it]
[2024-05-14 13:14:46,500] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 456
[2024-05-14 13:14:46,500] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:14:46,500] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 13:14:47,245] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 457
[2024-05-14 13:14:47,246] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 14%|█▍        | 459/3218 [15:49<53:51,  1.17s/it]
[2024-05-14 13:14:48,440] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 458
[2024-05-14 13:14:48,441] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:14:48,441] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 13:14:50,545] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 459
[2024-05-14 13:14:50,545] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:14:50,547] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25

 14%|█▍        | 461/3218 [15:53<1:09:05,  1.50s/it]
[2024-05-14 13:14:52,127] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 460
[2024-05-14 13:14:52,128] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 14%|█▍        | 462/3218 [15:54<1:11:08,  1.55s/it]
[2024-05-14 13:14:53,829] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 461
[2024-05-14 13:14:53,829] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:14:53,831] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 13:14:55,595] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 462
[2024-05-14 13:14:55,595] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 14%|█▍        | 463/3218 [15:56<1:14:33,  1.62s/it]
[2024-05-14 13:14:57,668] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 463
[2024-05-14 13:14:57,668] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 14%|█▍        | 464/3218 [15:58<1:21:04,  1.77s/it]
[2024-05-14 13:14:59,171] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 464
[2024-05-14 13:14:59,172] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 14%|█▍        | 465/3218 [16:00<1:17:06,  1.68s/it]
[2024-05-14 13:15:00,464] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 465
[2024-05-14 13:15:00,464] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 15%|█▍        | 467/3218 [16:02<1:08:20,  1.49s/it]
[2024-05-14 13:15:01,810] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 466
[2024-05-14 13:15:01,810] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:15:01,812] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 13:15:03,356] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 467
[2024-05-14 13:15:03,357] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 15%|█▍        | 468/3218 [16:04<1:09:30,  1.52s/it]
[2024-05-14 13:15:05,285] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 468
[2024-05-14 13:15:05,285] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 15%|█▍        | 469/3218 [16:06<1:14:43,  1.63s/it]
[2024-05-14 13:15:06,712] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 469
[2024-05-14 13:15:06,712] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:15:06,712] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25

 15%|█▍        | 471/3218 [16:09<1:13:47,  1.61s/it]
[2024-05-14 13:15:08,425] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 470
[2024-05-14 13:15:08,425] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 15%|█▍        | 472/3218 [16:11<1:16:29,  1.67s/it]
[2024-05-14 13:15:10,196] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 471
[2024-05-14 13:15:10,197] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:15:10,199] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 13:15:11,387] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 472
[2024-05-14 13:15:11,387] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 15%|█▍        | 473/3218 [16:12<1:09:31,  1.52s/it]
[2024-05-14 13:15:12,525] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 473
[2024-05-14 13:15:12,526] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 15%|█▍        | 475/3218 [16:14<1:02:02,  1.36s/it]
[2024-05-14 13:15:13,768] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 474
[2024-05-14 13:15:13,768] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:15:13,769] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 13:15:15,049] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 475
[2024-05-14 13:15:15,049] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 15%|█▍        | 477/3218 [16:17<56:36,  1.24s/it]
[2024-05-14 13:15:16,056] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 476
[2024-05-14 13:15:16,056] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:15:16,058] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 13:15:17,648] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 477
[2024-05-14 13:15:17,649] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 15%|█▍        | 478/3218 [16:18<1:01:14,  1.34s/it]
[2024-05-14 13:15:19,023] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 478
[2024-05-14 13:15:19,023] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 15%|█▍        | 480/3218 [16:21<59:23,  1.30s/it]
[2024-05-14 13:15:20,212] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 479
[2024-05-14 13:15:20,212] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:15:20,212] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
{'loss': 0.0, 'learning_rate': 0.0, 'epoch': 0.15}
[2024-05-14 13:15:21,654] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 480
[2024-05-14 13:15:21,655] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 15%|█▍        | 481/3218 [16:22<1:01:16,  1.34s/it]
[2024-05-14 13:15:22,784] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 481
[2024-05-14 13:15:22,785] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 15%|█▌        | 483/3218 [16:25<57:16,  1.26s/it]
[2024-05-14 13:15:24,007] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 482
[2024-05-14 13:15:24,008] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:15:24,008] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 13:15:25,111] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 483
[2024-05-14 13:15:25,111] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 15%|█▌        | 485/3218 [16:27<54:59,  1.21s/it]
[2024-05-14 13:15:26,295] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 484
[2024-05-14 13:15:26,296] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:15:26,296] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 13:15:27,412] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 485
[2024-05-14 13:15:27,413] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 15%|█▌        | 486/3218 [16:28<53:44,  1.18s/it]
[2024-05-14 13:15:28,805] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 486
[2024-05-14 13:15:28,806] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 15%|█▌        | 488/3218 [16:30<53:05,  1.17s/it]
[2024-05-14 13:15:29,806] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 487
[2024-05-14 13:15:29,806] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:15:29,808] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 13:15:31,023] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 488
[2024-05-14 13:15:31,024] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 15%|█▌        | 490/3218 [16:32<49:36,  1.09s/it]
[2024-05-14 13:15:31,950] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 489
[2024-05-14 13:15:31,950] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:15:31,950] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
{'loss': 0.0, 'learning_rate': 0.0, 'epoch': 0.15}
[2024-05-14 13:15:33,379] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 490
[2024-05-14 13:15:33,380] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 15%|█▌        | 491/3218 [16:34<54:45,  1.20s/it]
[2024-05-14 13:15:35,053] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 491
[2024-05-14 13:15:35,053] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 15%|█▌        | 493/3218 [16:36<52:26,  1.15s/it]
[2024-05-14 13:15:35,921] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 492
[2024-05-14 13:15:35,921] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:15:35,921] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 13:15:36,838] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 493
[2024-05-14 13:15:36,838] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 15%|█▌        | 495/3218 [16:39<57:18,  1.26s/it]
[2024-05-14 13:15:38,379] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 494
[2024-05-14 13:15:38,380] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:15:38,381] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 13:15:39,764] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 495
[2024-05-14 13:15:39,765] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 15%|█▌        | 496/3218 [16:40<58:48,  1.30s/it]
[2024-05-14 13:15:41,387] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 496
[2024-05-14 13:15:41,387] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 15%|█▌        | 497/3218 [16:42<1:03:31,  1.40s/it]
[2024-05-14 13:15:42,730] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 497
[2024-05-14 13:15:42,731] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 16%|█▌        | 499/3218 [16:45<1:05:08,  1.44s/it]
[2024-05-14 13:15:44,314] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 498
[2024-05-14 13:15:44,314] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:15:44,314] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 13:15:45,760] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 499
[2024-05-14 13:15:45,760] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
 16%|█▌        | 500/3218 [16:46<1:05:32,  1.45s/it][INFO|trainer.py:3166] 2024-05-14 13:15:45,966 >> ***** Running Evaluation *****
[INFO|trainer.py:3168] 2024-05-14 13:15:45,966 >>   Num examples = 521
[INFO|trainer.py:3171] 2024-05-14 13:15:45,966 >>   Batch size = 8
  6%|▌         | 2/33 [00:00<00:03,  8.53it/s]






 91%|█████████ | 30/33 [00:10<00:01,  2.83it/s]
{'eval_loss': nan, 'eval_runtime': 12.4057, 'eval_samples_per_second': 41.997, 'eval_steps_per_second': 2.66, 'epoch': 0.16}
[2024-05-14 13:15:59,633] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 500
[2024-05-14 13:15:59,633] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 16%|█▌        | 501/3218 [17:00<3:54:20,  5.18s/it]
[2024-05-14 13:16:01,079] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 501
[2024-05-14 13:16:01,080] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 16%|█▌        | 503/3218 [17:03<2:25:34,  3.22s/it]
[2024-05-14 13:16:02,351] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 502
[2024-05-14 13:16:02,351] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 16%|█▌        | 504/3218 [17:05<2:06:17,  2.79s/it]
[2024-05-14 13:16:04,082] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 503
[2024-05-14 13:16:04,082] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:16:04,082] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 13:16:05,672] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 504
[2024-05-14 13:16:05,673] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 16%|█▌        | 505/3218 [17:06<1:48:48,  2.41s/it]
[2024-05-14 13:16:07,081] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 505
[2024-05-14 13:16:07,081] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 16%|█▌        | 507/3218 [17:09<1:24:05,  1.86s/it]
[2024-05-14 13:16:08,370] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 506
[2024-05-14 13:16:08,371] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:16:08,371] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 13:16:09,562] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 507
[2024-05-14 13:16:09,563] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 16%|█▌        | 508/3218 [17:10<1:14:53,  1.66s/it]
[2024-05-14 13:16:10,993] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 508
[2024-05-14 13:16:10,994] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 16%|█▌        | 510/3218 [17:13<1:06:21,  1.47s/it]
[2024-05-14 13:16:12,178] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 509
[2024-05-14 13:16:12,178] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:16:12,179] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
{'loss': 0.0, 'learning_rate': 0.0, 'epoch': 0.16}
[2024-05-14 13:16:13,538] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 510
[2024-05-14 13:16:13,539] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 16%|█▌        | 511/3218 [17:14<1:04:17,  1.43s/it]
[2024-05-14 13:16:14,631] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 511
[2024-05-14 13:16:14,631] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 16%|█▌        | 513/3218 [17:17<1:00:33,  1.34s/it]
[2024-05-14 13:16:15,987] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 512
[2024-05-14 13:16:15,987] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:16:15,987] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 13:16:17,178] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 513
[2024-05-14 13:16:17,179] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 16%|█▌        | 515/3218 [17:19<57:12,  1.27s/it]
[2024-05-14 13:16:18,397] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 514
[2024-05-14 13:16:18,397] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:16:18,398] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 13:16:19,683] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 515
[2024-05-14 13:16:19,683] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 16%|█▌        | 516/3218 [17:20<57:21,  1.27s/it]
[2024-05-14 13:16:20,714] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 516
[2024-05-14 13:16:20,715] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:16:20,715] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 13:16:21,803] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 517
[2024-05-14 13:16:21,803] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 16%|█▌        | 518/3218 [17:22<52:27,  1.17s/it]
[2024-05-14 13:16:23,073] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 518
[2024-05-14 13:16:23,074] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 16%|█▌        | 520/3218 [17:25<51:42,  1.15s/it]
[2024-05-14 13:16:24,114] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 519
[2024-05-14 13:16:24,114] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:16:24,115] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
{'loss': 0.0, 'learning_rate': 0.0, 'epoch': 0.16}
[2024-05-14 13:16:25,176] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 520
[2024-05-14 13:16:25,176] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 16%|█▌        | 522/3218 [17:27<48:16,  1.07s/it]
[2024-05-14 13:16:26,146] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 521
[2024-05-14 13:16:26,146] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:16:26,146] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 13:16:27,234] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 522
[2024-05-14 13:16:27,234] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 16%|█▋        | 524/3218 [17:29<47:50,  1.07s/it]
[2024-05-14 13:16:28,271] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 523
[2024-05-14 13:16:28,271] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:16:28,271] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 13:16:29,215] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 524
[2024-05-14 13:16:29,216] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 16%|█▋        | 525/3218 [17:30<45:57,  1.02s/it]
[2024-05-14 13:16:30,666] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 525
[2024-05-14 13:16:30,667] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 16%|█▋        | 527/3218 [17:33<55:58,  1.25s/it]
[2024-05-14 13:16:32,089] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 526
[2024-05-14 13:16:32,089] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:16:32,089] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 13:16:32,799] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 527
[2024-05-14 13:16:32,800] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 16%|█▋        | 529/3218 [17:35<55:16,  1.23s/it]
[2024-05-14 13:16:34,303] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 528
[2024-05-14 13:16:34,303] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:16:34,303] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 13:16:35,769] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 529
[2024-05-14 13:16:35,770] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 16%|█▋        | 530/3218 [17:36<57:44,  1.29s/it]
{'loss': 0.0, 'learning_rate': 0.0, 'epoch': 0.16}
[2024-05-14 13:16:37,411] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 530
[2024-05-14 13:16:37,411] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 17%|█▋        | 531/3218 [17:38<1:02:43,  1.40s/it]
[2024-05-14 13:16:39,270] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 531
[2024-05-14 13:16:39,271] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 17%|█▋        | 532/3218 [17:40<1:09:14,  1.55s/it]
[2024-05-14 13:16:41,436] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 532
[2024-05-14 13:16:41,436] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 17%|█▋        | 533/3218 [17:42<1:17:52,  1.74s/it]
[2024-05-14 13:16:43,242] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 533
[2024-05-14 13:16:43,243] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 17%|█▋        | 534/3218 [17:44<1:18:30,  1.76s/it]
[2024-05-14 13:16:44,612] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 534
[2024-05-14 13:16:44,613] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 17%|█▋        | 536/3218 [17:47<1:09:11,  1.55s/it]
[2024-05-14 13:16:45,955] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 535
[2024-05-14 13:16:45,955] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:16:45,956] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 13:16:47,700] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 536
[2024-05-14 13:16:47,701] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 17%|█▋        | 537/3218 [17:48<1:11:58,  1.61s/it]
[2024-05-14 13:16:48,916] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 537
[2024-05-14 13:16:48,916] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 17%|█▋        | 539/3218 [17:51<1:06:47,  1.50s/it]
[2024-05-14 13:16:50,413] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 538
[2024-05-14 13:16:50,413] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:16:50,413] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 13:16:52,578] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 539
[2024-05-14 13:16:52,578] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:16:52,580] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
{'loss': 0.0, 'learning_rate': 0.0, 'epoch': 0.17}
[2024-05-14 13:16:53,886] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 540
[2024-05-14 13:16:53,886] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 17%|█▋        | 541/3218 [17:54<1:10:17,  1.58s/it]
[2024-05-14 13:16:55,232] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 541
[2024-05-14 13:16:55,233] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 17%|█▋        | 543/3218 [17:57<1:00:03,  1.35s/it]
[2024-05-14 13:16:56,235] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 542
[2024-05-14 13:16:56,235] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:16:56,236] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 13:16:57,493] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 543
[2024-05-14 13:16:57,494] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 17%|█▋        | 544/3218 [17:58<59:09,  1.33s/it]
[2024-05-14 13:16:58,922] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 544
[2024-05-14 13:16:58,922] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 17%|█▋        | 545/3218 [18:00<1:01:17,  1.38s/it]
[2024-05-14 13:17:00,830] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 545
[2024-05-14 13:17:00,830] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 17%|█▋        | 547/3218 [18:03<1:02:44,  1.41s/it]
[2024-05-14 13:17:02,034] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 546
[2024-05-14 13:17:02,034] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:17:02,036] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 13:17:03,570] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 547
[2024-05-14 13:17:03,571] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 17%|█▋        | 548/3218 [18:04<1:05:23,  1.47s/it]
[2024-05-14 13:17:04,887] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 548
[2024-05-14 13:17:04,888] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 17%|█▋        | 550/3218 [18:07<59:20,  1.33s/it]
[2024-05-14 13:17:06,045] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 549
[2024-05-14 13:17:06,046] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:17:06,046] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
{'loss': 0.0, 'learning_rate': 0.0, 'epoch': 0.17}
[2024-05-14 13:17:07,248] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 550
[2024-05-14 13:17:07,249] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 17%|█▋        | 552/3218 [18:09<53:16,  1.20s/it]
[2024-05-14 13:17:08,246] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 551
[2024-05-14 13:17:08,246] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:17:08,248] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 13:17:09,254] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 552
[2024-05-14 13:17:09,254] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 17%|█▋        | 553/3218 [18:10<51:01,  1.15s/it]
[2024-05-14 13:17:10,756] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 553
[2024-05-14 13:17:10,756] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:17:10,756] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 13:17:11,823] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 554
[2024-05-14 13:17:11,824] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 17%|█▋        | 555/3218 [18:12<53:05,  1.20s/it]
[2024-05-14 13:17:12,979] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 555
[2024-05-14 13:17:12,980] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:17:12,980] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 13:17:14,005] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 556
[2024-05-14 13:17:14,006] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 17%|█▋        | 557/3218 [18:15<50:23,  1.14s/it]
[2024-05-14 13:17:15,220] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 557
[2024-05-14 13:17:15,220] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 17%|█▋        | 558/3218 [18:16<51:15,  1.16s/it]
[2024-05-14 13:17:16,218] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 558
[2024-05-14 13:17:16,218] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:17:16,218] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 13:17:17,326] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 559
[2024-05-14 13:17:17,327] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:17:17,327] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25

 17%|█▋        | 560/3218 [18:18<48:54,  1.10s/it]
[2024-05-14 13:17:18,677] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 560
[2024-05-14 13:17:18,677] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 17%|█▋        | 561/3218 [18:19<51:50,  1.17s/it]
[2024-05-14 13:17:20,115] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 561
[2024-05-14 13:17:20,115] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:17:20,115] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 13:17:20,896] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 562
[2024-05-14 13:17:20,896] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 18%|█▊        | 564/3218 [18:23<51:23,  1.16s/it]
[2024-05-14 13:17:22,143] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 563
[2024-05-14 13:17:22,143] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:17:22,143] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 13:17:24,655] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 564
[2024-05-14 13:17:24,656] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 18%|█▊        | 565/3218 [18:25<1:10:26,  1.59s/it]
[2024-05-14 13:17:26,337] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 565
[2024-05-14 13:17:26,337] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 18%|█▊        | 566/3218 [18:27<1:10:57,  1.61s/it]
[2024-05-14 13:17:28,797] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 566
[2024-05-14 13:17:28,797] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 18%|█▊        | 567/3218 [18:29<1:22:17,  1.86s/it]
[2024-05-14 13:17:30,376] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 567
[2024-05-14 13:17:30,377] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:17:30,377] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 13:17:31,862] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 568
[2024-05-14 13:17:31,862] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 18%|█▊        | 569/3218 [18:32<1:14:34,  1.69s/it]
[2024-05-14 13:17:33,295] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 569
[2024-05-14 13:17:33,295] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:17:33,296] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25

 18%|█▊        | 570/3218 [18:34<1:10:49,  1.60s/it]
[2024-05-14 13:17:34,806] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 570
[2024-05-14 13:17:34,806] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 18%|█▊        | 571/3218 [18:35<1:09:50,  1.58s/it]
[2024-05-14 13:17:36,492] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 571
[2024-05-14 13:17:36,493] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:17:36,493] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 13:17:37,885] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 572
[2024-05-14 13:17:37,886] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 18%|█▊        | 573/3218 [18:38<1:07:52,  1.54s/it]
[2024-05-14 13:17:39,164] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 573
[2024-05-14 13:17:39,165] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 18%|█▊        | 574/3218 [18:40<1:04:11,  1.46s/it]
[2024-05-14 13:17:40,591] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 574
[2024-05-14 13:17:40,591] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:17:40,593] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 13:17:42,009] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 575
[2024-05-14 13:17:42,009] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 18%|█▊        | 576/3218 [18:43<1:03:55,  1.45s/it]
[2024-05-14 13:17:43,223] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 576
[2024-05-14 13:17:43,223] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 18%|█▊        | 577/3218 [18:44<1:00:19,  1.37s/it]
[2024-05-14 13:17:44,469] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 577
[2024-05-14 13:17:44,470] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 18%|█▊        | 578/3218 [18:45<58:39,  1.33s/it]
[2024-05-14 13:17:46,392] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 578
[2024-05-14 13:17:46,392] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 18%|█▊        | 579/3218 [18:47<1:06:18,  1.51s/it]
[2024-05-14 13:17:48,250] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 579
[2024-05-14 13:17:48,250] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:17:48,251] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
{'loss': 0.0, 'learning_rate': 0.0, 'epoch': 0.18}
[2024-05-14 13:17:49,736] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 580
[2024-05-14 13:17:49,737] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 18%|█▊        | 581/3218 [18:50<1:09:07,  1.57s/it]
[2024-05-14 13:17:51,362] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 581
[2024-05-14 13:17:51,363] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 18%|█▊        | 582/3218 [18:52<1:09:59,  1.59s/it]
[2024-05-14 13:17:52,701] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 582
[2024-05-14 13:17:52,701] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:17:52,701] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 13:17:54,028] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 583
[2024-05-14 13:17:54,029] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 18%|█▊        | 584/3218 [18:55<1:03:29,  1.45s/it]
[2024-05-14 13:17:55,425] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 584
[2024-05-14 13:17:55,426] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 18%|█▊        | 585/3218 [18:56<1:03:41,  1.45s/it]
[2024-05-14 13:17:56,865] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 585
[2024-05-14 13:17:56,865] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:17:56,866] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 13:17:58,078] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 586
[2024-05-14 13:17:58,079] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 18%|█▊        | 587/3218 [18:59<1:00:10,  1.37s/it]
[2024-05-14 13:17:59,290] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 587
[2024-05-14 13:17:59,290] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 18%|█▊        | 588/3218 [19:00<58:14,  1.33s/it]
[2024-05-14 13:18:00,491] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 588
[2024-05-14 13:18:00,491] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:18:00,491] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 13:18:01,996] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 589
[2024-05-14 13:18:01,996] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 18%|█▊        | 590/3218 [19:03<59:45,  1.36s/it]
{'loss': 0.0, 'learning_rate': 0.0, 'epoch': 0.18}
[2024-05-14 13:18:03,050] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 590
[2024-05-14 13:18:03,051] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 18%|█▊        | 591/3218 [19:04<55:09,  1.26s/it]
[2024-05-14 13:18:04,317] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 591
[2024-05-14 13:18:04,318] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:18:04,318] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 13:18:05,465] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 592
[2024-05-14 13:18:05,466] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 18%|█▊        | 593/3218 [19:06<53:30,  1.22s/it]
[2024-05-14 13:18:06,569] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 593
[2024-05-14 13:18:06,570] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:18:06,570] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 13:18:07,460] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 594
[2024-05-14 13:18:07,461] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 18%|█▊        | 595/3218 [19:08<47:46,  1.09s/it]
[2024-05-14 13:18:08,644] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 595
[2024-05-14 13:18:08,645] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 19%|█▊        | 596/3218 [19:09<48:41,  1.11s/it]
[2024-05-14 13:18:10,610] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 596
[2024-05-14 13:18:10,610] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:18:10,610] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 13:18:11,400] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 597
[2024-05-14 13:18:11,400] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 19%|█▊        | 598/3218 [19:12<50:21,  1.15s/it]
[2024-05-14 13:18:12,619] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 598
[2024-05-14 13:18:12,620] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 19%|█▊        | 599/3218 [19:13<52:57,  1.21s/it]
[2024-05-14 13:18:14,563] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 599
[2024-05-14 13:18:14,563] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:18:14,564] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
 19%|█▊        | 600/3218 [19:15<1:03:32,  1.46s/it][INFO|trainer.py:3166] 2024-05-14 13:18:14,785 >> ***** Running Evaluation *****
[INFO|trainer.py:3168] 2024-05-14 13:18:14,785 >>   Num examples = 521
[INFO|trainer.py:3171] 2024-05-14 13:18:14,785 >>   Batch size = 8





 97%|█████████▋| 32/33 [00:11<00:00,  2.77it/s]

 19%|█▊        | 600/3218 [19:28<1:03:32,  1.46[INFO|trainer.py:2889] 2024-05-14 13:18:33,159 >> Saving model checkpoint to outputs/instruct_tuning/model/tmp-checkpoint-600
[INFO|configuration_utils.py:483] 2024-05-14 13:18:33,162 >> Configuration saved in outputs/instruct_tuning/model/tmp-checkpoint-600/config.json
[INFO|configuration_utils.py:594] 2024-05-14 13:18:33,163 >> Configuration saved in outputs/instruct_tuning/model/tmp-checkpoint-600/generation_config.json
[2024-05-14 13:18:59,491] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step600 is about to be saved!
[2024-05-14 13:18:59,545] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: outputs/instruct_tuning/model/tmp-checkpoint-600/global_step600/mp_rank_00_model_states.pt
[2024-05-14 13:18:59,545] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving outputs/instruct_tuning/model/tmp-checkpoint-600/global_step600/mp_rank_00_model_states.pt...
[INFO|modeling_utils.py:2390] 2024-05-14 13:18:58,748 >> The model is bigger than the maximum size per checkpoint (5GB) and is going to be split in 3 checkpoint shards. You can find where each parameters has been saved in the index located at outputs/instruct_tuning/model/tmp-checkpoint-600/model.safetensors.index.json.
[INFO|tokenization_utils_base.py:2432] 2024-05-14 13:18:58,749 >> tokenizer config file saved in outputs/instruct_tuning/model/tmp-checkpoint-600/tokenizer_config.json
[INFO|tokenization_utils_base.py:2441] 2024-05-14 13:18:58,750 >> Special tokens file saved in outputs/instruct_tuning/model/tmp-checkpoint-600/special_tokens_map.json
/mounts/work/lyk/anaconda3/envs/torch2/lib/python3.8/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
[2024-05-14 13:20:13,456] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved outputs/instruct_tuning/model/tmp-checkpoint-600/global_step600/mp_rank_00_model_states.pt.
[2024-05-14 13:20:13,458] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step600 is ready now!
[INFO|trainer.py:2979] 2024-05-14 13:20:13,496 >> Deleting older checkpoint [outputs/instruct_tuning/model/checkpoint-1200] due to args.save_total_limit
[2024-05-14 13:20:19,311] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 600
[2024-05-14 13:20:19,311] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:20:19,311] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
 19%|█▊        | 601/3218 [21:20<27:56:25, 38.44s/it]
[2024-05-14 13:20:20,917] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 601
[2024-05-14 13:20:20,918] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 19%|█▊        | 602/3218 [21:21<19:54:14, 27.39s/it]
[2024-05-14 13:20:22,466] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 602
[2024-05-14 13:20:22,467] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 19%|█▊        | 603/3218 [21:23<14:15:00, 19.62s/it]
[2024-05-14 13:20:24,487] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 603
[2024-05-14 13:20:24,488] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:20:24,489] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 13:20:26,097] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 604
[2024-05-14 13:20:26,098] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 19%|█▉        | 605/3218 [21:27<7:37:53, 10.51s/it]
[2024-05-14 13:20:27,406] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 605
[2024-05-14 13:20:27,406] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 19%|█▉        | 606/3218 [21:28<5:37:34,  7.75s/it]
[2024-05-14 13:20:28,766] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 606
[2024-05-14 13:20:28,766] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 19%|█▉        | 608/3218 [21:31<3:18:55,  4.57s/it]
[2024-05-14 13:20:30,369] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 607
[2024-05-14 13:20:30,370] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:20:30,370] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 13:20:31,700] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 608
[2024-05-14 13:20:31,700] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 19%|█▉        | 609/3218 [21:32<2:35:57,  3.59s/it]
[2024-05-14 13:20:32,962] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 609
[2024-05-14 13:20:32,963] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:20:32,964] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
{'loss': 0.0, 'learning_rate': 0.0, 'epoch': 0.19}
[2024-05-14 13:20:34,210] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 610
[2024-05-14 13:20:34,210] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 19%|█▉        | 611/3218 [21:35<1:43:49,  2.39s/it]
[2024-05-14 13:20:36,060] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 611
[2024-05-14 13:20:36,060] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 19%|█▉        | 612/3218 [21:37<1:37:33,  2.25s/it]
[2024-05-14 13:20:37,335] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 612
[2024-05-14 13:20:37,335] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 19%|█▉        | 613/3218 [21:38<1:24:37,  1.95s/it]
[2024-05-14 13:20:38,740] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 613
[2024-05-14 13:20:38,740] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:20:38,740] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 13:20:40,113] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 614
[2024-05-14 13:20:40,114] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 19%|█▉        | 615/3218 [21:41<1:12:19,  1.67s/it]
[2024-05-14 13:20:41,327] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 615
[2024-05-14 13:20:41,328] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 19%|█▉        | 616/3218 [21:42<1:06:23,  1.53s/it]
[2024-05-14 13:20:42,809] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 616
[2024-05-14 13:20:42,810] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:20:42,810] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 13:20:44,146] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 617
[2024-05-14 13:20:44,147] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 19%|█▉        | 618/3218 [21:45<1:03:05,  1.46s/it]
[2024-05-14 13:20:45,427] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 618
[2024-05-14 13:20:45,428] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 19%|█▉        | 619/3218 [21:46<1:00:16,  1.39s/it]
[2024-05-14 13:20:46,687] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 619
[2024-05-14 13:20:46,687] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:20:46,688] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
{'loss': 0.0, 'learning_rate': 0.0, 'epoch': 0.19}
[2024-05-14 13:20:47,761] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 620
[2024-05-14 13:20:47,761] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 19%|█▉        | 621/3218 [21:48<55:20,  1.28s/it]
[2024-05-14 13:20:48,998] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 621
[2024-05-14 13:20:48,998] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:20:48,999] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 13:20:50,155] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 622
[2024-05-14 13:20:50,155] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 19%|█▉        | 623/3218 [21:51<53:20,  1.23s/it]
[2024-05-14 13:20:51,224] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 623
[2024-05-14 13:20:51,224] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:20:51,225] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 13:20:52,266] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 624
[2024-05-14 13:20:52,267] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 19%|█▉        | 625/3218 [21:53<49:52,  1.15s/it]
[2024-05-14 13:20:53,595] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 625
[2024-05-14 13:20:53,596] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 19%|█▉        | 626/3218 [21:54<51:29,  1.19s/it]
[2024-05-14 13:20:55,101] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 626
[2024-05-14 13:20:55,102] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:20:55,102] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 13:20:56,083] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 627
[2024-05-14 13:20:56,083] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 20%|█▉        | 628/3218 [21:57<51:17,  1.19s/it]
[2024-05-14 13:20:57,085] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 628
[2024-05-14 13:20:57,086] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:20:57,086] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 13:20:58,093] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 629
[2024-05-14 13:20:58,094] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:20:58,094] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25

 20%|█▉        | 630/3218 [21:59<46:58,  1.09s/it]
[2024-05-14 13:20:59,737] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 630
[2024-05-14 13:20:59,737] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 20%|█▉        | 631/3218 [22:00<54:04,  1.25s/it]
[2024-05-14 13:21:01,095] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 631
[2024-05-14 13:21:01,095] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:21:01,095] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 13:21:01,850] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 632
[2024-05-14 13:21:01,850] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 20%|█▉        | 633/3218 [22:02<46:49,  1.09s/it]
[2024-05-14 13:21:03,054] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 633
[2024-05-14 13:21:03,054] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 20%|█▉        | 635/3218 [22:05<53:08,  1.23s/it]
[2024-05-14 13:21:04,409] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 634
[2024-05-14 13:21:04,410] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:21:04,410] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 13:21:05,715] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 635
[2024-05-14 13:21:05,715] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 20%|█▉        | 636/3218 [22:06<53:40,  1.25s/it]
[2024-05-14 13:21:07,063] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 636
[2024-05-14 13:21:07,063] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 20%|█▉        | 637/3218 [22:08<54:56,  1.28s/it]
[2024-05-14 13:21:08,536] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 637
[2024-05-14 13:21:08,537] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:21:08,538] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 13:21:09,764] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 638
[2024-05-14 13:21:09,765] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 20%|█▉        | 639/3218 [22:10<56:41,  1.32s/it]
[2024-05-14 13:21:11,086] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 639
[2024-05-14 13:21:11,086] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:21:11,087] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25

 20%|█▉        | 640/3218 [22:12<56:21,  1.31s/it]
[2024-05-14 13:21:12,645] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 640
[2024-05-14 13:21:12,645] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:21:12,645] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 13:21:14,241] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 641
[2024-05-14 13:21:14,241] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 20%|█▉        | 642/3218 [22:15<1:02:40,  1.46s/it]
[2024-05-14 13:21:15,494] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 642
[2024-05-14 13:21:15,495] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 20%|█▉        | 643/3218 [22:16<59:45,  1.39s/it]
[2024-05-14 13:21:16,792] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 643
[2024-05-14 13:21:16,793] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:21:16,793] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 13:21:18,060] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 644
[2024-05-14 13:21:18,061] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 20%|██        | 645/3218 [22:19<56:44,  1.32s/it]
[2024-05-14 13:21:19,944] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 645
[2024-05-14 13:21:19,945] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 20%|██        | 646/3218 [22:21<1:04:46,  1.51s/it]
[2024-05-14 13:21:21,308] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 646
[2024-05-14 13:21:21,308] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:21:21,308] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 13:21:22,411] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 647
[2024-05-14 13:21:22,412] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 20%|██        | 648/3218 [22:23<57:42,  1.35s/it]
[2024-05-14 13:21:23,565] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 648
[2024-05-14 13:21:23,565] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 20%|██        | 649/3218 [22:24<55:25,  1.29s/it]
[2024-05-14 13:21:25,062] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 649
[2024-05-14 13:21:25,062] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:21:25,062] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
{'loss': 0.0, 'learning_rate': 0.0, 'epoch': 0.2}
[2024-05-14 13:21:26,280] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 650
[2024-05-14 13:21:26,280] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 20%|██        | 651/3218 [22:27<56:12,  1.31s/it]
[2024-05-14 13:21:27,413] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 651
[2024-05-14 13:21:27,414] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 20%|██        | 652/3218 [22:28<53:25,  1.25s/it]
[2024-05-14 13:21:28,545] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 652
[2024-05-14 13:21:28,545] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:21:28,546] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 13:21:29,523] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 653
[2024-05-14 13:21:29,523] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 20%|██        | 654/3218 [22:30<48:48,  1.14s/it]
[2024-05-14 13:21:30,687] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 654
[2024-05-14 13:21:30,688] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:21:30,688] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 13:21:31,636] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 655
[2024-05-14 13:21:31,636] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 20%|██        | 656/3218 [22:32<46:32,  1.09s/it]
[2024-05-14 13:21:32,700] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 656
[2024-05-14 13:21:32,700] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:21:32,701] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 13:21:33,785] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 657
[2024-05-14 13:21:33,785] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 20%|██        | 658/3218 [22:34<46:09,  1.08s/it]
[2024-05-14 13:21:34,723] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 658
[2024-05-14 13:21:34,723] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:21:34,724] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 13:21:35,808] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 659
[2024-05-14 13:21:35,809] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:21:35,809] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25

 21%|██        | 660/3218 [22:36<45:08,  1.06s/it]
[2024-05-14 13:21:37,310] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 660
[2024-05-14 13:21:37,310] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:21:37,312] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 13:21:38,173] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 661
[2024-05-14 13:21:38,174] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 21%|██        | 662/3218 [22:39<46:25,  1.09s/it]
[2024-05-14 13:21:39,044] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 662
[2024-05-14 13:21:39,044] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:21:39,044] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 13:21:40,304] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 663
[2024-05-14 13:21:40,304] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 21%|██        | 664/3218 [22:41<46:49,  1.10s/it]
[2024-05-14 13:21:41,288] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 664
[2024-05-14 13:21:41,289] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 21%|██        | 665/3218 [22:42<44:42,  1.05s/it]
[2024-05-14 13:21:42,923] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 665
[2024-05-14 13:21:42,923] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:21:42,923] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 13:21:44,344] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 666
[2024-05-14 13:21:44,344] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 21%|██        | 667/3218 [22:45<55:00,  1.29s/it]
[2024-05-14 13:21:45,202] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 667
[2024-05-14 13:21:45,202] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:21:45,202] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 13:21:46,465] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 668
[2024-05-14 13:21:46,466] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 21%|██        | 669/3218 [22:47<51:04,  1.20s/it]
[2024-05-14 13:21:48,294] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 669
[2024-05-14 13:21:48,295] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 21%|██        | 670/3218 [22:49<59:26,  1.40s/it]
{'loss': 0.0, 'learning_rate': 0.0, 'epoch': 0.21}
[2024-05-14 13:21:50,633] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 670
[2024-05-14 13:21:50,634] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:21:50,634] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 13:21:52,233] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 671
[2024-05-14 13:21:52,233] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 21%|██        | 672/3218 [22:53<1:09:44,  1.64s/it]
[2024-05-14 13:21:53,752] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 672
[2024-05-14 13:21:53,752] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 21%|██        | 673/3218 [22:54<1:08:03,  1.60s/it]
[2024-05-14 13:21:55,390] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 673
[2024-05-14 13:21:55,390] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 21%|██        | 674/3218 [22:56<1:08:55,  1.63s/it]
[2024-05-14 13:21:56,790] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 674
[2024-05-14 13:21:56,790] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:21:56,791] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 13:21:58,104] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 675
[2024-05-14 13:21:58,104] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 21%|██        | 676/3218 [22:59<1:02:35,  1.48s/it]
[2024-05-14 13:21:59,345] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 676
[2024-05-14 13:21:59,345] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 21%|██        | 677/3218 [23:00<59:40,  1.41s/it]
[2024-05-14 13:22:00,988] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 677
[2024-05-14 13:22:00,989] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:22:00,990] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 13:22:02,336] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 678
[2024-05-14 13:22:02,336] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 21%|██        | 679/3218 [23:03<1:01:01,  1.44s/it]
[2024-05-14 13:22:03,402] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 679
[2024-05-14 13:22:03,403] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:22:03,404] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25

 21%|██        | 680/3218 [23:04<56:13,  1.33s/it]
[2024-05-14 13:22:05,023] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 680
[2024-05-14 13:22:05,024] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 21%|██        | 681/3218 [23:06<1:00:14,  1.42s/it]
[2024-05-14 13:22:06,555] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 681
[2024-05-14 13:22:06,555] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:22:06,556] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 13:22:08,179] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 682
[2024-05-14 13:22:08,179] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 21%|██        | 683/3218 [23:09<1:03:21,  1.50s/it]
[2024-05-14 13:22:09,163] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 683
[2024-05-14 13:22:09,164] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 21%|██▏       | 684/3218 [23:10<56:21,  1.33s/it]
[2024-05-14 13:22:10,636] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 684
[2024-05-14 13:22:10,636] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:22:10,638] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 13:22:11,744] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 685
[2024-05-14 13:22:11,745] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 21%|██▏       | 686/3218 [23:12<54:32,  1.29s/it]
[2024-05-14 13:22:13,050] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 686
[2024-05-14 13:22:13,051] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:22:13,052] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 13:22:14,273] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 687
[2024-05-14 13:22:14,274] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 21%|██▏       | 688/3218 [23:15<53:22,  1.27s/it]
[2024-05-14 13:22:15,337] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 688
[2024-05-14 13:22:15,337] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 21%|██▏       | 689/3218 [23:16<51:08,  1.21s/it]
[2024-05-14 13:22:16,735] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 689
[2024-05-14 13:22:16,736] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:22:16,740] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
{'loss': 0.0, 'learning_rate': 0.0, 'epoch': 0.21}
[2024-05-14 13:22:17,907] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 690
[2024-05-14 13:22:17,907] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 21%|██▏       | 691/3218 [23:18<52:35,  1.25s/it]
[2024-05-14 13:22:19,367] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 691
[2024-05-14 13:22:19,368] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 22%|██▏       | 692/3218 [23:20<55:40,  1.32s/it]
[2024-05-14 13:22:20,875] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 692
[2024-05-14 13:22:20,875] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:22:20,876] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 13:22:22,305] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 693
[2024-05-14 13:22:22,305] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 22%|██▏       | 694/3218 [23:23<58:34,  1.39s/it]
[2024-05-14 13:22:23,324] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 694
[2024-05-14 13:22:23,325] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:22:23,325] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 13:22:24,288] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 695
[2024-05-14 13:22:24,289] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 22%|██▏       | 696/3218 [23:25<49:38,  1.18s/it]
[2024-05-14 13:22:25,479] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 696
[2024-05-14 13:22:25,479] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:22:25,480] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 13:22:26,464] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 697
[2024-05-14 13:22:26,465] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 22%|██▏       | 698/3218 [23:27<47:04,  1.12s/it]
[2024-05-14 13:22:27,605] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 698
[2024-05-14 13:22:27,605] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:22:27,605] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 13:22:28,423] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 699
[2024-05-14 13:22:28,424] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 22%|██▏       | 700/3218 [23:29<42:53,  1.02s/it]
 22%|██▏       | 700/3218 [23:29<42:53,  1.02s/it][INFO|trainer.py:3166] 2024-05-14 13:22:28,692 >> ***** Running Evaluation *****
[INFO|trainer.py:3168] 2024-05-14 13:22:28,692 >>   Num examples = 521
[INFO|trainer.py:3171] 2024-05-14 13:22:28,692 >>   Batch size = 8





100%|██████████| 33/33 [00:11<00:00,  2.32it/s]
{'eval_loss': nan, 'eval_runtime': 12.396, 'eval_samples_per_second': 42.03, 'eval_steps_per_second': 2.662, 'epoch': 0.22}
[2024-05-14 13:22:42,505] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 700
[2024-05-14 13:22:42,505] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

[2024-05-14 13:22:42,505] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 13:22:44,044] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 701
[2024-05-14 13:22:44,044] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 22%|██▏       | 702/3218 [23:45<2:44:48,  3.93s/it]
[2024-05-14 13:22:44,908] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 702
[2024-05-14 13:22:44,909] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:22:44,909] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 13:22:46,282] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 703
[2024-05-14 13:22:46,282] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 22%|██▏       | 704/3218 [23:47<1:46:36,  2.54s/it]
[2024-05-14 13:22:47,822] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 704
[2024-05-14 13:22:47,823] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 22%|██▏       | 705/3218 [23:48<1:33:32,  2.23s/it]
[2024-05-14 13:22:49,720] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 705
[2024-05-14 13:22:49,721] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 22%|██▏       | 706/3218 [23:50<1:29:03,  2.13s/it]
[2024-05-14 13:22:51,224] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 706
[2024-05-14 13:22:51,224] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 22%|██▏       | 707/3218 [23:52<1:21:22,  1.94s/it]
[2024-05-14 13:22:52,969] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 707
[2024-05-14 13:22:52,970] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:22:52,970] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 13:22:54,574] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 708
[2024-05-14 13:22:54,574] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 22%|██▏       | 709/3218 [23:55<1:15:17,  1.80s/it]
[2024-05-14 13:22:56,044] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 709
[2024-05-14 13:22:56,044] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:22:56,044] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25

 22%|██▏       | 710/3218 [23:57<1:11:03,  1.70s/it]
[2024-05-14 13:22:57,776] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 710
[2024-05-14 13:22:57,777] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 22%|██▏       | 711/3218 [23:58<1:11:50,  1.72s/it]
[2024-05-14 13:22:59,777] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 711
[2024-05-14 13:22:59,778] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 22%|██▏       | 712/3218 [24:00<1:15:48,  1.82s/it]
[2024-05-14 13:23:01,166] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 712
[2024-05-14 13:23:01,166] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:23:01,168] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 13:23:02,496] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 713
[2024-05-14 13:23:02,496] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 22%|██▏       | 714/3218 [24:03<1:05:14,  1.56s/it]
[2024-05-14 13:23:03,916] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 714
[2024-05-14 13:23:03,917] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 22%|██▏       | 715/3218 [24:04<1:03:28,  1.52s/it]
[2024-05-14 13:23:05,127] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 715
[2024-05-14 13:23:05,127] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 22%|██▏       | 716/3218 [24:06<59:27,  1.43s/it]
[2024-05-14 13:23:06,692] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 716
[2024-05-14 13:23:06,693] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:23:06,696] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 13:23:08,443] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 717
[2024-05-14 13:23:08,443] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 22%|██▏       | 718/3218 [24:09<1:04:59,  1.56s/it]
[2024-05-14 13:23:09,908] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 718
[2024-05-14 13:23:09,909] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 22%|██▏       | 719/3218 [24:10<1:03:33,  1.53s/it]
[2024-05-14 13:23:11,191] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 719
[2024-05-14 13:23:11,192] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:23:11,192] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25

 22%|██▏       | 720/3218 [24:12<1:00:16,  1.45s/it]
[2024-05-14 13:23:12,720] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 720
[2024-05-14 13:23:12,720] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:23:12,720] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 13:23:14,134] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 721
[2024-05-14 13:23:14,134] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 22%|██▏       | 722/3218 [24:15<1:00:41,  1.46s/it]
[2024-05-14 13:23:15,501] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 722
[2024-05-14 13:23:15,502] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:23:15,504] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 13:23:16,635] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 723
[2024-05-14 13:23:16,636] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 22%|██▏       | 724/3218 [24:17<55:33,  1.34s/it]
[2024-05-14 13:23:17,980] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 724
[2024-05-14 13:23:17,980] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 23%|██▎       | 725/3218 [24:19<56:02,  1.35s/it]
[2024-05-14 13:23:19,161] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 725
[2024-05-14 13:23:19,162] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:23:19,162] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 13:23:20,345] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 726
[2024-05-14 13:23:20,345] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 23%|██▎       | 727/3218 [24:21<52:25,  1.26s/it]
[2024-05-14 13:23:21,449] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 727
[2024-05-14 13:23:21,450] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:23:21,451] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 13:23:22,532] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 728
[2024-05-14 13:23:22,533] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 23%|██▎       | 729/3218 [24:23<48:33,  1.17s/it]
[2024-05-14 13:23:23,705] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 729
[2024-05-14 13:23:23,705] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:23:23,707] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25

 23%|██▎       | 730/3218 [24:24<48:44,  1.18s/it]
[2024-05-14 13:23:24,739] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 730
[2024-05-14 13:23:24,740] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:23:24,742] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 13:23:25,728] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 731
[2024-05-14 13:23:25,728] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 23%|██▎       | 732/3218 [24:26<45:05,  1.09s/it]
[2024-05-14 13:23:27,037] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 732
[2024-05-14 13:23:27,037] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:23:27,042] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 13:23:28,079] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 733
[2024-05-14 13:23:28,079] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 23%|██▎       | 734/3218 [24:29<46:36,  1.13s/it]
[2024-05-14 13:23:28,884] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 734
[2024-05-14 13:23:28,884] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:23:28,884] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 13:23:30,120] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 735
[2024-05-14 13:23:30,121] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 23%|██▎       | 736/3218 [24:31<45:25,  1.10s/it]
[2024-05-14 13:23:31,590] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 736
[2024-05-14 13:23:31,590] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:23:31,591] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 13:23:32,328] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 737
[2024-05-14 13:23:32,328] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 23%|██▎       | 738/3218 [24:33<42:07,  1.02s/it]
[2024-05-14 13:23:33,302] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 738
[2024-05-14 13:23:33,303] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 23%|██▎       | 739/3218 [24:34<43:42,  1.06s/it]
[2024-05-14 13:23:34,884] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 739
[2024-05-14 13:23:34,885] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:23:34,886] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
{'loss': 0.0, 'learning_rate': 0.0, 'epoch': 0.23}
[2024-05-14 13:23:36,383] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 740
[2024-05-14 13:23:36,383] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 23%|██▎       | 741/3218 [24:37<53:34,  1.30s/it]
[2024-05-14 13:23:37,960] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 741
[2024-05-14 13:23:37,960] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 23%|██▎       | 742/3218 [24:39<57:41,  1.40s/it]
[2024-05-14 13:23:39,740] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 742
[2024-05-14 13:23:39,740] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 23%|██▎       | 743/3218 [24:40<1:02:02,  1.50s/it]
[2024-05-14 13:23:41,394] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 743
[2024-05-14 13:23:41,394] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:23:41,394] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 13:23:42,639] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 744
[2024-05-14 13:23:42,640] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 23%|██▎       | 745/3218 [24:43<59:48,  1.45s/it]
[2024-05-14 13:23:44,128] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 745
[2024-05-14 13:23:44,128] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 23%|██▎       | 746/3218 [24:45<1:00:24,  1.47s/it]
[2024-05-14 13:23:45,541] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 746
[2024-05-14 13:23:45,541] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 23%|██▎       | 747/3218 [24:46<59:31,  1.45s/it]
[2024-05-14 13:23:47,723] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 747
[2024-05-14 13:23:47,723] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 23%|██▎       | 748/3218 [24:48<1:09:06,  1.68s/it]
[2024-05-14 13:23:49,111] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 748
[2024-05-14 13:23:49,111] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:23:49,112] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 13:23:50,390] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 749
[2024-05-14 13:23:50,391] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:23:50,391] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25

 23%|██▎       | 750/3218 [24:51<1:01:25,  1.49s/it]
[2024-05-14 13:23:51,838] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 750
[2024-05-14 13:23:51,839] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 23%|██▎       | 751/3218 [24:52<1:00:27,  1.47s/it]
[2024-05-14 13:23:53,134] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 751
[2024-05-14 13:23:53,135] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:23:53,135] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 13:23:54,597] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 752
[2024-05-14 13:23:54,598] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 23%|██▎       | 753/3218 [24:55<59:10,  1.44s/it]
[2024-05-14 13:23:56,102] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 753
[2024-05-14 13:23:56,103] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 23%|██▎       | 754/3218 [24:57<59:43,  1.45s/it]
[2024-05-14 13:23:57,465] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 754
[2024-05-14 13:23:57,466] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 23%|██▎       | 756/3218 [24:59<55:07,  1.34s/it]
[2024-05-14 13:23:58,586] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 755
[2024-05-14 13:23:58,587] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:23:58,587] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 13:23:59,818] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 756
[2024-05-14 13:23:59,818] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 24%|██▎       | 757/3218 [25:00<53:22,  1.30s/it]
[2024-05-14 13:24:01,058] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 757
[2024-05-14 13:24:01,058] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:24:01,058] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 13:24:02,485] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 758
[2024-05-14 13:24:02,485] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 24%|██▎       | 759/3218 [25:03<54:24,  1.33s/it]
[2024-05-14 13:24:03,612] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 759
[2024-05-14 13:24:03,613] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:24:03,613] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25

 24%|██▎       | 760/3218 [25:04<52:02,  1.27s/it]
[2024-05-14 13:24:04,770] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 760
[2024-05-14 13:24:04,771] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:24:04,771] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 13:24:05,934] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 761
[2024-05-14 13:24:05,935] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 24%|██▎       | 762/3218 [25:06<49:37,  1.21s/it]
[2024-05-14 13:24:06,984] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 762
[2024-05-14 13:24:06,984] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:24:06,984] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 13:24:08,638] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 763
[2024-05-14 13:24:08,639] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 24%|██▎       | 764/3218 [25:09<53:25,  1.31s/it]
[2024-05-14 13:24:09,789] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 764
[2024-05-14 13:24:09,789] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 24%|██▍       | 765/3218 [25:10<51:50,  1.27s/it]
[2024-05-14 13:24:10,957] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 765
[2024-05-14 13:24:10,957] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:24:10,957] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 13:24:11,997] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 766
[2024-05-14 13:24:11,997] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 24%|██▍       | 767/3218 [25:13<47:44,  1.17s/it]
[2024-05-14 13:24:12,899] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 767
[2024-05-14 13:24:12,900] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:24:12,900] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 13:24:13,882] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 768
[2024-05-14 13:24:13,883] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 24%|██▍       | 769/3218 [25:14<43:01,  1.05s/it]
[2024-05-14 13:24:14,823] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 769
[2024-05-14 13:24:14,824] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:24:14,824] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
{'loss': 0.0, 'learning_rate': 0.0, 'epoch': 0.24}
[2024-05-14 13:24:16,284] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 770
[2024-05-14 13:24:16,284] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 24%|██▍       | 771/3218 [25:17<46:34,  1.14s/it]
[2024-05-14 13:24:17,743] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 771
[2024-05-14 13:24:17,743] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:24:17,743] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 13:24:18,657] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 772
[2024-05-14 13:24:18,658] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 24%|██▍       | 773/3218 [25:19<44:55,  1.10s/it]
[2024-05-14 13:24:20,504] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 773
[2024-05-14 13:24:20,504] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 24%|██▍       | 774/3218 [25:21<56:11,  1.38s/it]
[2024-05-14 13:24:22,289] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 774
[2024-05-14 13:24:22,290] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 24%|██▍       | 775/3218 [25:23<1:00:53,  1.50s/it]
[2024-05-14 13:24:24,225] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 775
[2024-05-14 13:24:24,226] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 24%|██▍       | 776/3218 [25:25<1:06:38,  1.64s/it]
[2024-05-14 13:24:26,074] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 776
[2024-05-14 13:24:26,074] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 24%|██▍       | 777/3218 [25:27<1:08:53,  1.69s/it]
[2024-05-14 13:24:27,515] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 777
[2024-05-14 13:24:27,515] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 24%|██▍       | 778/3218 [25:28<1:05:42,  1.62s/it]
[2024-05-14 13:24:29,025] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 778
[2024-05-14 13:24:29,025] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:24:29,025] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 13:24:30,508] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 779
[2024-05-14 13:24:30,508] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:24:30,509] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25

 24%|██▍       | 780/3218 [25:31<1:02:47,  1.55s/it]
[2024-05-14 13:24:32,018] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 780
[2024-05-14 13:24:32,018] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 24%|██▍       | 781/3218 [25:33<1:02:46,  1.55s/it]
[2024-05-14 13:24:34,270] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 781
[2024-05-14 13:24:34,271] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 24%|██▍       | 782/3218 [25:35<1:11:57,  1.77s/it]
[2024-05-14 13:24:35,754] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 782
[2024-05-14 13:24:35,754] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 24%|██▍       | 783/3218 [25:36<1:07:58,  1.67s/it]
[2024-05-14 13:24:37,433] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 783
[2024-05-14 13:24:37,433] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 24%|██▍       | 784/3218 [25:38<1:08:13,  1.68s/it]
[2024-05-14 13:24:38,869] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 784
[2024-05-14 13:24:38,870] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:24:38,870] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 13:24:40,281] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 785
[2024-05-14 13:24:40,281] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 24%|██▍       | 786/3218 [25:41<1:02:26,  1.54s/it]
[2024-05-14 13:24:41,946] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 786
[2024-05-14 13:24:41,946] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 24%|██▍       | 787/3218 [25:42<1:03:36,  1.57s/it]
[2024-05-14 13:24:43,446] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 787
[2024-05-14 13:24:43,446] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:24:43,449] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 13:24:44,749] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 788
[2024-05-14 13:24:44,750] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 25%|██▍       | 789/3218 [25:45<59:27,  1.47s/it]
[2024-05-14 13:24:46,377] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 789
[2024-05-14 13:24:46,377] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:24:46,379] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25

 25%|██▍       | 790/3218 [25:47<1:02:37,  1.55s/it]
[2024-05-14 13:24:47,635] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 790
[2024-05-14 13:24:47,636] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 25%|██▍       | 791/3218 [25:48<58:21,  1.44s/it]
[2024-05-14 13:24:48,976] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 791
[2024-05-14 13:24:48,977] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:24:48,977] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 13:24:49,992] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 792
[2024-05-14 13:24:49,992] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 25%|██▍       | 793/3218 [25:51<52:17,  1.29s/it]
[2024-05-14 13:24:51,502] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 793
[2024-05-14 13:24:51,502] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 25%|██▍       | 794/3218 [25:52<54:45,  1.36s/it]
[2024-05-14 13:24:53,272] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 794
[2024-05-14 13:24:53,272] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:24:53,273] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 13:24:54,587] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 795
[2024-05-14 13:24:54,588] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 25%|██▍       | 796/3218 [25:55<58:09,  1.44s/it]
[2024-05-14 13:24:56,226] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 796
[2024-05-14 13:24:56,227] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 25%|██▍       | 797/3218 [25:57<1:00:23,  1.50s/it]
[2024-05-14 13:24:57,530] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 797
[2024-05-14 13:24:57,530] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:24:57,530] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 13:24:58,743] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 798
[2024-05-14 13:24:58,744] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 25%|██▍       | 799/3218 [25:59<54:59,  1.36s/it]
[2024-05-14 13:25:00,308] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 799
[2024-05-14 13:25:00,309] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:25:00,310] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
 25%|██▍       | 800/3218 [26:01<58:07,  1.44s/it][INFO|trainer.py:3166] 2024-05-14 13:25:00,529 >> ***** Running Evaluation *****
[INFO|trainer.py:3168] 2024-05-14 13:25:00,529 >>   Num examples = 521
[INFO|trainer.py:3171] 2024-05-14 13:25:00,529 >>   Batch size = 8






 88%|████████▊ | 29/33 [00:10<00:01,  2.81it/s]
 25%|██▍       | 800/3218 [26:13<58:07,  1.44s/[INFO|trainer.py:2889] 2024-05-14 13:25:17,985 >> Saving model checkpoint to outputs/instruct_tuning/model/tmp-checkpoint-800
[INFO|configuration_utils.py:483] 2024-05-14 13:25:17,987 >> Configuration saved in outputs/instruct_tuning/model/tmp-checkpoint-800/config.json
[INFO|configuration_utils.py:594] 2024-05-14 13:25:17,988 >> Configuration saved in outputs/instruct_tuning/model/tmp-checkpoint-800/generation_config.json
[2024-05-14 13:25:42,468] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step800 is about to be saved!
[2024-05-14 13:25:42,474] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: outputs/instruct_tuning/model/tmp-checkpoint-800/global_step800/mp_rank_00_model_states.pt
[2024-05-14 13:25:42,475] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving outputs/instruct_tuning/model/tmp-checkpoint-800/global_step800/mp_rank_00_model_states.pt...
[INFO|modeling_utils.py:2390] 2024-05-14 13:25:41,767 >> The model is bigger than the maximum size per checkpoint (5GB) and is going to be split in 3 checkpoint shards. You can find where each parameters has been saved in the index located at outputs/instruct_tuning/model/tmp-checkpoint-800/model.safetensors.index.json.
[INFO|tokenization_utils_base.py:2432] 2024-05-14 13:25:41,768 >> tokenizer config file saved in outputs/instruct_tuning/model/tmp-checkpoint-800/tokenizer_config.json
[INFO|tokenization_utils_base.py:2441] 2024-05-14 13:25:41,769 >> Special tokens file saved in outputs/instruct_tuning/model/tmp-checkpoint-800/special_tokens_map.json
/mounts/work/lyk/anaconda3/envs/torch2/lib/python3.8/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
[2024-05-14 13:26:53,100] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved outputs/instruct_tuning/model/tmp-checkpoint-800/global_step800/mp_rank_00_model_states.pt.
[2024-05-14 13:26:53,101] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step800 is ready now!
[INFO|trainer.py:2979] 2024-05-14 13:26:53,109 >> Deleting older checkpoint [outputs/instruct_tuning/model/checkpoint-400] due to args.save_total_limit
[2024-05-14 13:26:57,360] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 800
[2024-05-14 13:26:57,361] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:26:57,362] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 13:26:58,466] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 801
[2024-05-14 13:26:58,466] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:26:58,468] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
 25%|██▍       | 802/3218 [27:59<17:10:53, 25.60s/it]
[2024-05-14 13:26:59,567] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 802
[2024-05-14 13:26:59,568] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:26:59,568] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 13:27:00,653] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 803
[2024-05-14 13:27:00,654] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 25%|██▍       | 804/3218 [28:01<8:47:04, 13.10s/it]
[2024-05-14 13:27:01,718] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 804
[2024-05-14 13:27:01,719] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 25%|██▌       | 805/3218 [28:02<6:21:23,  9.48s/it]
[2024-05-14 13:27:03,362] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 805
[2024-05-14 13:27:03,363] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 25%|██▌       | 806/3218 [28:04<4:46:42,  7.13s/it]
[2024-05-14 13:27:05,412] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 806
[2024-05-14 13:27:05,413] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:27:05,413] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 13:27:06,122] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 807
[2024-05-14 13:27:06,122] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 25%|██▌       | 808/3218 [28:07<2:44:32,  4.10s/it]
[2024-05-14 13:27:07,475] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 808
[2024-05-14 13:27:07,476] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 25%|██▌       | 809/3218 [28:08<2:13:09,  3.32s/it]
[2024-05-14 13:27:09,211] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 809
[2024-05-14 13:27:09,212] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:27:09,212] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25

 25%|██▌       | 810/3218 [28:10<1:54:14,  2.85s/it]
[2024-05-14 13:27:11,128] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 810
[2024-05-14 13:27:11,128] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:27:11,128] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 13:27:12,657] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 811
[2024-05-14 13:27:12,657] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 25%|██▌       | 812/3218 [28:13<1:30:19,  2.25s/it]
[2024-05-14 13:27:14,573] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 812
[2024-05-14 13:27:14,573] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 25%|██▌       | 813/3218 [28:15<1:26:34,  2.16s/it]
[2024-05-14 13:27:16,467] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 813
[2024-05-14 13:27:16,467] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 25%|██▌       | 814/3218 [28:17<1:23:37,  2.09s/it]
[2024-05-14 13:27:18,572] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 814
[2024-05-14 13:27:18,572] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 25%|██▌       | 815/3218 [28:19<1:24:06,  2.10s/it]
[2024-05-14 13:27:20,150] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 815
[2024-05-14 13:27:20,151] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 25%|██▌       | 816/3218 [28:21<1:16:55,  1.92s/it]
[2024-05-14 13:27:21,594] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 816
[2024-05-14 13:27:21,594] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 25%|██▌       | 817/3218 [28:22<1:11:18,  1.78s/it]
[2024-05-14 13:27:23,473] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 817
[2024-05-14 13:27:23,474] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:27:23,475] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 13:27:24,984] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 818
[2024-05-14 13:27:24,984] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 25%|██▌       | 818/3218 [28:24<1:12:19,  1.81s/it]
[2024-05-14 13:27:26,442] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 819
[2024-05-14 13:27:26,443] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:27:26,443] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25

 25%|██▌       | 820/3218 [28:27<1:05:15,  1.63s/it]
[2024-05-14 13:27:27,904] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 820
[2024-05-14 13:27:27,904] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 26%|██▌       | 821/3218 [28:28<1:03:42,  1.59s/it]
[2024-05-14 13:27:29,425] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 821
[2024-05-14 13:27:29,426] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:27:29,426] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 13:27:31,105] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 822
[2024-05-14 13:27:31,106] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 26%|██▌       | 822/3218 [28:30<1:02:39,  1.57s/it]
[2024-05-14 13:27:32,370] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 823
[2024-05-14 13:27:32,370] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 26%|██▌       | 824/3218 [28:33<59:37,  1.49s/it]
[2024-05-14 13:27:33,575] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 824
[2024-05-14 13:27:33,576] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:27:33,576] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 13:27:34,881] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 825
[2024-05-14 13:27:34,882] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 26%|██▌       | 826/3218 [28:35<55:09,  1.38s/it]
[2024-05-14 13:27:36,277] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 826
[2024-05-14 13:27:36,278] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 26%|██▌       | 827/3218 [28:37<54:53,  1.38s/it]
[2024-05-14 13:27:37,499] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 827
[2024-05-14 13:27:37,500] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:27:37,500] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 13:27:38,951] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 828
[2024-05-14 13:27:38,951] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 26%|██▌       | 829/3218 [28:40<54:46,  1.38s/it]
[2024-05-14 13:27:40,054] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 829
[2024-05-14 13:27:40,055] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:27:40,055] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25

 26%|██▌       | 830/3218 [28:41<51:10,  1.29s/it]
[2024-05-14 13:27:41,282] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 830
[2024-05-14 13:27:41,283] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:27:41,283] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 13:27:42,432] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 831
[2024-05-14 13:27:42,433] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 26%|██▌       | 832/3218 [28:43<49:12,  1.24s/it]
[2024-05-14 13:27:43,593] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 832
[2024-05-14 13:27:43,593] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:27:43,593] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 13:27:44,964] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 833
[2024-05-14 13:27:44,965] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 26%|██▌       | 834/3218 [28:45<49:51,  1.25s/it]
[2024-05-14 13:27:46,120] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 834
[2024-05-14 13:27:46,121] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 26%|██▌       | 835/3218 [28:47<49:03,  1.24s/it]
[2024-05-14 13:27:47,380] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 835
[2024-05-14 13:27:47,380] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:27:47,380] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 13:27:48,604] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 836
[2024-05-14 13:27:48,605] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 26%|██▌       | 837/3218 [28:49<48:57,  1.23s/it]
[2024-05-14 13:27:49,759] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 837
[2024-05-14 13:27:49,759] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:27:49,759] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 13:27:50,983] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 838
[2024-05-14 13:27:50,984] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 26%|██▌       | 839/3218 [28:51<47:45,  1.20s/it]
[2024-05-14 13:27:51,999] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 839
[2024-05-14 13:27:51,999] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:27:51,999] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25

 26%|██▌       | 840/3218 [28:53<45:40,  1.15s/it]
[2024-05-14 13:27:53,467] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 840
[2024-05-14 13:27:53,468] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 26%|██▌       | 841/3218 [28:54<49:23,  1.25s/it]
[2024-05-14 13:27:56,072] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 841
[2024-05-14 13:27:56,072] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:27:56,073] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 13:27:56,926] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 842
[2024-05-14 13:27:56,926] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 26%|██▌       | 843/3218 [28:57<54:11,  1.37s/it]
[2024-05-14 13:27:58,140] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 843
[2024-05-14 13:27:58,140] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 26%|██▌       | 844/3218 [28:59<54:42,  1.38s/it]
[2024-05-14 13:27:59,788] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 844
[2024-05-14 13:27:59,788] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:27:59,789] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 13:28:01,217] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 845
[2024-05-14 13:28:01,218] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 26%|██▋       | 845/3218 [29:00<57:43,  1.46s/it]
[2024-05-14 13:28:03,102] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 846
[2024-05-14 13:28:03,102] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 26%|██▋       | 846/3218 [29:02<57:01,  1.44s/it]
[2024-05-14 13:28:04,671] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 847
[2024-05-14 13:28:04,671] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 26%|██▋       | 848/3218 [29:05<1:02:14,  1.58s/it]
[2024-05-14 13:28:06,037] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 848
[2024-05-14 13:28:06,038] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 26%|██▋       | 849/3218 [29:07<59:38,  1.51s/it]
[2024-05-14 13:28:07,461] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 849
[2024-05-14 13:28:07,462] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:28:07,463] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
{'loss': 0.0, 'learning_rate': 0.0, 'epoch': 0.26}
[2024-05-14 13:28:08,966] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 850
[2024-05-14 13:28:08,966] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 26%|██▋       | 851/3218 [29:10<58:57,  1.49s/it]
[2024-05-14 13:28:10,398] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 851
[2024-05-14 13:28:10,398] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 26%|██▋       | 852/3218 [29:11<58:42,  1.49s/it]
[2024-05-14 13:28:11,781] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 852
[2024-05-14 13:28:11,782] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:28:11,783] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 13:28:13,183] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 853
[2024-05-14 13:28:13,184] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 27%|██▋       | 853/3218 [29:12<57:00,  1.45s/it]
[2024-05-14 13:28:14,779] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 854
[2024-05-14 13:28:14,780] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 27%|██▋       | 855/3218 [29:15<57:58,  1.47s/it]
[2024-05-14 13:28:16,130] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 855
[2024-05-14 13:28:16,131] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 27%|██▋       | 856/3218 [29:17<57:04,  1.45s/it]
[2024-05-14 13:28:17,600] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 856
[2024-05-14 13:28:17,600] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:28:17,600] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 13:28:19,092] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 857
[2024-05-14 13:28:19,092] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 27%|██▋       | 857/3218 [29:18<57:00,  1.45s/it]
[2024-05-14 13:28:20,214] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 858
[2024-05-14 13:28:20,214] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 27%|██▋       | 859/3218 [29:21<53:16,  1.36s/it]
[2024-05-14 13:28:21,553] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 859
[2024-05-14 13:28:21,554] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:28:21,554] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
{'loss': 0.0, 'learning_rate': 0.0, 'epoch': 0.27}
[2024-05-14 13:28:22,662] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 860
[2024-05-14 13:28:22,662] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 27%|██▋       | 861/3218 [29:23<50:10,  1.28s/it]
[2024-05-14 13:28:23,819] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 861
[2024-05-14 13:28:23,820] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 27%|██▋       | 862/3218 [29:24<48:30,  1.24s/it]
[2024-05-14 13:28:25,362] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 862
[2024-05-14 13:28:25,362] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:28:25,366] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 13:28:27,152] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 863
[2024-05-14 13:28:27,152] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 27%|██▋       | 863/3218 [29:26<52:35,  1.34s/it]
[2024-05-14 13:28:28,531] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 864
[2024-05-14 13:28:28,532] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 27%|██▋       | 865/3218 [29:29<56:24,  1.44s/it]
[2024-05-14 13:28:29,463] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 865
[2024-05-14 13:28:29,464] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:28:29,464] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 13:28:30,583] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 866
[2024-05-14 13:28:30,583] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 27%|██▋       | 867/3218 [29:31<48:28,  1.24s/it]
[2024-05-14 13:28:31,661] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 867
[2024-05-14 13:28:31,661] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:28:31,661] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 13:28:33,073] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 868
[2024-05-14 13:28:33,073] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 27%|██▋       | 869/3218 [29:34<48:54,  1.25s/it]
[2024-05-14 13:28:34,173] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 869
[2024-05-14 13:28:34,173] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:28:34,173] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25

 27%|██▋       | 870/3218 [29:35<47:28,  1.21s/it]
[2024-05-14 13:28:35,381] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 870
[2024-05-14 13:28:35,381] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:28:35,381] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 13:28:36,355] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 871
[2024-05-14 13:28:36,356] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 27%|██▋       | 872/3218 [29:37<44:20,  1.13s/it]
[2024-05-14 13:28:37,339] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 872
[2024-05-14 13:28:37,340] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:28:37,340] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 13:28:38,200] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 873
[2024-05-14 13:28:38,200] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:28:38,200] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 13:28:39,101] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 874
[2024-05-14 13:28:39,101] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 27%|██▋       | 875/3218 [29:40<38:05,  1.03it/s]
[2024-05-14 13:28:40,918] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 875
[2024-05-14 13:28:40,918] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 27%|██▋       | 876/3218 [29:41<48:01,  1.23s/it]
[2024-05-14 13:28:43,245] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 876
[2024-05-14 13:28:43,246] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:28:43,246] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 13:28:44,022] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 877
[2024-05-14 13:28:44,022] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 27%|██▋       | 878/3218 [29:44<49:56,  1.28s/it]
[2024-05-14 13:28:45,599] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 878
[2024-05-14 13:28:45,599] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:28:45,600] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 13:28:47,332] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 879
[2024-05-14 13:28:47,333] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 27%|██▋       | 879/3218 [29:46<54:56,  1.41s/it]

 27%|██▋       | 880/3218 [29:48<58:37,  1.50s/it]
[2024-05-14 13:28:49,646] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 880
[2024-05-14 13:28:49,646] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:28:49,647] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 13:28:51,295] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 881
[2024-05-14 13:28:51,295] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25


 27%|██▋       | 882/3218 [29:52<1:06:40,  1.71s/it]
[2024-05-14 13:28:53,609] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 882
[2024-05-14 13:28:53,610] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:28:53,610] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 13:28:55,325] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 883
[2024-05-14 13:28:55,326] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 27%|██▋       | 883/3218 [29:54<1:13:55,  1.90s/it]
[2024-05-14 13:28:56,933] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 884
[2024-05-14 13:28:56,934] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 28%|██▊       | 885/3218 [29:57<1:08:49,  1.77s/it]
[2024-05-14 13:28:58,604] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 885
[2024-05-14 13:28:58,604] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 28%|██▊       | 886/3218 [29:59<1:08:34,  1.76s/it]
[2024-05-14 13:29:00,038] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 886
[2024-05-14 13:29:00,039] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:29:00,039] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 13:29:01,258] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 887
[2024-05-14 13:29:01,259] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 28%|██▊       | 887/3218 [30:01<1:03:52,  1.64s/it]
[2024-05-14 13:29:02,539] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 888
[2024-05-14 13:29:02,539] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 28%|██▊       | 889/3218 [30:03<55:53,  1.44s/it]
[2024-05-14 13:29:04,114] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 889
[2024-05-14 13:29:04,115] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:29:04,115] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25

 28%|██▊       | 890/3218 [30:05<58:22,  1.50s/it]
[2024-05-14 13:29:06,257] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 890
[2024-05-14 13:29:06,258] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 28%|██▊       | 891/3218 [30:07<1:05:38,  1.69s/it]
[2024-05-14 13:29:07,774] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 891
[2024-05-14 13:29:07,774] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 28%|██▊       | 892/3218 [30:08<1:03:19,  1.63s/it]
[2024-05-14 13:29:09,582] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 892
[2024-05-14 13:29:09,583] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:29:09,583] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 13:29:10,889] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 893
[2024-05-14 13:29:10,889] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 28%|██▊       | 894/3218 [30:11<1:00:58,  1.57s/it]
[2024-05-14 13:29:12,491] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 894
[2024-05-14 13:29:12,491] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 28%|██▊       | 895/3218 [30:13<1:00:53,  1.57s/it]
[2024-05-14 13:29:13,952] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 895
[2024-05-14 13:29:13,952] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 28%|██▊       | 896/3218 [30:15<59:48,  1.55s/it]
[2024-05-14 13:29:15,524] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 896
[2024-05-14 13:29:15,524] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:29:15,525] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 13:29:16,681] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 897
[2024-05-14 13:29:16,682] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 28%|██▊       | 898/3218 [30:17<55:22,  1.43s/it]
[2024-05-14 13:29:17,909] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 898
[2024-05-14 13:29:17,909] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:29:17,910] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 13:29:19,335] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 899
[2024-05-14 13:29:19,335] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 28%|██▊       | 899/3218 [30:18<52:40,  1.36s/it]
 28%|██▊       | 900/3218 [30:20<53:39,  1.39s/it][INFO|trainer.py:3166] 2024-05-14 13:29:19,526 >> ***** Running Evaluation *****
[INFO|trainer.py:3168] 2024-05-14 13:29:19,526 >>   Num examples = 521
[INFO|trainer.py:3171] 2024-05-14 13:29:19,526 >>   Batch size = 8





 97%|█████████▋| 32/33 [00:11<00:00,  2.78it/s]
{'eval_loss': nan, 'eval_runtime': 12.4221, 'eval_samples_per_second': 41.942, 'eval_steps_per_second': 2.657, 'epoch': 0.28}
[2024-05-14 13:29:32,977] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 900
[2024-05-14 13:29:32,977] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

[2024-05-14 13:29:32,977] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 13:29:34,430] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 901
[2024-05-14 13:29:34,430] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 28%|██▊       | 902/3218 [30:35<2:33:52,  3.99s/it]
[2024-05-14 13:29:35,622] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 902
[2024-05-14 13:29:35,622] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:29:35,622] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 13:29:36,752] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 903
[2024-05-14 13:29:36,752] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 28%|██▊       | 904/3218 [30:37<1:37:37,  2.53s/it]
[2024-05-14 13:29:38,120] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 904
[2024-05-14 13:29:38,121] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:29:38,121] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 13:29:39,264] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 905
[2024-05-14 13:29:39,265] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 28%|██▊       | 905/3218 [30:39<1:24:42,  2.20s/it]
[2024-05-14 13:29:40,204] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 906
[2024-05-14 13:29:40,205] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 28%|██▊       | 907/3218 [30:41<1:01:13,  1.59s/it]
[2024-05-14 13:29:41,605] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 907
[2024-05-14 13:29:41,605] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:29:41,606] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 13:29:42,879] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 908
[2024-05-14 13:29:42,879] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 28%|██▊       | 909/3218 [30:43<55:30,  1.44s/it]
[2024-05-14 13:29:44,063] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 909
[2024-05-14 13:29:44,063] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:29:44,063] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25

 28%|██▊       | 910/3218 [30:45<52:51,  1.37s/it]
[2024-05-14 13:29:45,528] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 910
[2024-05-14 13:29:45,528] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:29:45,528] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 13:29:47,140] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 911
[2024-05-14 13:29:47,141] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 28%|██▊       | 912/3218 [30:48<56:14,  1.46s/it]
[2024-05-14 13:29:47,957] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 912
[2024-05-14 13:29:47,957] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 28%|██▊       | 913/3218 [30:48<47:00,  1.22s/it]
[2024-05-14 13:29:49,720] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 913
[2024-05-14 13:29:49,720] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:29:49,721] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 13:29:51,232] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 914
[2024-05-14 13:29:51,232] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 28%|██▊       | 914/3218 [30:50<55:18,  1.44s/it]
[2024-05-14 13:29:52,685] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 915
[2024-05-14 13:29:52,685] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 28%|██▊       | 916/3218 [30:53<55:59,  1.46s/it]
[2024-05-14 13:29:54,499] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 916
[2024-05-14 13:29:54,499] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 28%|██▊       | 917/3218 [30:55<1:00:04,  1.57s/it]
[2024-05-14 13:29:55,976] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 917
[2024-05-14 13:29:55,976] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 29%|██▊       | 918/3218 [30:57<58:47,  1.53s/it]
[2024-05-14 13:29:57,966] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 918
[2024-05-14 13:29:57,966] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 29%|██▊       | 919/3218 [30:59<1:04:22,  1.68s/it]
[2024-05-14 13:29:59,532] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 919
[2024-05-14 13:29:59,532] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:29:59,533] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
{'loss': 0.0, 'learning_rate': 0.0, 'epoch': 0.29}
[2024-05-14 13:30:00,877] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 920
[2024-05-14 13:30:00,877] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 29%|██▊       | 921/3218 [31:01<59:14,  1.55s/it]
[2024-05-14 13:30:02,402] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 921
[2024-05-14 13:30:02,403] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 29%|██▊       | 922/3218 [31:03<59:09,  1.55s/it]
[2024-05-14 13:30:03,775] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 922
[2024-05-14 13:30:03,775] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:30:03,776] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 13:30:05,281] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 923
[2024-05-14 13:30:05,282] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 29%|██▊       | 923/3218 [31:04<57:07,  1.49s/it]
[2024-05-14 13:30:06,705] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 924
[2024-05-14 13:30:06,706] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 29%|██▊       | 925/3218 [31:07<56:19,  1.47s/it]
[2024-05-14 13:30:08,017] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 925
[2024-05-14 13:30:08,017] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:30:08,018] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 13:30:09,341] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 926
[2024-05-14 13:30:09,342] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 29%|██▉       | 926/3218 [31:09<54:51,  1.44s/it]
[2024-05-14 13:30:10,941] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 927
[2024-05-14 13:30:10,942] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 29%|██▉       | 928/3218 [31:12<55:59,  1.47s/it]
[2024-05-14 13:30:12,278] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 928
[2024-05-14 13:30:12,278] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 29%|██▉       | 929/3218 [31:13<54:15,  1.42s/it]
[2024-05-14 13:30:13,566] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 929
[2024-05-14 13:30:13,566] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:30:13,566] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
{'loss': 0.0, 'learning_rate': 0.0, 'epoch': 0.29}
[2024-05-14 13:30:14,927] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 930
[2024-05-14 13:30:14,928] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 29%|██▉       | 931/3218 [31:15<52:21,  1.37s/it]
[2024-05-14 13:30:16,041] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 931
[2024-05-14 13:30:16,041] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:30:16,042] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 13:30:17,476] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 932
[2024-05-14 13:30:17,477] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 29%|██▉       | 932/3218 [31:17<49:00,  1.29s/it]
[2024-05-14 13:30:18,745] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 933
[2024-05-14 13:30:18,745] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 29%|██▉       | 934/3218 [31:19<50:00,  1.31s/it]
[2024-05-14 13:30:19,911] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 934
[2024-05-14 13:30:19,912] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:30:19,912] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 13:30:21,200] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 935
[2024-05-14 13:30:21,201] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 29%|██▉       | 935/3218 [31:20<48:23,  1.27s/it]
[2024-05-14 13:30:22,299] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 936
[2024-05-14 13:30:22,299] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:30:22,299] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25

 29%|██▉       | 937/3218 [31:23<46:27,  1.22s/it]
[2024-05-14 13:30:23,521] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:30:23,522] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 13:30:25,196] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 938
[2024-05-14 13:30:25,196] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 29%|██▉       | 939/3218 [31:26<51:45,  1.36s/it]
[2024-05-14 13:30:26,293] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 939
[2024-05-14 13:30:26,294] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:30:26,294] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
{'loss': 0.0, 'learning_rate': 0.0, 'epoch': 0.29}
[2024-05-14 13:30:27,506] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 940
[2024-05-14 13:30:27,506] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 29%|██▉       | 940/3218 [31:27<48:41,  1.28s/it]
[2024-05-14 13:30:28,626] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 941
[2024-05-14 13:30:28,626] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 29%|██▉       | 942/3218 [31:29<46:04,  1.21s/it]
[2024-05-14 13:30:29,606] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 942
[2024-05-14 13:30:29,606] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:30:29,607] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 13:30:30,714] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 943
[2024-05-14 13:30:30,715] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 29%|██▉       | 944/3218 [31:31<43:04,  1.14s/it]
[2024-05-14 13:30:31,788] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 944
[2024-05-14 13:30:31,789] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:30:31,789] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 13:30:33,509] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 945
[2024-05-14 13:30:33,510] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25


 29%|██▉       | 946/3218 [31:34<49:05,  1.30s/it]
[2024-05-14 13:30:35,638] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 946
[2024-05-14 13:30:35,639] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:30:35,639] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 13:30:36,463] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 947
[2024-05-14 13:30:36,463] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:30:36,463] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 13:30:37,552] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 948
[2024-05-14 13:30:37,552] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 29%|██▉       | 948/3218 [31:37<48:23,  1.28s/it]
[2024-05-14 13:30:39,334] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 949
[2024-05-14 13:30:39,335] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:30:39,335] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25

 29%|██▉       | 949/3218 [31:38<48:23,  1.28s/it]
[2024-05-14 13:30:40,833] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 950
[2024-05-14 13:30:40,833] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 30%|██▉       | 951/3218 [31:41<54:34,  1.44s/it]
[2024-05-14 13:30:41,991] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 951
[2024-05-14 13:30:41,991] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 30%|██▉       | 952/3218 [31:43<51:26,  1.36s/it]
[2024-05-14 13:30:43,640] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 952
[2024-05-14 13:30:43,641] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 30%|██▉       | 953/3218 [31:44<54:36,  1.45s/it]
[2024-05-14 13:30:45,490] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 953
[2024-05-14 13:30:45,490] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:30:45,491] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 13:30:47,075] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 954
[2024-05-14 13:30:47,075] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 30%|██▉       | 955/3218 [31:48<59:21,  1.57s/it]
[2024-05-14 13:30:48,775] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 955
[2024-05-14 13:30:48,776] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 30%|██▉       | 956/3218 [31:49<1:00:23,  1.60s/it]
[2024-05-14 13:30:50,383] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 956
[2024-05-14 13:30:50,383] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 30%|██▉       | 957/3218 [31:51<1:01:00,  1.62s/it]
[2024-05-14 13:30:52,001] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 957
[2024-05-14 13:30:52,001] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 30%|██▉       | 958/3218 [31:53<1:00:52,  1.62s/it]
[2024-05-14 13:30:53,403] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 958
[2024-05-14 13:30:53,403] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:30:53,405] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 13:30:54,746] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 959
[2024-05-14 13:30:54,746] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:30:54,748] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25

 30%|██▉       | 960/3218 [31:55<56:00,  1.49s/it]
[2024-05-14 13:30:56,174] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 960
[2024-05-14 13:30:56,175] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 30%|██▉       | 961/3218 [31:57<55:05,  1.46s/it]
[2024-05-14 13:30:58,031] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 961
[2024-05-14 13:30:58,032] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 30%|██▉       | 962/3218 [31:59<59:15,  1.58s/it]
[2024-05-14 13:30:59,289] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 962
[2024-05-14 13:30:59,289] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:30:59,290] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 13:31:00,965] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 963
[2024-05-14 13:31:00,966] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 30%|██▉       | 964/3218 [32:02<57:47,  1.54s/it]
[2024-05-14 13:31:02,447] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 964
[2024-05-14 13:31:02,447] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 30%|██▉       | 965/3218 [32:03<57:20,  1.53s/it]
[2024-05-14 13:31:03,531] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 965
[2024-05-14 13:31:03,532] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:31:03,533] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 13:31:04,954] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 966
[2024-05-14 13:31:04,954] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 30%|███       | 967/3218 [32:06<52:58,  1.41s/it]
[2024-05-14 13:31:06,353] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 967
[2024-05-14 13:31:06,353] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 30%|███       | 968/3218 [32:07<52:17,  1.39s/it]
[2024-05-14 13:31:07,475] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 968
[2024-05-14 13:31:07,475] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:31:07,476] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 13:31:08,620] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 969
[2024-05-14 13:31:08,620] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:31:08,620] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25

 30%|███       | 970/3218 [32:09<47:11,  1.26s/it]
[2024-05-14 13:31:09,778] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 970
[2024-05-14 13:31:09,778] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:31:09,778] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 13:31:11,160] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 971
[2024-05-14 13:31:11,161] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 30%|███       | 972/3218 [32:12<47:44,  1.28s/it]
[2024-05-14 13:31:12,324] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 972
[2024-05-14 13:31:12,325] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 30%|███       | 973/3218 [32:13<46:39,  1.25s/it]
[2024-05-14 13:31:13,566] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 973
[2024-05-14 13:31:13,567] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:31:13,567] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 13:31:14,694] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 974
[2024-05-14 13:31:14,695] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 30%|███       | 975/3218 [32:15<44:57,  1.20s/it]
[2024-05-14 13:31:15,764] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 975
[2024-05-14 13:31:15,764] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:31:15,767] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 13:31:16,884] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 976
[2024-05-14 13:31:16,884] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 30%|███       | 977/3218 [32:17<42:44,  1.14s/it]
[2024-05-14 13:31:17,862] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 977
[2024-05-14 13:31:17,863] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:31:17,863] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 13:31:18,896] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 978
[2024-05-14 13:31:18,896] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 30%|███       | 979/3218 [32:19<40:24,  1.08s/it]
[2024-05-14 13:31:19,754] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 979
[2024-05-14 13:31:19,754] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:31:19,754] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
{'loss': 0.0, 'learning_rate': 0.0, 'epoch': 0.3}
[2024-05-14 13:31:21,042] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 980
[2024-05-14 13:31:21,043] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 30%|███       | 981/3218 [32:22<40:42,  1.09s/it]
[2024-05-14 13:31:22,810] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 981
[2024-05-14 13:31:22,810] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 31%|███       | 982/3218 [32:23<49:12,  1.32s/it]
[2024-05-14 13:31:23,758] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 982
[2024-05-14 13:31:23,759] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:31:23,759] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 13:31:25,026] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 983
[2024-05-14 13:31:25,027] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 31%|███       | 984/3218 [32:26<46:19,  1.24s/it]
[2024-05-14 13:31:26,909] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 984
[2024-05-14 13:31:26,909] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 31%|███       | 985/3218 [32:27<53:05,  1.43s/it]
[2024-05-14 13:31:28,199] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 985
[2024-05-14 13:31:28,199] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 31%|███       | 986/3218 [32:29<51:39,  1.39s/it]
[2024-05-14 13:31:29,678] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 986
[2024-05-14 13:31:29,679] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:31:29,684] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 13:31:30,904] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 987
[2024-05-14 13:31:30,904] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 31%|███       | 988/3218 [32:31<50:18,  1.35s/it]
[2024-05-14 13:31:32,261] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 988
[2024-05-14 13:31:32,262] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 31%|███       | 989/3218 [32:33<50:42,  1.36s/it]
[2024-05-14 13:31:34,499] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 989
[2024-05-14 13:31:34,499] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:31:34,500] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25

 31%|███       | 990/3218 [32:35<59:58,  1.61s/it]
[2024-05-14 13:31:36,558] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 990
[2024-05-14 13:31:36,558] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 31%|███       | 991/3218 [32:37<1:05:15,  1.76s/it]
[2024-05-14 13:31:38,228] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 991
[2024-05-14 13:31:38,228] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 31%|███       | 992/3218 [32:39<1:03:51,  1.72s/it]
[2024-05-14 13:31:40,182] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 992
[2024-05-14 13:31:40,182] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 31%|███       | 993/3218 [32:41<1:06:52,  1.80s/it]
[2024-05-14 13:31:41,492] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 993
[2024-05-14 13:31:41,493] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:31:41,493] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 13:31:42,890] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 994
[2024-05-14 13:31:42,891] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 31%|███       | 995/3218 [32:43<57:52,  1.56s/it]
[2024-05-14 13:31:44,271] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 995
[2024-05-14 13:31:44,272] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 31%|███       | 996/3218 [32:45<56:04,  1.51s/it]
[2024-05-14 13:31:45,631] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 996
[2024-05-14 13:31:45,632] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:31:45,632] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 13:31:46,829] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 997
[2024-05-14 13:31:46,829] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 31%|███       | 998/3218 [32:47<51:11,  1.38s/it]
[2024-05-14 13:31:48,078] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 998
[2024-05-14 13:31:48,079] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:31:48,079] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 13:31:49,115] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 999
[2024-05-14 13:31:49,116] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
 31%|███       | 1000/3218 [32:50<46:25,  1.26s/it][INFO|trainer.py:3166] 2024-05-14 13:31:49,420 >> ***** Running Evaluation *****
[INFO|trainer.py:3168] 2024-05-14 13:31:49,420 >>   Num examples = 521
[INFO|trainer.py:3171] 2024-05-14 13:31:49,420 >>   Batch size = 8
{'loss': 0.0, 'learning_rate': 0.0, 'epoch': 0.31}





100%|██████████| 33/33 [00:11<00:00,  2.31it/s]

 31%|███       | 1000/3218 [33:02<46:25,  1.26s[INFO|trainer.py:2889] 2024-05-14 13:32:07,660 >> Saving model checkpoint to outputs/instruct_tuning/model/tmp-checkpoint-1000
[INFO|configuration_utils.py:483] 2024-05-14 13:32:07,662 >> Configuration saved in outputs/instruct_tuning/model/tmp-checkpoint-1000/config.json
[INFO|configuration_utils.py:594] 2024-05-14 13:32:07,663 >> Configuration saved in outputs/instruct_tuning/model/tmp-checkpoint-1000/generation_config.json
[2024-05-14 13:32:32,852] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step1000 is about to be saved!
[2024-05-14 13:32:32,858] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: outputs/instruct_tuning/model/tmp-checkpoint-1000/global_step1000/mp_rank_00_model_states.pt
[2024-05-14 13:32:32,858] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving outputs/instruct_tuning/model/tmp-checkpoint-1000/global_step1000/mp_rank_00_model_states.pt...
[INFO|modeling_utils.py:2390] 2024-05-14 13:32:31,811 >> The model is bigger than the maximum size per checkpoint (5GB) and is going to be split in 3 checkpoint shards. You can find where each parameters has been saved in the index located at outputs/instruct_tuning/model/tmp-checkpoint-1000/model.safetensors.index.json.
[INFO|tokenization_utils_base.py:2432] 2024-05-14 13:32:31,813 >> tokenizer config file saved in outputs/instruct_tuning/model/tmp-checkpoint-1000/tokenizer_config.json
[INFO|tokenization_utils_base.py:2441] 2024-05-14 13:32:31,813 >> Special tokens file saved in outputs/instruct_tuning/model/tmp-checkpoint-1000/special_tokens_map.json
/mounts/work/lyk/anaconda3/envs/torch2/lib/python3.8/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
[2024-05-14 13:33:48,186] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved outputs/instruct_tuning/model/tmp-checkpoint-1000/global_step1000/mp_rank_00_model_states.pt.
[2024-05-14 13:33:48,187] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step1000 is ready now!
[INFO|trainer.py:2979] 2024-05-14 13:33:48,203 >> Deleting older checkpoint [outputs/instruct_tuning/model/checkpoint-600] due to args.save_total_limit
[2024-05-14 13:33:52,571] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1000
[2024-05-14 13:33:52,571] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:33:52,571] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
 31%|███       | 1001/3218 [34:53<23:21:13, 37.92s/it]
[2024-05-14 13:33:53,996] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1001
[2024-05-14 13:33:53,996] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:33:53,996] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 13:33:55,275] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1002
[2024-05-14 13:33:55,275] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 31%|███       | 1003/3218 [34:56<11:51:09, 19.26s/it]
[2024-05-14 13:33:56,434] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1003
[2024-05-14 13:33:56,434] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 31%|███       | 1005/3218 [34:58<6:07:59,  9.98s/it]
[2024-05-14 13:33:57,423] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1004
[2024-05-14 13:33:57,423] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:33:57,424] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 13:33:58,418] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1005
[2024-05-14 13:33:58,419] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 31%|███▏      | 1007/3218 [35:00<3:19:06,  5.40s/it]
[2024-05-14 13:33:59,450] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1006
[2024-05-14 13:33:59,450] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:33:59,450] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 13:34:00,512] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1007
[2024-05-14 13:34:00,512] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 31%|███▏      | 1008/3218 [35:01<2:31:13,  4.11s/it]
[2024-05-14 13:34:01,989] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1008
[2024-05-14 13:34:01,989] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:34:01,989] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 13:34:03,098] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1009
[2024-05-14 13:34:03,099] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 31%|███▏      | 1010/3218 [35:04<1:37:38,  2.65s/it]
{'loss': 0.0, 'learning_rate': 0.0, 'epoch': 0.31}
[2024-05-14 13:34:04,133] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1010
[2024-05-14 13:34:04,133] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:34:04,133] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 13:34:05,290] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1011
[2024-05-14 13:34:05,290] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 31%|███▏      | 1012/3218 [35:06<1:08:23,  1.86s/it]
[2024-05-14 13:34:06,377] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1012
[2024-05-14 13:34:06,377] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:34:06,377] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 13:34:07,417] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1013
[2024-05-14 13:34:07,417] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 32%|███▏      | 1014/3218 [35:08<53:14,  1.45s/it]
[2024-05-14 13:34:08,514] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1014
[2024-05-14 13:34:08,514] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 32%|███▏      | 1015/3218 [35:09<49:11,  1.34s/it]
[2024-05-14 13:34:10,506] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1015
[2024-05-14 13:34:10,506] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 32%|███▏      | 1016/3218 [35:11<55:46,  1.52s/it]
[2024-05-14 13:34:12,726] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1016
[2024-05-14 13:34:12,727] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 32%|███▏      | 1017/3218 [35:13<1:04:29,  1.76s/it]
[2024-05-14 13:34:13,710] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1017
[2024-05-14 13:34:13,710] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:34:13,710] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 13:34:14,858] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1018
[2024-05-14 13:34:14,858] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 32%|███▏      | 1019/3218 [35:15<52:01,  1.42s/it]
[2024-05-14 13:34:16,619] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1019
[2024-05-14 13:34:16,619] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:34:16,622] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25

 32%|███▏      | 1020/3218 [35:17<55:46,  1.52s/it]
[2024-05-14 13:34:18,628] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1020
[2024-05-14 13:34:18,628] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 32%|███▏      | 1021/3218 [35:19<1:01:32,  1.68s/it]
[2024-05-14 13:34:20,207] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1021
[2024-05-14 13:34:20,208] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 32%|███▏      | 1022/3218 [35:21<59:46,  1.63s/it]
[2024-05-14 13:34:21,662] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1022
[2024-05-14 13:34:21,663] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:34:21,663] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 13:34:23,345] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1023
[2024-05-14 13:34:23,345] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 32%|███▏      | 1024/3218 [35:24<59:17,  1.62s/it]
[2024-05-14 13:34:24,779] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1024
[2024-05-14 13:34:24,780] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 32%|███▏      | 1025/3218 [35:25<56:36,  1.55s/it]
[2024-05-14 13:34:26,127] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1025
[2024-05-14 13:34:26,128] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 32%|███▏      | 1027/3218 [35:28<53:12,  1.46s/it]
[2024-05-14 13:34:27,514] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1026
[2024-05-14 13:34:27,515] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:34:27,515] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 13:34:28,890] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1027
[2024-05-14 13:34:28,891] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 32%|███▏      | 1028/3218 [35:29<52:23,  1.44s/it]
[2024-05-14 13:34:30,585] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1028
[2024-05-14 13:34:30,585] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 32%|███▏      | 1029/3218 [35:31<55:42,  1.53s/it]
[2024-05-14 13:34:32,454] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1029
[2024-05-14 13:34:32,454] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:34:32,455] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25

 32%|███▏      | 1030/3218 [35:33<59:03,  1.62s/it]
[2024-05-14 13:34:33,720] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1030
[2024-05-14 13:34:33,720] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:34:33,725] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 13:34:34,958] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1031
[2024-05-14 13:34:34,958] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 32%|███▏      | 1032/3218 [35:35<52:09,  1.43s/it]
[2024-05-14 13:34:36,782] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1032
[2024-05-14 13:34:36,782] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 32%|███▏      | 1033/3218 [35:37<56:36,  1.55s/it]
[2024-05-14 13:34:38,470] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1033
[2024-05-14 13:34:38,470] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 32%|███▏      | 1034/3218 [35:39<58:27,  1.61s/it]
[2024-05-14 13:34:40,302] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1034
[2024-05-14 13:34:40,302] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:34:40,303] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 13:34:41,433] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1035
[2024-05-14 13:34:41,433] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 32%|███▏      | 1036/3218 [35:42<54:57,  1.51s/it]
[2024-05-14 13:34:42,730] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1036
[2024-05-14 13:34:42,730] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 32%|███▏      | 1037/3218 [35:43<52:42,  1.45s/it]
[2024-05-14 13:34:44,061] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1037
[2024-05-14 13:34:44,061] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:34:44,061] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 13:34:45,239] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1038
[2024-05-14 13:34:45,240] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 32%|███▏      | 1039/3218 [35:46<48:13,  1.33s/it]
[2024-05-14 13:34:46,506] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1039
[2024-05-14 13:34:46,506] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:34:46,506] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25

 32%|███▏      | 1040/3218 [35:47<47:55,  1.32s/it]
[2024-05-14 13:34:47,827] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1040
[2024-05-14 13:34:47,827] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:34:47,827] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 13:34:49,022] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1041
[2024-05-14 13:34:49,022] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 32%|███▏      | 1042/3218 [35:50<46:14,  1.27s/it]
[2024-05-14 13:34:49,983] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1042
[2024-05-14 13:34:49,983] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:34:49,983] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 13:34:51,333] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1043
[2024-05-14 13:34:51,333] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 32%|███▏      | 1044/3218 [35:52<44:41,  1.23s/it]
[2024-05-14 13:34:52,623] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1044
[2024-05-14 13:34:52,624] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 32%|███▏      | 1045/3218 [35:53<45:36,  1.26s/it]
[2024-05-14 13:34:53,722] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1045
[2024-05-14 13:34:53,723] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:34:53,723] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 13:34:54,932] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1046
[2024-05-14 13:34:54,932] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 33%|███▎      | 1047/3218 [35:55<43:25,  1.20s/it]
[2024-05-14 13:34:55,884] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1047
[2024-05-14 13:34:55,884] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:34:55,884] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 13:34:56,962] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1048
[2024-05-14 13:34:56,963] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 33%|███▎      | 1049/3218 [35:58<40:24,  1.12s/it]
[2024-05-14 13:34:58,169] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1049
[2024-05-14 13:34:58,169] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:34:58,169] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25

 33%|███▎      | 1050/3218 [35:59<41:03,  1.14s/it]
[2024-05-14 13:35:00,508] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1050
[2024-05-14 13:35:00,509] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 33%|███▎      | 1051/3218 [36:01<54:18,  1.50s/it]
[2024-05-14 13:35:02,095] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1051
[2024-05-14 13:35:02,096] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:35:02,096] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 13:35:02,914] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1052
[2024-05-14 13:35:02,914] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 33%|███▎      | 1053/3218 [36:03<45:47,  1.27s/it]
[2024-05-14 13:35:04,365] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1053
[2024-05-14 13:35:04,366] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 33%|███▎      | 1054/3218 [36:05<49:58,  1.39s/it]
[2024-05-14 13:35:06,032] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1054
[2024-05-14 13:35:06,032] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 33%|███▎      | 1055/3218 [36:07<53:08,  1.47s/it]
[2024-05-14 13:35:08,071] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1055
[2024-05-14 13:35:08,072] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 33%|███▎      | 1056/3218 [36:09<59:01,  1.64s/it]
[2024-05-14 13:35:10,094] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1056
[2024-05-14 13:35:10,095] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:35:10,095] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 13:35:11,447] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1057
[2024-05-14 13:35:11,448] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 33%|███▎      | 1058/3218 [36:12<58:16,  1.62s/it]
[2024-05-14 13:35:12,796] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1058
[2024-05-14 13:35:12,797] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 33%|███▎      | 1059/3218 [36:13<55:44,  1.55s/it]
[2024-05-14 13:35:14,101] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1059
[2024-05-14 13:35:14,101] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:35:14,102] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25

 33%|███▎      | 1061/3218 [36:16<52:20,  1.46s/it]
[2024-05-14 13:35:15,580] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1060
[2024-05-14 13:35:15,581] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:35:15,582] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 13:35:17,078] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1061
[2024-05-14 13:35:17,078] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 33%|███▎      | 1062/3218 [36:18<53:38,  1.49s/it]
[2024-05-14 13:35:18,631] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1062
[2024-05-14 13:35:18,632] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 33%|███▎      | 1063/3218 [36:19<54:13,  1.51s/it]
[2024-05-14 13:35:19,840] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1063
[2024-05-14 13:35:19,840] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:35:19,841] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 13:35:21,313] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1064
[2024-05-14 13:35:21,313] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 33%|███▎      | 1065/3218 [36:22<51:22,  1.43s/it]
[2024-05-14 13:35:23,502] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1065
[2024-05-14 13:35:23,502] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 33%|███▎      | 1066/3218 [36:24<1:00:05,  1.68s/it]
[2024-05-14 13:35:24,826] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1066
[2024-05-14 13:35:24,826] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 33%|███▎      | 1067/3218 [36:25<55:16,  1.54s/it]
[2024-05-14 13:35:26,095] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1067
[2024-05-14 13:35:26,096] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:35:26,096] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 13:35:27,420] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1068
[2024-05-14 13:35:27,420] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 33%|███▎      | 1069/3218 [36:28<50:55,  1.42s/it]
[2024-05-14 13:35:28,741] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1069
[2024-05-14 13:35:28,742] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:35:28,743] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25

 33%|███▎      | 1070/3218 [36:29<49:53,  1.39s/it]
[2024-05-14 13:35:30,013] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1070
[2024-05-14 13:35:30,013] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 33%|███▎      | 1071/3218 [36:31<48:19,  1.35s/it]
[2024-05-14 13:35:31,878] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1071
[2024-05-14 13:35:31,878] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:35:31,878] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 13:35:33,118] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1072
[2024-05-14 13:35:33,119] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 33%|███▎      | 1073/3218 [36:34<50:54,  1.42s/it]
[2024-05-14 13:35:34,467] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1073
[2024-05-14 13:35:34,467] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 33%|███▎      | 1074/3218 [36:35<49:59,  1.40s/it]
[2024-05-14 13:35:35,713] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1074
[2024-05-14 13:35:35,714] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:35:35,714] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 13:35:36,874] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1075
[2024-05-14 13:35:36,874] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 33%|███▎      | 1076/3218 [36:37<46:27,  1.30s/it]
[2024-05-14 13:35:38,592] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1076
[2024-05-14 13:35:38,593] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 33%|███▎      | 1077/3218 [36:39<51:21,  1.44s/it]
[2024-05-14 13:35:39,791] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1077
[2024-05-14 13:35:39,791] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:35:39,791] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 13:35:41,101] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1078
[2024-05-14 13:35:41,102] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 34%|███▎      | 1079/3218 [36:42<47:06,  1.32s/it]
[2024-05-14 13:35:42,140] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1079
[2024-05-14 13:35:42,141] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:35:42,141] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
{'loss': 0.0, 'learning_rate': 0.0, 'epoch': 0.34}
[2024-05-14 13:35:43,152] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1080
[2024-05-14 13:35:43,153] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 34%|███▎      | 1081/3218 [36:44<42:01,  1.18s/it]
[2024-05-14 13:35:44,343] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1081
[2024-05-14 13:35:44,343] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:35:44,344] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 13:35:45,331] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1082
[2024-05-14 13:35:45,332] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 34%|███▎      | 1083/3218 [36:46<40:17,  1.13s/it]
[2024-05-14 13:35:46,429] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1083
[2024-05-14 13:35:46,429] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:35:46,429] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 13:35:47,222] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1084
[2024-05-14 13:35:47,222] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 34%|███▎      | 1085/3218 [36:48<36:07,  1.02s/it]
[2024-05-14 13:35:48,419] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1085
[2024-05-14 13:35:48,419] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 34%|███▎      | 1086/3218 [36:49<37:40,  1.06s/it]
[2024-05-14 13:35:50,064] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1086
[2024-05-14 13:35:50,064] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:35:50,065] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 13:35:51,090] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1087
[2024-05-14 13:35:51,091] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 34%|███▍      | 1088/3218 [36:51<40:39,  1.15s/it]
[2024-05-14 13:35:52,405] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1088
[2024-05-14 13:35:52,406] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 34%|███▍      | 1089/3218 [36:53<44:03,  1.24s/it]
[2024-05-14 13:35:53,955] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1089
[2024-05-14 13:35:53,956] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:35:53,956] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
{'loss': 0.0, 'learning_rate': 0.0, 'epoch': 0.34}
[2024-05-14 13:35:55,385] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1090
[2024-05-14 13:35:55,386] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 34%|███▍      | 1091/3218 [36:56<48:20,  1.36s/it]
[2024-05-14 13:35:56,811] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1091
[2024-05-14 13:35:56,812] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 34%|███▍      | 1092/3218 [36:57<49:18,  1.39s/it]
[2024-05-14 13:35:58,199] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1092
[2024-05-14 13:35:58,200] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:35:58,200] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 13:35:59,623] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1093
[2024-05-14 13:35:59,623] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 34%|███▍      | 1094/3218 [37:00<49:27,  1.40s/it]
[2024-05-14 13:36:01,184] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1094
[2024-05-14 13:36:01,184] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 34%|███▍      | 1095/3218 [37:02<51:41,  1.46s/it]
[2024-05-14 13:36:03,136] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1095
[2024-05-14 13:36:03,136] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 34%|███▍      | 1096/3218 [37:04<56:39,  1.60s/it]
[2024-05-14 13:36:04,542] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1096
[2024-05-14 13:36:04,542] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 34%|███▍      | 1097/3218 [37:05<54:00,  1.53s/it]
[2024-05-14 13:36:06,110] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1097
[2024-05-14 13:36:06,111] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 34%|███▍      | 1098/3218 [37:07<55:05,  1.56s/it]
[2024-05-14 13:36:07,780] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1098
[2024-05-14 13:36:07,781] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:36:07,781] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 13:36:09,160] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1099
[2024-05-14 13:36:09,160] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:36:09,162] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
 34%|███▍      | 1100/3218 [37:10<53:44,  1.52s/it][INFO|trainer.py:3166] 2024-05-14 13:36:09,355 >> ***** Running Evaluation *****
[INFO|trainer.py:3168] 2024-05-14 13:36:09,355 >>   Num examples = 521
[INFO|trainer.py:3171] 2024-05-14 13:36:09,355 >>   Batch size = 8






 88%|████████▊ | 29/33 [00:10<00:01,  2.82it/s]
{'eval_loss': nan, 'eval_runtime': 12.4084, 'eval_samples_per_second': 41.988, 'eval_steps_per_second': 2.659, 'epoch': 0.34}
[2024-05-14 13:36:22,931] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1100
[2024-05-14 13:36:22,931] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 34%|███▍      | 1101/3218 [37:24<3:03:43,  5.21s/it]
[2024-05-14 13:36:24,163] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1101
[2024-05-14 13:36:24,163] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 34%|███▍      | 1102/3218 [37:25<2:21:20,  4.01s/it]
[2024-05-14 13:36:25,936] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1102
[2024-05-14 13:36:25,937] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:36:25,937] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 13:36:27,385] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1103
[2024-05-14 13:36:27,386] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 34%|███▍      | 1104/3218 [37:28<1:37:43,  2.77s/it]
[2024-05-14 13:36:28,655] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1104
[2024-05-14 13:36:28,655] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 34%|███▍      | 1105/3218 [37:29<1:21:34,  2.32s/it]
[2024-05-14 13:36:29,868] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1105
[2024-05-14 13:36:29,869] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:36:29,869] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 13:36:30,910] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1106
[2024-05-14 13:36:30,911] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 34%|███▍      | 1107/3218 [37:31<59:34,  1.69s/it]
[2024-05-14 13:36:32,039] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1107
[2024-05-14 13:36:32,039] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:36:32,039] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 13:36:33,188] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1108
[2024-05-14 13:36:33,188] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 34%|███▍      | 1109/3218 [37:34<49:32,  1.41s/it]
[2024-05-14 13:36:34,380] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1109
[2024-05-14 13:36:34,381] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:36:34,381] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
{'loss': 0.0, 'learning_rate': 0.0, 'epoch': 0.34}
[2024-05-14 13:36:35,417] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1110
[2024-05-14 13:36:35,417] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 35%|███▍      | 1111/3218 [37:36<44:01,  1.25s/it]
[2024-05-14 13:36:37,063] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1111
[2024-05-14 13:36:37,063] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 35%|███▍      | 1112/3218 [37:38<48:36,  1.39s/it]
[2024-05-14 13:36:38,079] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1112
[2024-05-14 13:36:38,080] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:36:38,080] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 13:36:39,201] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1113
[2024-05-14 13:36:39,201] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 35%|███▍      | 1114/3218 [37:40<42:32,  1.21s/it]
[2024-05-14 13:36:40,096] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1114
[2024-05-14 13:36:40,097] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:36:40,098] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 13:36:41,009] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1115
[2024-05-14 13:36:41,009] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 35%|███▍      | 1116/3218 [37:42<37:22,  1.07s/it]
[2024-05-14 13:36:42,019] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1116
[2024-05-14 13:36:42,020] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:36:42,020] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 13:36:42,972] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1117
[2024-05-14 13:36:42,972] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 35%|███▍      | 1118/3218 [37:44<35:35,  1.02s/it]
[2024-05-14 13:36:44,214] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1118
[2024-05-14 13:36:44,214] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:36:44,215] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 13:36:45,187] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1119
[2024-05-14 13:36:45,187] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:36:45,188] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25

 35%|███▍      | 1120/3218 [37:46<36:16,  1.04s/it]
[2024-05-14 13:36:46,425] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1120
[2024-05-14 13:36:46,425] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:36:46,425] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 13:36:47,691] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1121
[2024-05-14 13:36:47,691] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 35%|███▍      | 1122/3218 [37:48<40:24,  1.16s/it]
[2024-05-14 13:36:48,447] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1122
[2024-05-14 13:36:48,447] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:36:48,447] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 13:36:49,685] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1123
[2024-05-14 13:36:49,685] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 35%|███▍      | 1124/3218 [37:50<38:50,  1.11s/it]
[2024-05-14 13:36:51,124] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1124
[2024-05-14 13:36:51,125] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 35%|███▍      | 1125/3218 [37:52<41:54,  1.20s/it]
[2024-05-14 13:36:52,754] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1125
[2024-05-14 13:36:52,755] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 35%|███▍      | 1126/3218 [37:53<46:50,  1.34s/it]
[2024-05-14 13:36:54,224] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1126
[2024-05-14 13:36:54,225] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:36:54,225] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 13:36:55,727] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1127
[2024-05-14 13:36:55,727] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 35%|███▌      | 1127/3218 [37:55<48:05,  1.38s/it]
[2024-05-14 13:36:57,093] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1128
[2024-05-14 13:36:57,094] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 35%|███▌      | 1129/3218 [37:58<48:48,  1.40s/it]
[2024-05-14 13:36:58,519] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1129
[2024-05-14 13:36:58,519] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:36:58,519] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25

 35%|███▌      | 1130/3218 [37:59<48:57,  1.41s/it]
[2024-05-14 13:36:59,806] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1130
[2024-05-14 13:36:59,806] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:36:59,806] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 13:37:01,011] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1131
[2024-05-14 13:37:01,011] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 35%|███▌      | 1132/3218 [38:01<45:23,  1.31s/it]
[2024-05-14 13:37:02,377] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1132
[2024-05-14 13:37:02,378] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 35%|███▌      | 1133/3218 [38:03<46:23,  1.34s/it]
[2024-05-14 13:37:03,826] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1133
[2024-05-14 13:37:03,827] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:37:03,827] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 13:37:05,470] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1134
[2024-05-14 13:37:05,471] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 35%|███▌      | 1135/3218 [38:06<50:30,  1.45s/it]
[2024-05-14 13:37:07,193] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1135
[2024-05-14 13:37:07,194] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 35%|███▌      | 1136/3218 [38:08<53:38,  1.55s/it]
[2024-05-14 13:37:08,356] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1136
[2024-05-14 13:37:08,356] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:37:08,356] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 13:37:09,406] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1137
[2024-05-14 13:37:09,406] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 35%|███▌      | 1138/3218 [38:10<45:24,  1.31s/it]
[2024-05-14 13:37:10,939] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1138
[2024-05-14 13:37:10,940] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 35%|███▌      | 1139/3218 [38:11<47:47,  1.38s/it]
[2024-05-14 13:37:12,826] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1139
[2024-05-14 13:37:12,827] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:37:12,827] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25

 35%|███▌      | 1140/3218 [38:13<53:10,  1.54s/it]
[2024-05-14 13:37:14,243] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1140
[2024-05-14 13:37:14,243] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:37:14,244] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 13:37:15,737] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1141
[2024-05-14 13:37:15,737] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 35%|███▌      | 1141/3218 [38:15<52:01,  1.50s/it]
[2024-05-14 13:37:16,870] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1142
[2024-05-14 13:37:16,870] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 36%|███▌      | 1143/3218 [38:17<47:53,  1.38s/it]
[2024-05-14 13:37:18,008] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1143
[2024-05-14 13:37:18,009] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:37:18,009] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 13:37:19,718] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1144
[2024-05-14 13:37:19,718] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 36%|███▌      | 1144/3218 [38:19<45:01,  1.30s/it]
[2024-05-14 13:37:20,763] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1145
[2024-05-14 13:37:20,763] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 36%|███▌      | 1146/3218 [38:21<45:30,  1.32s/it]
[2024-05-14 13:37:22,006] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1146
[2024-05-14 13:37:22,006] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:37:22,007] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 13:37:23,168] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1147
[2024-05-14 13:37:23,168] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 36%|███▌      | 1148/3218 [38:24<42:59,  1.25s/it]
[2024-05-14 13:37:24,499] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1148
[2024-05-14 13:37:24,500] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:37:24,500] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 13:37:25,539] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1149
[2024-05-14 13:37:25,539] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:37:25,539] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25

 36%|███▌      | 1150/3218 [38:26<41:19,  1.20s/it]
[2024-05-14 13:37:26,869] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1150
[2024-05-14 13:37:26,870] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 36%|███▌      | 1151/3218 [38:27<43:08,  1.25s/it]
[2024-05-14 13:37:28,437] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1151
[2024-05-14 13:37:28,437] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:37:28,437] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 13:37:29,532] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1152
[2024-05-14 13:37:29,533] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25


 36%|███▌      | 1155/3218 [38:32<40:42,  1.18s/it]
[2024-05-14 13:37:30,616] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1153
[2024-05-14 13:37:30,616] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:37:30,616] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 13:37:31,747] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1154
[2024-05-14 13:37:31,748] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:37:31,748] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25

 36%|███▌      | 1157/3218 [38:36<55:05,  1.60s/it]
[2024-05-14 13:37:33,826] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:37:33,826] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 13:37:35,750] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1156
[2024-05-14 13:37:35,751] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 36%|███▌      | 1158/3218 [38:37<46:21,  1.35s/it]
[2024-05-14 13:37:36,681] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1157
[2024-05-14 13:37:36,681] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 36%|███▌      | 1159/3218 [38:39<50:41,  1.48s/it]
[2024-05-14 13:37:38,298] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1158
[2024-05-14 13:37:38,298] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 36%|███▌      | 1160/3218 [38:41<53:59,  1.57s/it]
[2024-05-14 13:37:40,057] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1159
[2024-05-14 13:37:40,057] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:37:40,059] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
{'loss': 0.0, 'learning_rate': 0.0, 'epoch': 0.36}
[2024-05-14 13:37:41,766] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1160
[2024-05-14 13:37:41,767] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 36%|███▌      | 1162/3218 [38:44<53:30,  1.56s/it]
[2024-05-14 13:37:43,250] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1161
[2024-05-14 13:37:43,251] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 36%|███▌      | 1163/3218 [38:45<54:35,  1.59s/it]
[2024-05-14 13:37:44,908] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1162
[2024-05-14 13:37:44,908] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 36%|███▌      | 1164/3218 [38:47<54:03,  1.58s/it]
[2024-05-14 13:37:46,477] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1163
[2024-05-14 13:37:46,477] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 36%|███▌      | 1166/3218 [38:50<50:20,  1.47s/it]
[2024-05-14 13:37:48,085] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1164
[2024-05-14 13:37:48,086] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:37:48,086] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 13:37:49,299] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1165
[2024-05-14 13:37:49,300] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 36%|███▋      | 1167/3218 [38:51<49:04,  1.44s/it]
[2024-05-14 13:37:50,648] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1166
[2024-05-14 13:37:50,649] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 36%|███▋      | 1168/3218 [38:53<48:50,  1.43s/it]
[2024-05-14 13:37:52,054] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1167
[2024-05-14 13:37:52,055] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 36%|███▋      | 1169/3218 [38:55<56:35,  1.66s/it]
[2024-05-14 13:37:54,175] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1168
[2024-05-14 13:37:54,176] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:37:54,184] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 13:37:55,780] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1169
[2024-05-14 13:37:55,781] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 36%|███▋      | 1171/3218 [38:58<53:14,  1.56s/it]
{'loss': 0.0, 'learning_rate': 0.0, 'epoch': 0.36}
[2024-05-14 13:37:57,212] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1170
[2024-05-14 13:37:57,212] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 36%|███▋      | 1172/3218 [38:59<54:20,  1.59s/it]
[2024-05-14 13:37:58,884] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1171
[2024-05-14 13:37:58,884] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 36%|███▋      | 1174/3218 [39:02<52:19,  1.54s/it]
[2024-05-14 13:38:00,153] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1172
[2024-05-14 13:38:00,154] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:38:00,154] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 13:38:01,728] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1173
[2024-05-14 13:38:01,728] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 37%|███▋      | 1175/3218 [39:04<50:38,  1.49s/it]
[2024-05-14 13:38:03,141] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1174
[2024-05-14 13:38:03,142] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 37%|███▋      | 1177/3218 [39:06<44:14,  1.30s/it]
[2024-05-14 13:38:04,248] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1175
[2024-05-14 13:38:04,248] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:38:04,248] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 13:38:05,374] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1176
[2024-05-14 13:38:05,375] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 37%|███▋      | 1179/3218 [39:08<39:49,  1.17s/it]
[2024-05-14 13:38:06,549] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1177
[2024-05-14 13:38:06,549] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:38:06,550] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 13:38:07,518] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1178
[2024-05-14 13:38:07,519] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 37%|███▋      | 1180/3218 [39:09<40:10,  1.18s/it]
[2024-05-14 13:38:08,718] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1179
[2024-05-14 13:38:08,718] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:38:08,719] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
{'loss': 0.0, 'learning_rate': 0.0, 'epoch': 0.37}
[2024-05-14 13:38:09,928] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1180
[2024-05-14 13:38:09,928] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 37%|███▋      | 1182/3218 [39:12<39:52,  1.17s/it]
[2024-05-14 13:38:11,062] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1181
[2024-05-14 13:38:11,062] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 37%|███▋      | 1184/3218 [39:14<42:22,  1.25s/it]
[2024-05-14 13:38:12,163] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1182
[2024-05-14 13:38:12,164] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:38:12,164] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 13:38:13,608] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1183
[2024-05-14 13:38:13,608] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 37%|███▋      | 1186/3218 [39:16<37:41,  1.11s/it]
[2024-05-14 13:38:14,742] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1184
[2024-05-14 13:38:14,743] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:38:14,744] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 13:38:15,652] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1185
[2024-05-14 13:38:15,652] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 37%|███▋      | 1188/3218 [39:18<36:53,  1.09s/it]
[2024-05-14 13:38:16,647] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1186
[2024-05-14 13:38:16,648] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:38:16,648] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 13:38:17,790] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1187
[2024-05-14 13:38:17,790] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 37%|███▋      | 1190/3218 [39:20<34:45,  1.03s/it]
[2024-05-14 13:38:18,839] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1188
[2024-05-14 13:38:18,839] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:38:18,840] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 13:38:19,738] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1189
[2024-05-14 13:38:19,739] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:38:19,739] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25

 37%|███▋      | 1191/3218 [39:22<38:58,  1.15s/it]
[2024-05-14 13:38:21,181] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1190
[2024-05-14 13:38:21,181] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 37%|███▋      | 1193/3218 [39:24<37:14,  1.10s/it]
[2024-05-14 13:38:22,852] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1191
[2024-05-14 13:38:22,852] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:38:22,853] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 13:38:23,631] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1192
[2024-05-14 13:38:23,631] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 37%|███▋      | 1194/3218 [39:25<38:26,  1.14s/it]
[2024-05-14 13:38:24,705] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1193
[2024-05-14 13:38:24,705] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 37%|███▋      | 1195/3218 [39:27<40:23,  1.20s/it]
[2024-05-14 13:38:26,034] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1194
[2024-05-14 13:38:26,035] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 37%|███▋      | 1196/3218 [39:29<52:16,  1.55s/it]
[2024-05-14 13:38:28,394] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1195
[2024-05-14 13:38:28,394] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 37%|███▋      | 1198/3218 [39:32<53:17,  1.58s/it]
[2024-05-14 13:38:30,007] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1196
[2024-05-14 13:38:30,007] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:38:30,008] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 13:38:31,619] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1197
[2024-05-14 13:38:31,619] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 37%|███▋      | 1199/3218 [39:34<50:49,  1.51s/it]
[2024-05-14 13:38:32,962] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1198
[2024-05-14 13:38:32,963] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
 37%|███▋      | 1200/3218 [39:36<57:17,  1.70s/it][INFO|trainer.py:3166] 2024-05-14 13:38:35,326 >> ***** Running Evaluation *****
[INFO|trainer.py:3168] 2024-05-14 13:38:35,326 >>   Num examples = 521
[INFO|trainer.py:3171] 2024-05-14 13:38:35,326 >>   Batch size = 8
  6%|▌         | 2/33 [00:00<00:03,  8.77it/s]
[2024-05-14 13:38:35,110] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1199
[2024-05-14 13:38:35,110] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:38:35,110] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25






 88%|████████▊ | 29/33 [00:10<00:01,  2.81it/s]
 37%|███▋      | 1200/3218 [39:48<57:17,  1.70s[INFO|trainer.py:2889] 2024-05-14 13:38:53,820 >> Saving model checkpoint to outputs/instruct_tuning/model/tmp-checkpoint-1200
[INFO|configuration_utils.py:483] 2024-05-14 13:38:53,822 >> Configuration saved in outputs/instruct_tuning/model/tmp-checkpoint-1200/config.json
[INFO|configuration_utils.py:594] 2024-05-14 13:38:53,823 >> Configuration saved in outputs/instruct_tuning/model/tmp-checkpoint-1200/generation_config.json
[INFO|modeling_utils.py:2390] 2024-05-14 13:39:18,956 >> The model is bigger than the maximum size per checkpoint (5GB) and is going to be split in 3 checkpoint shards. You can find where each parameters has been saved in the index located at outputs/instruct_tuning/model/tmp-checkpoint-1200/model.safetensors.index.json.
[INFO|tokenization_utils_base.py:2432] 2024-05-14 13:39:18,958 >> tokenizer config file saved in outputs/instruct_tuning/model/tmp-checkpoint-1200/tokenizer_config.json
[INFO|tokenization_utils_base.py:2441] 2024-05-14 13:39:18,958 >> Special tokens file saved in outputs/instruct_tuning/model/tmp-checkpoint-1200/special_tokens_map.json
/mounts/work/lyk/anaconda3/envs/torch2/lib/python3.8/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
[2024-05-14 13:39:19,952] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step1200 is about to be saved!
[2024-05-14 13:39:19,958] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: outputs/instruct_tuning/model/tmp-checkpoint-1200/global_step1200/mp_rank_00_model_states.pt
[2024-05-14 13:39:19,958] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving outputs/instruct_tuning/model/tmp-checkpoint-1200/global_step1200/mp_rank_00_model_states.pt...
[INFO|trainer.py:2979] 2024-05-14 13:40:35,604 >> Deleting older checkpoint [outputs/instruct_tuning/model/checkpoint-800] due to args.save_total_limit
[2024-05-14 13:40:35,587] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved outputs/instruct_tuning/model/tmp-checkpoint-1200/global_step1200/mp_rank_00_model_states.pt.
[2024-05-14 13:40:35,588] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step1200 is ready now!
 37%|███▋      | 1201/3218 [41:40<21:37:53, 38.61s/it]
[2024-05-14 13:40:39,840] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1200
[2024-05-14 13:40:39,840] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 37%|███▋      | 1202/3218 [41:42<15:24:53, 27.53s/it]
[2024-05-14 13:40:41,552] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1201
[2024-05-14 13:40:41,553] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 37%|███▋      | 1203/3218 [41:43<11:00:18, 19.66s/it]
[2024-05-14 13:40:42,879] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1202
[2024-05-14 13:40:42,879] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 37%|███▋      | 1205/3218 [41:46<5:46:53, 10.34s/it]
[2024-05-14 13:40:44,283] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1203
[2024-05-14 13:40:44,284] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:40:44,284] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 13:40:45,625] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1204
[2024-05-14 13:40:45,626] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 37%|███▋      | 1206/3218 [41:47<4:15:07,  7.61s/it]
[2024-05-14 13:40:46,815] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1205
[2024-05-14 13:40:46,816] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 38%|███▊      | 1208/3218 [41:50<2:31:49,  4.53s/it]
[2024-05-14 13:40:48,244] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1206
[2024-05-14 13:40:48,245] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:40:48,246] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 13:40:49,961] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1207
[2024-05-14 13:40:49,962] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 38%|███▊      | 1209/3218 [41:52<2:02:26,  3.66s/it]
[2024-05-14 13:40:51,555] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1208
[2024-05-14 13:40:51,555] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 38%|███▊      | 1211/3218 [41:54<1:19:50,  2.39s/it]
[2024-05-14 13:40:52,818] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1209
[2024-05-14 13:40:52,818] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:40:52,818] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
{'loss': 0.0, 'learning_rate': 0.0, 'epoch': 0.38}
[2024-05-14 13:40:53,924] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1210
[2024-05-14 13:40:53,925] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 38%|███▊      | 1212/3218 [41:56<1:11:42,  2.14s/it]
[2024-05-14 13:40:55,473] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1211
[2024-05-14 13:40:55,474] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 38%|███▊      | 1214/3218 [41:59<55:40,  1.67s/it]
[2024-05-14 13:40:56,822] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1212
[2024-05-14 13:40:56,823] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:40:56,823] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 13:40:57,982] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1213
[2024-05-14 13:40:57,982] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 38%|███▊      | 1215/3218 [42:00<53:02,  1.59s/it]
[2024-05-14 13:40:59,336] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1214
[2024-05-14 13:40:59,336] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 38%|███▊      | 1217/3218 [42:02<47:12,  1.42s/it]
[2024-05-14 13:41:00,459] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1215
[2024-05-14 13:41:00,459] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:41:00,459] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 13:41:01,805] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1216
[2024-05-14 13:41:01,806] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 38%|███▊      | 1218/3218 [42:04<46:29,  1.39s/it]
[2024-05-14 13:41:03,143] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1217
[2024-05-14 13:41:03,144] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:41:03,144] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 13:41:04,155] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1218
[2024-05-14 13:41:04,156] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 38%|███▊      | 1220/3218 [42:06<39:58,  1.20s/it]
[2024-05-14 13:41:05,196] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1219
[2024-05-14 13:41:05,196] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:41:05,196] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25

 38%|███▊      | 1222/3218 [42:08<43:04,  1.29s/it]
[2024-05-14 13:41:06,352] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1220
[2024-05-14 13:41:06,352] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:41:06,352] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 13:41:07,838] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1221
[2024-05-14 13:41:07,838] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 38%|███▊      | 1223/3218 [42:09<38:15,  1.15s/it]
[2024-05-14 13:41:08,725] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1222
[2024-05-14 13:41:08,725] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:41:08,727] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 13:41:10,036] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1223
[2024-05-14 13:41:10,037] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 38%|███▊      | 1225/3218 [42:11<36:38,  1.10s/it]
[2024-05-14 13:41:10,939] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1224
[2024-05-14 13:41:10,939] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 38%|███▊      | 1227/3218 [42:14<43:35,  1.31s/it]
[2024-05-14 13:41:12,429] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1225
[2024-05-14 13:41:12,430] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:41:12,430] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 13:41:13,960] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1226
[2024-05-14 13:41:13,961] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 38%|███▊      | 1229/3218 [42:16<38:41,  1.17s/it]
[2024-05-14 13:41:14,656] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1227
[2024-05-14 13:41:14,657] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:41:14,657] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 13:41:15,888] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1228
[2024-05-14 13:41:15,888] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 38%|███▊      | 1230/3218 [42:18<43:51,  1.32s/it]
[2024-05-14 13:41:17,513] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1229
[2024-05-14 13:41:17,513] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:41:17,513] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25

 38%|███▊      | 1231/3218 [42:20<51:13,  1.55s/it]
[2024-05-14 13:41:19,593] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1230
[2024-05-14 13:41:19,593] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 38%|███▊      | 1232/3218 [42:22<55:12,  1.67s/it]
[2024-05-14 13:41:21,539] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1231
[2024-05-14 13:41:21,539] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 38%|███▊      | 1233/3218 [42:24<54:20,  1.64s/it]
[2024-05-14 13:41:23,130] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1232
[2024-05-14 13:41:23,131] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 38%|███▊      | 1234/3218 [42:26<57:19,  1.73s/it]
[2024-05-14 13:41:25,041] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1233
[2024-05-14 13:41:25,042] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 38%|███▊      | 1235/3218 [42:27<57:04,  1.73s/it]
[2024-05-14 13:41:26,770] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1234
[2024-05-14 13:41:26,770] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:41:26,771] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 13:41:28,230] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1235
[2024-05-14 13:41:28,230] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 38%|███▊      | 1237/3218 [42:30<51:32,  1.56s/it]
[2024-05-14 13:41:29,621] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1236
[2024-05-14 13:41:29,621] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 38%|███▊      | 1238/3218 [42:32<54:40,  1.66s/it]
[2024-05-14 13:41:31,458] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1237
[2024-05-14 13:41:31,459] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 39%|███▊      | 1240/3218 [42:35<47:37,  1.44s/it]
[2024-05-14 13:41:32,845] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1238
[2024-05-14 13:41:32,846] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:41:32,846] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 13:41:33,979] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1239
[2024-05-14 13:41:33,979] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 39%|███▊      | 1241/3218 [42:36<48:06,  1.46s/it]
{'loss': 0.0, 'learning_rate': 0.0, 'epoch': 0.39}
[2024-05-14 13:41:35,461] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1240
[2024-05-14 13:41:35,461] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 39%|███▊      | 1242/3218 [42:37<46:24,  1.41s/it]
[2024-05-14 13:41:36,783] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1241
[2024-05-14 13:41:36,783] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 39%|███▊      | 1244/3218 [42:40<46:07,  1.40s/it]
[2024-05-14 13:41:38,326] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1242
[2024-05-14 13:41:38,327] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:41:38,327] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 13:41:39,624] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1243
[2024-05-14 13:41:39,624] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 39%|███▊      | 1246/3218 [42:42<41:26,  1.26s/it]
[2024-05-14 13:41:40,797] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1244
[2024-05-14 13:41:40,797] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:41:40,797] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 13:41:41,891] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1245
[2024-05-14 13:41:41,891] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 39%|███▉      | 1247/3218 [42:44<42:43,  1.30s/it]
[2024-05-14 13:41:43,289] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1246
[2024-05-14 13:41:43,289] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 39%|███▉      | 1249/3218 [42:46<40:46,  1.24s/it]
[2024-05-14 13:41:44,447] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1247
[2024-05-14 13:41:44,447] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:41:44,447] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 13:41:45,650] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1248
[2024-05-14 13:41:45,650] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 39%|███▉      | 1250/3218 [42:48<41:39,  1.27s/it]
[2024-05-14 13:41:46,954] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1249
[2024-05-14 13:41:46,954] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:41:46,955] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
{'loss': 0.0, 'learning_rate': 0.0, 'epoch': 0.39}
[2024-05-14 13:41:48,143] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1250
[2024-05-14 13:41:48,144] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 39%|███▉      | 1252/3218 [42:50<38:49,  1.18s/it]
[2024-05-14 13:41:49,229] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1251
[2024-05-14 13:41:49,229] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 39%|███▉      | 1254/3218 [42:52<36:47,  1.12s/it]
[2024-05-14 13:41:50,390] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1252
[2024-05-14 13:41:50,390] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:41:50,390] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 13:41:51,379] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1253
[2024-05-14 13:41:51,379] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 39%|███▉      | 1256/3218 [42:54<33:43,  1.03s/it]
[2024-05-14 13:41:52,357] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1254
[2024-05-14 13:41:52,358] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:41:52,360] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 13:41:53,258] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1255
[2024-05-14 13:41:53,259] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 39%|███▉      | 1258/3218 [42:57<38:49,  1.19s/it]
[2024-05-14 13:41:54,643] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1256
[2024-05-14 13:41:54,644] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:41:54,644] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 13:41:55,967] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1257
[2024-05-14 13:41:55,967] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 39%|███▉      | 1260/3218 [42:58<34:48,  1.07s/it]
[2024-05-14 13:41:56,975] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1258
[2024-05-14 13:41:56,975] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:41:56,975] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 13:41:57,935] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1259
[2024-05-14 13:41:57,936] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:41:57,936] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25

 39%|███▉      | 1261/3218 [43:00<38:15,  1.17s/it]
[2024-05-14 13:41:59,331] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1260
[2024-05-14 13:41:59,332] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 39%|███▉      | 1263/3218 [43:02<35:49,  1.10s/it]
[2024-05-14 13:42:00,638] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1261
[2024-05-14 13:42:00,638] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:42:00,639] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 13:42:01,626] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1262
[2024-05-14 13:42:01,626] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 39%|███▉      | 1264/3218 [43:04<43:21,  1.33s/it]
[2024-05-14 13:42:03,255] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1263
[2024-05-14 13:42:03,256] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 39%|███▉      | 1265/3218 [43:05<45:59,  1.41s/it]
[2024-05-14 13:42:04,957] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1264
[2024-05-14 13:42:04,957] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 39%|███▉      | 1266/3218 [43:07<46:28,  1.43s/it]
[2024-05-14 13:42:06,415] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1265
[2024-05-14 13:42:06,415] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:42:06,415] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 13:42:08,144] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1266
[2024-05-14 13:42:08,144] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 39%|███▉      | 1268/3218 [43:10<46:53,  1.44s/it]
[2024-05-14 13:42:09,427] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1267
[2024-05-14 13:42:09,427] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 39%|███▉      | 1269/3218 [43:12<49:11,  1.51s/it]
[2024-05-14 13:42:11,063] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1268
[2024-05-14 13:42:11,063] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 39%|███▉      | 1270/3218 [43:14<56:09,  1.73s/it]
[2024-05-14 13:42:13,323] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1269
[2024-05-14 13:42:13,324] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:42:13,330] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25

 39%|███▉      | 1271/3218 [43:15<54:36,  1.68s/it]
[2024-05-14 13:42:14,914] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1270
[2024-05-14 13:42:14,914] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:42:14,915] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 13:42:16,287] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1271
[2024-05-14 13:42:16,288] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 40%|███▉      | 1273/3218 [43:18<49:28,  1.53s/it]
[2024-05-14 13:42:17,682] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1272
[2024-05-14 13:42:17,682] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 40%|███▉      | 1274/3218 [43:20<48:22,  1.49s/it]
[2024-05-14 13:42:19,091] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1273
[2024-05-14 13:42:19,091] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 40%|███▉      | 1275/3218 [43:21<51:07,  1.58s/it]
[2024-05-14 13:42:20,814] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1274
[2024-05-14 13:42:20,814] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 40%|███▉      | 1277/3218 [43:24<47:37,  1.47s/it]
[2024-05-14 13:42:22,384] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1275
[2024-05-14 13:42:22,384] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:42:22,384] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 13:42:23,635] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1276
[2024-05-14 13:42:23,635] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 40%|███▉      | 1278/3218 [43:26<46:15,  1.43s/it]
[2024-05-14 13:42:24,962] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1277
[2024-05-14 13:42:24,963] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 40%|███▉      | 1280/3218 [43:28<46:45,  1.45s/it]
[2024-05-14 13:42:26,537] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1278
[2024-05-14 13:42:26,538] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:42:26,538] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 13:42:27,909] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1279
[2024-05-14 13:42:27,909] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:42:27,909] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25

 40%|███▉      | 1281/3218 [43:30<45:03,  1.40s/it]
[2024-05-14 13:42:29,209] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1280
[2024-05-14 13:42:29,209] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 40%|███▉      | 1283/3218 [43:32<43:41,  1.35s/it]
[2024-05-14 13:42:30,586] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1281
[2024-05-14 13:42:30,586] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:42:30,586] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 13:42:31,867] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1282
[2024-05-14 13:42:31,867] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 40%|███▉      | 1284/3218 [43:34<43:42,  1.36s/it]
[2024-05-14 13:42:33,179] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1283
[2024-05-14 13:42:33,179] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:42:33,179] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 13:42:34,371] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1284
[2024-05-14 13:42:34,371] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 40%|███▉      | 1286/3218 [43:36<41:27,  1.29s/it]
[2024-05-14 13:42:35,601] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1285
[2024-05-14 13:42:35,601] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 40%|████      | 1288/3218 [43:38<38:55,  1.21s/it]
[2024-05-14 13:42:36,911] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1286
[2024-05-14 13:42:36,911] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:42:36,912] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 13:42:37,976] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1287
[2024-05-14 13:42:37,976] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 40%|████      | 1289/3218 [43:40<38:39,  1.20s/it]
[2024-05-14 13:42:39,122] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1288
[2024-05-14 13:42:39,122] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:42:39,123] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 13:42:40,216] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1289
[2024-05-14 13:42:40,217] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:42:40,217] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25

 40%|████      | 1291/3218 [43:42<37:56,  1.18s/it]
[2024-05-14 13:42:41,427] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1290
[2024-05-14 13:42:41,427] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 40%|████      | 1293/3218 [43:44<35:09,  1.10s/it]
[2024-05-14 13:42:42,397] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1291
[2024-05-14 13:42:42,398] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:42:42,398] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 13:42:43,495] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1292
[2024-05-14 13:42:43,496] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 40%|████      | 1295/3218 [43:46<31:41,  1.01it/s]
[2024-05-14 13:42:44,517] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1293
[2024-05-14 13:42:44,517] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:42:44,518] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 13:42:45,321] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1294
[2024-05-14 13:42:45,321] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 40%|████      | 1296/3218 [43:47<36:47,  1.15s/it]
[2024-05-14 13:42:46,845] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1295
[2024-05-14 13:42:46,846] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 40%|████      | 1298/3218 [43:50<39:37,  1.24s/it]
[2024-05-14 13:42:48,914] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1296
[2024-05-14 13:42:48,915] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:42:48,915] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 13:42:49,836] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1297
[2024-05-14 13:42:49,836] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 40%|████      | 1299/3218 [43:52<40:44,  1.27s/it]
[2024-05-14 13:42:51,025] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1298
[2024-05-14 13:42:51,026] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
 40%|████      | 1300/3218 [43:53<46:21,  1.45s/it][INFO|trainer.py:3166] 2024-05-14 13:42:53,065 >> ***** Running Evaluation *****
[INFO|trainer.py:3168] 2024-05-14 13:42:53,065 >>   Num examples = 521
[INFO|trainer.py:3171] 2024-05-14 13:42:53,065 >>   Batch size = 8
  9%|▉         | 3/33 [00:00<00:07,  3.79it/s]
[2024-05-14 13:42:52,877] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1299
[2024-05-14 13:42:52,878] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:42:52,878] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25






 94%|█████████▍| 31/33 [00:10<00:00,  2.82it/s]

 40%|████      | 1301/3218 [44:07<2:44:30,  5.15s/it]
[2024-05-14 13:43:06,635] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1300
[2024-05-14 13:43:06,635] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 40%|████      | 1303/3218 [44:11<1:47:12,  3.36s/it]
[2024-05-14 13:43:08,548] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1301
[2024-05-14 13:43:08,548] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:43:08,548] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 13:43:10,045] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1302
[2024-05-14 13:43:10,045] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 41%|████      | 1304/3218 [44:12<1:27:48,  2.75s/it]
[2024-05-14 13:43:11,374] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1303
[2024-05-14 13:43:11,375] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 41%|████      | 1306/3218 [44:15<1:05:14,  2.05s/it]
[2024-05-14 13:43:12,875] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1304
[2024-05-14 13:43:12,875] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:43:12,875] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 13:43:14,132] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1305
[2024-05-14 13:43:14,132] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 41%|████      | 1307/3218 [44:17<1:03:19,  1.99s/it]
[2024-05-14 13:43:15,946] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1306
[2024-05-14 13:43:15,946] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 41%|████      | 1308/3218 [44:18<58:18,  1.83s/it]
[2024-05-14 13:43:17,442] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1307
[2024-05-14 13:43:17,443] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 41%|████      | 1309/3218 [44:19<53:10,  1.67s/it]
[2024-05-14 13:43:18,762] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1308
[2024-05-14 13:43:18,763] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 41%|████      | 1310/3218 [44:21<55:28,  1.74s/it]
[2024-05-14 13:43:20,660] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1309
[2024-05-14 13:43:20,660] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:43:20,661] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25

 41%|████      | 1311/3218 [44:23<58:01,  1.83s/it]
[2024-05-14 13:43:22,624] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1310
[2024-05-14 13:43:22,624] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 41%|████      | 1313/3218 [44:26<53:49,  1.70s/it]
[2024-05-14 13:43:24,524] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1311
[2024-05-14 13:43:24,524] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:43:24,524] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 13:43:25,933] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1312
[2024-05-14 13:43:25,933] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 41%|████      | 1314/3218 [44:28<51:20,  1.62s/it]
[2024-05-14 13:43:27,373] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1313
[2024-05-14 13:43:27,374] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 41%|████      | 1316/3218 [44:30<44:26,  1.40s/it]
[2024-05-14 13:43:28,517] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1314
[2024-05-14 13:43:28,517] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:43:28,524] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 13:43:29,739] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1315
[2024-05-14 13:43:29,739] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 41%|████      | 1317/3218 [44:32<47:54,  1.51s/it]
[2024-05-14 13:43:31,502] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1316
[2024-05-14 13:43:31,502] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 41%|████      | 1318/3218 [44:33<44:15,  1.40s/it]
[2024-05-14 13:43:32,666] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1317
[2024-05-14 13:43:32,666] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:43:32,666] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 13:43:34,476] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1318
[2024-05-14 13:43:34,477] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 41%|████      | 1320/3218 [44:36<44:38,  1.41s/it]
[2024-05-14 13:43:35,627] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1319
[2024-05-14 13:43:35,627] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:43:35,628] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25

 41%|████      | 1322/3218 [44:39<40:43,  1.29s/it]
[2024-05-14 13:43:36,882] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1320
[2024-05-14 13:43:36,882] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:43:36,882] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 13:43:37,986] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1321
[2024-05-14 13:43:37,986] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 41%|████      | 1323/3218 [44:40<40:29,  1.28s/it]
[2024-05-14 13:43:39,241] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1322
[2024-05-14 13:43:39,242] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:43:39,242] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 13:43:40,353] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1323
[2024-05-14 13:43:40,353] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 41%|████      | 1325/3218 [44:42<38:16,  1.21s/it]
[2024-05-14 13:43:41,536] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1324
[2024-05-14 13:43:41,536] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 41%|████      | 1327/3218 [44:44<35:52,  1.14s/it]
[2024-05-14 13:43:42,665] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1325
[2024-05-14 13:43:42,666] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:43:42,666] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 13:43:43,679] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1326
[2024-05-14 13:43:43,679] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 41%|████▏     | 1329/3218 [44:47<35:51,  1.14s/it]
[2024-05-14 13:43:45,052] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1327
[2024-05-14 13:43:45,052] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:43:45,052] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 13:43:46,053] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1328
[2024-05-14 13:43:46,054] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 41%|████▏     | 1330/3218 [44:48<35:19,  1.12s/it]
[2024-05-14 13:43:47,155] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1329
[2024-05-14 13:43:47,156] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:43:47,156] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25

 41%|████▏     | 1331/3218 [44:50<43:54,  1.40s/it]
[2024-05-14 13:43:49,166] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1330
[2024-05-14 13:43:49,166] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 41%|████▏     | 1333/3218 [44:52<40:13,  1.28s/it]
[2024-05-14 13:43:51,084] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1331
[2024-05-14 13:43:51,085] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:43:51,085] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 13:43:51,888] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1332
[2024-05-14 13:43:51,888] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 41%|████▏     | 1334/3218 [44:54<41:52,  1.33s/it]
[2024-05-14 13:43:53,201] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1333
[2024-05-14 13:43:53,202] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 41%|████▏     | 1335/3218 [44:55<44:43,  1.43s/it]
[2024-05-14 13:43:54,833] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1334
[2024-05-14 13:43:54,833] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:43:54,833] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 13:43:56,520] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1335
[2024-05-14 13:43:56,520] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 42%|████▏     | 1337/3218 [44:59<47:10,  1.50s/it]
[2024-05-14 13:43:58,010] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1336
[2024-05-14 13:43:58,011] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 42%|████▏     | 1338/3218 [45:00<48:01,  1.53s/it]
[2024-05-14 13:43:59,604] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1337
[2024-05-14 13:43:59,604] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 42%|████▏     | 1339/3218 [45:02<46:33,  1.49s/it]
[2024-05-14 13:44:00,962] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1338
[2024-05-14 13:44:00,962] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 42%|████▏     | 1340/3218 [45:03<49:58,  1.60s/it]
[2024-05-14 13:44:02,886] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1339
[2024-05-14 13:44:02,886] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:44:02,886] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
{'loss': 0.0, 'learning_rate': 0.0, 'epoch': 0.42}
[2024-05-14 13:44:04,428] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1340
[2024-05-14 13:44:04,428] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 42%|████▏     | 1342/3218 [45:06<47:59,  1.53s/it]
[2024-05-14 13:44:05,835] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1341
[2024-05-14 13:44:05,836] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 42%|████▏     | 1343/3218 [45:08<47:44,  1.53s/it]
[2024-05-14 13:44:07,316] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1342
[2024-05-14 13:44:07,316] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 42%|████▏     | 1344/3218 [45:09<47:53,  1.53s/it]
[2024-05-14 13:44:08,926] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1343
[2024-05-14 13:44:08,927] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:44:08,927] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 13:44:10,451] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1344
[2024-05-14 13:44:10,451] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 42%|████▏     | 1346/3218 [45:12<47:41,  1.53s/it]
[2024-05-14 13:44:11,957] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1345
[2024-05-14 13:44:11,958] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 42%|████▏     | 1348/3218 [45:15<41:40,  1.34s/it]
[2024-05-14 13:44:13,061] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1346
[2024-05-14 13:44:13,062] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:44:13,062] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 13:44:14,243] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1347
[2024-05-14 13:44:14,243] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 42%|████▏     | 1349/3218 [45:16<41:14,  1.32s/it]
[2024-05-14 13:44:15,517] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1348
[2024-05-14 13:44:15,517] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 42%|████▏     | 1351/3218 [45:19<41:27,  1.33s/it]
[2024-05-14 13:44:16,995] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1349
[2024-05-14 13:44:16,996] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:44:16,998] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
{'loss': 0.0, 'learning_rate': 0.0, 'epoch': 0.42}
[2024-05-14 13:44:18,246] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1350
[2024-05-14 13:44:18,247] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 42%|████▏     | 1352/3218 [45:20<43:14,  1.39s/it]
[2024-05-14 13:44:19,795] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1351
[2024-05-14 13:44:19,795] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 42%|████▏     | 1354/3218 [45:22<37:11,  1.20s/it]
[2024-05-14 13:44:20,970] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1352
[2024-05-14 13:44:20,970] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:44:20,970] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 13:44:21,875] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1353
[2024-05-14 13:44:21,875] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 42%|████▏     | 1356/3218 [45:25<36:42,  1.18s/it]
[2024-05-14 13:44:23,277] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1354
[2024-05-14 13:44:23,278] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:44:23,279] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 13:44:24,270] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1355
[2024-05-14 13:44:24,270] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 42%|████▏     | 1357/3218 [45:26<38:28,  1.24s/it]
[2024-05-14 13:44:25,622] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1356
[2024-05-14 13:44:25,622] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:44:25,622] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 13:44:26,558] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1357
[2024-05-14 13:44:26,559] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 42%|████▏     | 1359/3218 [45:28<34:40,  1.12s/it]
[2024-05-14 13:44:27,641] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1358
[2024-05-14 13:44:27,641] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:44:27,642] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 13:44:28,504] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1359
[2024-05-14 13:44:28,504] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 42%|████▏     | 1361/3218 [45:30<32:12,  1.04s/it]
{'loss': 0.0, 'learning_rate': 0.0, 'epoch': 0.42}
[2024-05-14 13:44:29,532] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1360
[2024-05-14 13:44:29,532] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:44:29,532] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 13:44:30,534] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1361
[2024-05-14 13:44:30,534] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 42%|████▏     | 1363/3218 [45:32<30:16,  1.02it/s]
[2024-05-14 13:44:31,407] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1362
[2024-05-14 13:44:31,408] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:44:31,408] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 13:44:32,386] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1363
[2024-05-14 13:44:32,387] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 42%|████▏     | 1365/3218 [45:34<29:35,  1.04it/s]
[2024-05-14 13:44:33,321] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1364
[2024-05-14 13:44:33,322] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 42%|████▏     | 1366/3218 [45:36<38:44,  1.26s/it]
[2024-05-14 13:44:35,231] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1365
[2024-05-14 13:44:35,231] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 43%|████▎     | 1368/3218 [45:38<37:40,  1.22s/it]
[2024-05-14 13:44:37,289] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1366
[2024-05-14 13:44:37,289] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:44:37,289] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 13:44:38,045] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1367
[2024-05-14 13:44:38,045] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 43%|████▎     | 1369/3218 [45:40<41:21,  1.34s/it]
[2024-05-14 13:44:39,499] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1368
[2024-05-14 13:44:39,500] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 43%|████▎     | 1370/3218 [45:42<46:54,  1.52s/it]
[2024-05-14 13:44:41,449] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1369
[2024-05-14 13:44:41,450] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:44:41,451] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25

 43%|████▎     | 1371/3218 [45:44<50:08,  1.63s/it]
[2024-05-14 13:44:43,319] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1370
[2024-05-14 13:44:43,319] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 43%|████▎     | 1372/3218 [45:45<48:19,  1.57s/it]
[2024-05-14 13:44:44,794] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1371
[2024-05-14 13:44:44,795] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 43%|████▎     | 1373/3218 [45:48<54:29,  1.77s/it]
[2024-05-14 13:44:47,042] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1372
[2024-05-14 13:44:47,042] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 43%|████▎     | 1375/3218 [45:51<51:18,  1.67s/it]
[2024-05-14 13:44:48,791] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1373
[2024-05-14 13:44:48,791] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:44:48,792] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 13:44:50,222] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1374
[2024-05-14 13:44:50,223] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 43%|████▎     | 1376/3218 [45:52<48:37,  1.58s/it]
[2024-05-14 13:44:51,595] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1375
[2024-05-14 13:44:51,595] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 43%|████▎     | 1377/3218 [45:54<47:31,  1.55s/it]
[2024-05-14 13:44:53,027] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1376
[2024-05-14 13:44:53,027] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:44:53,028] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 13:44:54,441] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1377
[2024-05-14 13:44:54,442] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 43%|████▎     | 1378/3218 [45:55<45:41,  1.49s/it]
[2024-05-14 13:44:56,547] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1378
[2024-05-14 13:44:56,548] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 43%|████▎     | 1380/3218 [45:58<48:56,  1.60s/it]
[2024-05-14 13:44:57,992] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1379
[2024-05-14 13:44:57,993] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:44:57,994] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25

 43%|████▎     | 1381/3218 [46:00<45:39,  1.49s/it]
[2024-05-14 13:44:59,189] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1380
[2024-05-14 13:44:59,189] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:44:59,189] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 13:45:00,262] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1381
[2024-05-14 13:45:00,361] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 43%|████▎     | 1383/3218 [46:02<40:56,  1.34s/it]
[2024-05-14 13:45:01,590] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1382
[2024-05-14 13:45:01,591] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 43%|████▎     | 1385/3218 [46:05<41:47,  1.37s/it]
[2024-05-14 13:45:02,821] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1383
[2024-05-14 13:45:02,822] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:45:02,822] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 13:45:04,316] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1384
[2024-05-14 13:45:04,317] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 43%|████▎     | 1386/3218 [46:06<40:52,  1.34s/it]
[2024-05-14 13:45:05,616] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1385
[2024-05-14 13:45:05,616] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 43%|████▎     | 1387/3218 [46:08<42:57,  1.41s/it]
[2024-05-14 13:45:07,140] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1386
[2024-05-14 13:45:07,140] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:45:07,140] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 13:45:08,497] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1387
[2024-05-14 13:45:08,498] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 43%|████▎     | 1389/3218 [46:10<40:15,  1.32s/it]
[2024-05-14 13:45:09,664] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1388
[2024-05-14 13:45:09,664] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 43%|████▎     | 1391/3218 [46:12<36:41,  1.20s/it]
[2024-05-14 13:45:10,830] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1389
[2024-05-14 13:45:10,830] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:45:10,831] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
{'loss': 0.0, 'learning_rate': 0.0, 'epoch': 0.43}
[2024-05-14 13:45:11,851] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1390
[2024-05-14 13:45:11,851] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 43%|████▎     | 1392/3218 [46:14<37:43,  1.24s/it]
[2024-05-14 13:45:13,200] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1391
[2024-05-14 13:45:13,200] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:45:13,202] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 13:45:14,405] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1392
[2024-05-14 13:45:14,405] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 43%|████▎     | 1394/3218 [46:16<37:57,  1.25s/it]
[2024-05-14 13:45:15,671] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1393
[2024-05-14 13:45:15,672] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 43%|████▎     | 1395/3218 [46:18<38:22,  1.26s/it]
[2024-05-14 13:45:16,998] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1394
[2024-05-14 13:45:16,998] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:45:16,998] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 13:45:18,484] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1395
[2024-05-14 13:45:18,485] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 43%|████▎     | 1397/3218 [46:20<37:57,  1.25s/it]
[2024-05-14 13:45:19,551] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1396
[2024-05-14 13:45:19,552] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:45:19,556] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 13:45:20,586] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1397
[2024-05-14 13:45:20,586] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 43%|████▎     | 1399/3218 [46:22<34:40,  1.14s/it]
[2024-05-14 13:45:21,633] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1398
[2024-05-14 13:45:21,634] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:45:21,634] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 13:45:22,601] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1399
[2024-05-14 13:45:22,601] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
 44%|████▎     | 1400/3218 [46:23<32:37,  1.08s/it][INFO|trainer.py:3166] 2024-05-14 13:45:22,731 >> ***** Running Evaluation *****
[INFO|trainer.py:3168] 2024-05-14 13:45:22,732 >>   Num examples = 521
[INFO|trainer.py:3171] 2024-05-14 13:45:22,732 >>   Batch size = 8
 15%|█▌        | 5/33 [00:01<00:09,  3.01it/s]






 97%|█████████▋| 32/33 [00:11<00:00,  2.77it/s]
 44%|████▎     | 1400/3218 [46:36<32:37,  1.08s[INFO|trainer.py:2889] 2024-05-14 13:45:40,913 >> Saving model checkpoint to outputs/instruct_tuning/model/tmp-checkpoint-1400
[INFO|configuration_utils.py:483] 2024-05-14 13:45:40,915 >> Configuration saved in outputs/instruct_tuning/model/tmp-checkpoint-1400/config.json
[INFO|configuration_utils.py:594] 2024-05-14 13:45:40,916 >> Configuration saved in outputs/instruct_tuning/model/tmp-checkpoint-1400/generation_config.json
[INFO|modeling_utils.py:2390] 2024-05-14 13:46:05,029 >> The model is bigger than the maximum size per checkpoint (5GB) and is going to be split in 3 checkpoint shards. You can find where each parameters has been saved in the index located at outputs/instruct_tuning/model/tmp-checkpoint-1400/model.safetensors.index.json.
[INFO|tokenization_utils_base.py:2432] 2024-05-14 13:46:05,030 >> tokenizer config file saved in outputs/instruct_tuning/model/tmp-checkpoint-1400/tokenizer_config.json
[INFO|tokenization_utils_base.py:2441] 2024-05-14 13:46:05,031 >> Special tokens file saved in outputs/instruct_tuning/model/tmp-checkpoint-1400/special_tokens_map.json
/mounts/work/lyk/anaconda3/envs/torch2/lib/python3.8/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
[2024-05-14 13:46:05,880] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step1400 is about to be saved!
[2024-05-14 13:46:05,886] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: outputs/instruct_tuning/model/tmp-checkpoint-1400/global_step1400/mp_rank_00_model_states.pt
[2024-05-14 13:46:05,886] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving outputs/instruct_tuning/model/tmp-checkpoint-1400/global_step1400/mp_rank_00_model_states.pt...
[INFO|trainer.py:2979] 2024-05-14 13:47:07,297 >> Deleting older checkpoint [outputs/instruct_tuning/model/checkpoint-1000] due to args.save_total_limit
[2024-05-14 13:47:07,280] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved outputs/instruct_tuning/model/tmp-checkpoint-1400/global_step1400/mp_rank_00_model_states.pt.
[2024-05-14 13:47:07,282] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step1400 is ready now!
 44%|████▎     | 1401/3218 [48:13<16:58:58, 33.65s/it]
[2024-05-14 13:47:12,245] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1400
[2024-05-14 13:47:12,245] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 44%|████▎     | 1403/3218 [48:15<8:34:50, 17.02s/it]
[2024-05-14 13:47:13,934] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1401
[2024-05-14 13:47:13,935] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:47:13,935] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 13:47:14,666] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1402
[2024-05-14 13:47:14,666] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:47:14,667] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 13:47:16,795] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1403
[2024-05-14 13:47:16,796] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25


 44%|████▎     | 1405/3218 [48:19<4:45:55,  9.46s/it]
[2024-05-14 13:47:18,948] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1404
[2024-05-14 13:47:18,948] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 44%|████▎     | 1406/3218 [48:22<3:40:22,  7.30s/it]
[2024-05-14 13:47:21,116] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1405
[2024-05-14 13:47:21,116] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 44%|████▎     | 1407/3218 [48:24<2:51:34,  5.68s/it]
[2024-05-14 13:47:23,035] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1406
[2024-05-14 13:47:23,035] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 44%|████▍     | 1408/3218 [48:26<2:17:22,  4.55s/it]
[2024-05-14 13:47:25,031] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1407
[2024-05-14 13:47:25,031] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:47:25,034] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 13:47:26,705] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1408
[2024-05-14 13:47:26,706] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 44%|████▍     | 1410/3218 [48:29<1:31:53,  3.05s/it]
[2024-05-14 13:47:28,274] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1409
[2024-05-14 13:47:28,275] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:47:28,275] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25

 44%|████▍     | 1411/3218 [48:30<1:17:37,  2.58s/it]
[2024-05-14 13:47:29,739] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1410
[2024-05-14 13:47:29,739] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 44%|████▍     | 1412/3218 [48:31<1:05:17,  2.17s/it]
[2024-05-14 13:47:30,984] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1411
[2024-05-14 13:47:30,985] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:47:30,986] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 13:47:32,690] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1412
[2024-05-14 13:47:32,691] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 44%|████▍     | 1414/3218 [48:35<55:37,  1.85s/it]
[2024-05-14 13:47:34,101] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1413
[2024-05-14 13:47:34,101] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 44%|████▍     | 1415/3218 [48:36<51:22,  1.71s/it]
[2024-05-14 13:47:35,551] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1414
[2024-05-14 13:47:35,551] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 44%|████▍     | 1416/3218 [48:38<54:12,  1.81s/it]
[2024-05-14 13:47:37,461] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1415
[2024-05-14 13:47:37,462] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 44%|████▍     | 1418/3218 [48:41<46:40,  1.56s/it]
[2024-05-14 13:47:38,897] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1416
[2024-05-14 13:47:38,897] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:47:38,898] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 13:47:40,149] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1417
[2024-05-14 13:47:40,150] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 44%|████▍     | 1419/3218 [48:43<50:47,  1.69s/it]
[2024-05-14 13:47:42,100] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1418
[2024-05-14 13:47:42,101] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 44%|████▍     | 1420/3218 [48:44<46:50,  1.56s/it]
[2024-05-14 13:47:43,411] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1419
[2024-05-14 13:47:43,412] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:47:43,414] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
{'loss': 0.0, 'learning_rate': 0.0, 'epoch': 0.44}
[2024-05-14 13:47:44,662] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1420
[2024-05-14 13:47:44,663] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 44%|████▍     | 1422/3218 [48:47<46:55,  1.57s/it]
[2024-05-14 13:47:46,401] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1421
[2024-05-14 13:47:46,401] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 44%|████▍     | 1423/3218 [48:48<43:10,  1.44s/it]
[2024-05-14 13:47:47,618] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1422
[2024-05-14 13:47:47,619] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 44%|████▍     | 1425/3218 [48:51<40:22,  1.35s/it]
[2024-05-14 13:47:48,950] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1423
[2024-05-14 13:47:48,951] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:47:48,951] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 13:47:50,167] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1424
[2024-05-14 13:47:50,168] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 44%|████▍     | 1426/3218 [48:52<40:40,  1.36s/it]
[2024-05-14 13:47:51,562] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1425
[2024-05-14 13:47:51,562] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:47:51,562] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 13:47:52,907] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1426
[2024-05-14 13:47:52,908] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 44%|████▍     | 1428/3218 [48:55<38:19,  1.28s/it]
[2024-05-14 13:47:54,029] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1427
[2024-05-14 13:47:54,030] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 44%|████▍     | 1430/3218 [48:57<35:38,  1.20s/it]
[2024-05-14 13:47:55,204] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1428
[2024-05-14 13:47:55,204] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:47:55,204] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 13:47:56,253] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1429
[2024-05-14 13:47:56,254] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:47:56,254] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25

 44%|████▍     | 1432/3218 [48:59<34:51,  1.17s/it]
[2024-05-14 13:47:57,313] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1430
[2024-05-14 13:47:57,313] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:47:57,313] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 13:47:58,525] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1431
[2024-05-14 13:47:58,525] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 45%|████▍     | 1433/3218 [49:00<33:45,  1.13s/it]
[2024-05-14 13:47:59,580] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1432
[2024-05-14 13:47:59,581] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:47:59,581] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 13:48:00,630] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1433
[2024-05-14 13:48:00,630] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 45%|████▍     | 1435/3218 [49:02<33:59,  1.14s/it]
[2024-05-14 13:48:01,878] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1434
[2024-05-14 13:48:01,878] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 45%|████▍     | 1436/3218 [49:04<38:36,  1.30s/it]
[2024-05-14 13:48:03,572] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1435
[2024-05-14 13:48:03,573] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 45%|████▍     | 1438/3218 [49:07<37:28,  1.26s/it]
[2024-05-14 13:48:05,422] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1436
[2024-05-14 13:48:05,423] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:48:05,423] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 13:48:06,356] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1437
[2024-05-14 13:48:06,356] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 45%|████▍     | 1439/3218 [49:08<38:05,  1.28s/it]
[2024-05-14 13:48:07,531] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1438
[2024-05-14 13:48:07,532] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 45%|████▍     | 1440/3218 [49:10<41:58,  1.42s/it]
[2024-05-14 13:48:09,252] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1439
[2024-05-14 13:48:09,252] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:48:09,252] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
{'loss': 0.0, 'learning_rate': 0.0, 'epoch': 0.45}
[2024-05-14 13:48:10,789] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1440
[2024-05-14 13:48:10,789] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 45%|████▍     | 1442/3218 [49:13<43:33,  1.47s/it]
[2024-05-14 13:48:12,326] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1441
[2024-05-14 13:48:12,331] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 45%|████▍     | 1443/3218 [49:14<44:56,  1.52s/it]
[2024-05-14 13:48:13,921] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1442
[2024-05-14 13:48:13,922] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 45%|████▍     | 1444/3218 [49:16<46:47,  1.58s/it]
[2024-05-14 13:48:15,637] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1443
[2024-05-14 13:48:15,637] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 45%|████▍     | 1445/3218 [49:18<49:38,  1.68s/it]
[2024-05-14 13:48:17,549] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1444
[2024-05-14 13:48:17,550] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 45%|████▍     | 1447/3218 [49:21<46:47,  1.59s/it]
[2024-05-14 13:48:19,080] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1445
[2024-05-14 13:48:19,080] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:48:19,080] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 13:48:20,590] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1446
[2024-05-14 13:48:20,591] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 45%|████▍     | 1448/3218 [49:23<47:25,  1.61s/it]
[2024-05-14 13:48:22,238] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1447
[2024-05-14 13:48:22,238] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 45%|████▌     | 1449/3218 [49:25<49:49,  1.69s/it]
[2024-05-14 13:48:24,058] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1448
[2024-05-14 13:48:24,059] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 45%|████▌     | 1450/3218 [49:26<45:28,  1.54s/it]
[2024-05-14 13:48:25,315] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1449
[2024-05-14 13:48:25,315] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:48:25,316] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
{'loss': 0.0, 'learning_rate': 0.0, 'epoch': 0.45}
[2024-05-14 13:48:26,635] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1450
[2024-05-14 13:48:26,636] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 45%|████▌     | 1451/3218 [49:27<43:14,  1.47s/it]
[2024-05-14 13:48:28,717] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1451
[2024-05-14 13:48:28,718] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 45%|████▌     | 1453/3218 [49:31<45:56,  1.56s/it]
[2024-05-14 13:48:30,059] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1452
[2024-05-14 13:48:30,059] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 45%|████▌     | 1454/3218 [49:32<43:23,  1.48s/it]
[2024-05-14 13:48:31,332] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1453
[2024-05-14 13:48:31,333] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:48:31,333] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 13:48:32,657] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1454
[2024-05-14 13:48:32,657] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 45%|████▌     | 1456/3218 [49:34<39:58,  1.36s/it]
[2024-05-14 13:48:33,863] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1455
[2024-05-14 13:48:33,863] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 45%|████▌     | 1458/3218 [49:37<38:49,  1.32s/it]
[2024-05-14 13:48:35,130] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1456
[2024-05-14 13:48:35,130] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:48:35,131] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 13:48:36,425] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1457
[2024-05-14 13:48:36,425] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 45%|████▌     | 1459/3218 [49:38<39:04,  1.33s/it]
[2024-05-14 13:48:37,770] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1458
[2024-05-14 13:48:37,770] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:48:37,771] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 13:48:38,958] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1459
[2024-05-14 13:48:38,959] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 45%|████▌     | 1461/3218 [49:41<37:02,  1.26s/it]
{'loss': 0.0, 'learning_rate': 0.0, 'epoch': 0.45}
[2024-05-14 13:48:40,157] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1460
[2024-05-14 13:48:40,158] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 45%|████▌     | 1463/3218 [49:43<34:59,  1.20s/it]
[2024-05-14 13:48:41,385] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1461
[2024-05-14 13:48:41,385] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:48:41,391] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 13:48:42,453] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1462
[2024-05-14 13:48:42,454] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 45%|████▌     | 1464/3218 [49:44<35:58,  1.23s/it]
[2024-05-14 13:48:43,777] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1463
[2024-05-14 13:48:43,777] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:48:43,777] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 13:48:44,847] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1464
[2024-05-14 13:48:44,848] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 46%|████▌     | 1466/3218 [49:47<36:53,  1.26s/it]
[2024-05-14 13:48:46,314] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1465
[2024-05-14 13:48:46,314] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 46%|████▌     | 1467/3218 [49:48<35:28,  1.22s/it]
[2024-05-14 13:48:47,399] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1466
[2024-05-14 13:48:47,400] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:48:47,400] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 13:48:48,679] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1467
[2024-05-14 13:48:48,679] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 46%|████▌     | 1469/3218 [49:50<35:30,  1.22s/it]
[2024-05-14 13:48:49,869] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1468
[2024-05-14 13:48:49,870] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:48:49,870] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 13:48:50,891] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1469
[2024-05-14 13:48:50,891] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 46%|████▌     | 1470/3218 [49:51<33:35,  1.15s/it]

 46%|████▌     | 1471/3218 [49:54<44:34,  1.53s/it]
[2024-05-14 13:48:53,313] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1470
[2024-05-14 13:48:53,313] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:48:53,313] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 13:48:54,737] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1471
[2024-05-14 13:48:54,737] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 46%|████▌     | 1474/3218 [49:57<36:38,  1.26s/it]
[2024-05-14 13:48:55,406] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1472
[2024-05-14 13:48:55,406] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:48:55,406] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 13:48:56,630] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1473
[2024-05-14 13:48:56,630] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 46%|████▌     | 1475/3218 [49:59<42:24,  1.46s/it]
[2024-05-14 13:48:58,559] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1474
[2024-05-14 13:48:58,560] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 46%|████▌     | 1476/3218 [50:01<44:00,  1.52s/it]
[2024-05-14 13:49:00,214] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1475
[2024-05-14 13:49:00,215] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 46%|████▌     | 1477/3218 [50:03<47:08,  1.62s/it]
[2024-05-14 13:49:02,112] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1476
[2024-05-14 13:49:02,112] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 46%|████▌     | 1478/3218 [50:04<48:47,  1.68s/it]
[2024-05-14 13:49:03,885] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1477
[2024-05-14 13:49:03,885] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 46%|████▌     | 1479/3218 [50:06<47:32,  1.64s/it]
[2024-05-14 13:49:05,397] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1478
[2024-05-14 13:49:05,397] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:49:05,397] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 13:49:06,976] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1479
[2024-05-14 13:49:06,977] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 46%|████▌     | 1481/3218 [50:09<45:58,  1.59s/it]
{'loss': 0.0, 'learning_rate': 0.0, 'epoch': 0.46}
[2024-05-14 13:49:08,499] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1480
[2024-05-14 13:49:08,500] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 46%|████▌     | 1482/3218 [50:11<46:10,  1.60s/it]
[2024-05-14 13:49:10,125] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1481
[2024-05-14 13:49:10,126] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 46%|████▌     | 1483/3218 [50:12<42:56,  1.48s/it]
[2024-05-14 13:49:11,365] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1482
[2024-05-14 13:49:11,366] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:49:11,367] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 13:49:12,712] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1483
[2024-05-14 13:49:12,713] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 46%|████▌     | 1485/3218 [50:15<41:45,  1.45s/it]
[2024-05-14 13:49:14,186] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1484
[2024-05-14 13:49:14,187] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 46%|████▌     | 1486/3218 [50:16<40:18,  1.40s/it]
[2024-05-14 13:49:15,484] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1485
[2024-05-14 13:49:15,484] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:49:15,485] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 13:49:16,838] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1486
[2024-05-14 13:49:16,838] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 46%|████▌     | 1487/3218 [50:17<40:10,  1.39s/it]
[2024-05-14 13:49:18,754] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1487
[2024-05-14 13:49:18,755] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 46%|████▋     | 1489/3218 [50:21<41:58,  1.46s/it]
[2024-05-14 13:49:20,012] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1488
[2024-05-14 13:49:20,013] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 46%|████▋     | 1491/3218 [50:23<37:55,  1.32s/it]
[2024-05-14 13:49:21,205] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1489
[2024-05-14 13:49:21,205] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:49:21,205] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
{'loss': 0.0, 'learning_rate': 0.0, 'epoch': 0.46}
[2024-05-14 13:49:22,399] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1490
[2024-05-14 13:49:22,399] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 46%|████▋     | 1492/3218 [50:24<37:12,  1.29s/it]
[2024-05-14 13:49:23,618] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1491
[2024-05-14 13:49:23,619] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:49:23,630] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 13:49:24,876] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1492
[2024-05-14 13:49:24,877] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 46%|████▋     | 1494/3218 [50:27<41:07,  1.43s/it]
[2024-05-14 13:49:26,625] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1493
[2024-05-14 13:49:26,626] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 46%|████▋     | 1495/3218 [50:28<38:01,  1.32s/it]
[2024-05-14 13:49:27,707] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1494
[2024-05-14 13:49:27,708] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 46%|████▋     | 1496/3218 [50:30<41:51,  1.46s/it]
[2024-05-14 13:49:29,423] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1495
[2024-05-14 13:49:29,423] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:49:29,424] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 13:49:30,866] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1496
[2024-05-14 13:49:30,866] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 47%|████▋     | 1498/3218 [50:33<39:20,  1.37s/it]
[2024-05-14 13:49:32,094] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1497
[2024-05-14 13:49:32,094] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
 47%|████▋     | 1500/3218 [50:35<35:48,  1.25s/it][INFO|trainer.py:3166] 2024-05-14 13:49:34,540 >> ***** Running Evaluation *****
[INFO|trainer.py:3168] 2024-05-14 13:49:34,540 >>   Num examples = 521
[INFO|trainer.py:3171] 2024-05-14 13:49:34,540 >>   Batch size = 8
  6%|▌         | 2/33 [00:00<00:03,  8.49it/s]
[2024-05-14 13:49:33,288] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1498
[2024-05-14 13:49:33,289] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:49:33,289] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 13:49:34,359] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1499
[2024-05-14 13:49:34,359] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:49:34,359] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25






100%|██████████| 33/33 [00:11<00:00,  2.32it/s]

{'eval_loss': nan, 'eval_runtime': 12.4125, 'eval_samples_per_second': 41.974, 'eval_steps_per_second': 2.659, 'epoch': 0.47}
[2024-05-14 13:49:48,045] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1500
[2024-05-14 13:49:48,045] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 47%|████▋     | 1503/3218 [50:51<1:29:43,  3.14s/it]
[2024-05-14 13:49:49,197] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1501
[2024-05-14 13:49:49,198] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:49:49,198] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 13:49:50,677] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1502
[2024-05-14 13:49:50,677] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 47%|████▋     | 1505/3218 [50:53<57:49,  2.03s/it]
[2024-05-14 13:49:51,698] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1503
[2024-05-14 13:49:51,699] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:49:51,699] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 13:49:52,672] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1504
[2024-05-14 13:49:52,672] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 47%|████▋     | 1506/3218 [50:55<51:54,  1.82s/it]
[2024-05-14 13:49:53,993] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1505
[2024-05-14 13:49:53,993] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 47%|████▋     | 1508/3218 [50:57<38:38,  1.36s/it]
[2024-05-14 13:49:55,433] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1506
[2024-05-14 13:49:55,434] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:49:55,434] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 13:49:56,130] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1507
[2024-05-14 13:49:56,130] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 47%|████▋     | 1509/3218 [50:58<39:10,  1.38s/it]
[2024-05-14 13:49:57,399] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1508
[2024-05-14 13:49:57,399] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 47%|████▋     | 1510/3218 [51:00<46:37,  1.64s/it]
[2024-05-14 13:49:59,600] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1509
[2024-05-14 13:49:59,601] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:49:59,602] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25

 47%|████▋     | 1512/3218 [51:03<43:45,  1.54s/it]
[2024-05-14 13:50:01,211] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1510
[2024-05-14 13:50:01,211] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:50:01,211] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 13:50:02,607] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1511
[2024-05-14 13:50:02,608] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 47%|████▋     | 1513/3218 [51:05<43:07,  1.52s/it]
[2024-05-14 13:50:04,012] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1512
[2024-05-14 13:50:04,013] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 47%|████▋     | 1514/3218 [51:06<42:44,  1.51s/it]
[2024-05-14 13:50:05,470] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1513
[2024-05-14 13:50:05,470] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:50:05,471] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 13:50:07,035] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1514
[2024-05-14 13:50:07,035] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 47%|████▋     | 1516/3218 [51:09<42:10,  1.49s/it]
[2024-05-14 13:50:08,464] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1515
[2024-05-14 13:50:08,465] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 47%|████▋     | 1517/3218 [51:11<45:23,  1.60s/it]
[2024-05-14 13:50:10,318] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1516
[2024-05-14 13:50:10,318] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 47%|████▋     | 1518/3218 [51:12<44:09,  1.56s/it]
[2024-05-14 13:50:11,751] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1517
[2024-05-14 13:50:11,751] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 47%|████▋     | 1520/3218 [51:15<41:03,  1.45s/it]
[2024-05-14 13:50:13,292] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1518
[2024-05-14 13:50:13,293] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:50:13,293] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 13:50:14,518] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1519
[2024-05-14 13:50:14,519] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:50:14,520] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25

 47%|████▋     | 1521/3218 [51:16<39:50,  1.41s/it]
[2024-05-14 13:50:15,851] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1520
[2024-05-14 13:50:15,851] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:50:15,851] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 13:50:17,025] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1521
[2024-05-14 13:50:17,026] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 47%|████▋     | 1522/3218 [51:18<37:47,  1.34s/it]
[2024-05-14 13:50:18,856] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1522
[2024-05-14 13:50:18,856] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 47%|████▋     | 1524/3218 [51:21<42:55,  1.52s/it]
[2024-05-14 13:50:20,418] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1523
[2024-05-14 13:50:20,418] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 47%|████▋     | 1525/3218 [51:23<43:29,  1.54s/it]
[2024-05-14 13:50:21,990] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1524
[2024-05-14 13:50:21,991] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 47%|████▋     | 1527/3218 [51:25<38:37,  1.37s/it]
[2024-05-14 13:50:23,231] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1525
[2024-05-14 13:50:23,231] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:50:23,231] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 13:50:24,455] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1526
[2024-05-14 13:50:24,456] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 47%|████▋     | 1528/3218 [51:27<40:27,  1.44s/it]
[2024-05-14 13:50:26,033] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1527
[2024-05-14 13:50:26,033] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 48%|████▊     | 1530/3218 [51:29<38:59,  1.39s/it]
[2024-05-14 13:50:27,455] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1528
[2024-05-14 13:50:27,455] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:50:27,455] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 13:50:28,753] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1529
[2024-05-14 13:50:28,753] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:50:28,753] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25

 48%|████▊     | 1531/3218 [51:30<36:51,  1.31s/it]
[2024-05-14 13:50:29,902] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1530
[2024-05-14 13:50:29,902] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:50:29,902] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 13:50:31,105] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1531
[2024-05-14 13:50:31,105] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 48%|████▊     | 1533/3218 [51:33<34:51,  1.24s/it]
[2024-05-14 13:50:32,266] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1532
[2024-05-14 13:50:32,266] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 48%|████▊     | 1534/3218 [51:34<37:10,  1.32s/it]
[2024-05-14 13:50:33,780] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1533
[2024-05-14 13:50:33,780] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:50:33,780] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 13:50:34,978] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1534
[2024-05-14 13:50:34,979] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 48%|████▊     | 1536/3218 [51:37<35:20,  1.26s/it]
[2024-05-14 13:50:36,184] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1535
[2024-05-14 13:50:36,185] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 48%|████▊     | 1538/3218 [51:39<33:31,  1.20s/it]
[2024-05-14 13:50:37,348] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1536
[2024-05-14 13:50:37,349] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:50:37,349] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 13:50:38,478] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1537
[2024-05-14 13:50:38,478] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 48%|████▊     | 1540/3218 [51:41<30:06,  1.08s/it]
[2024-05-14 13:50:39,614] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1538
[2024-05-14 13:50:39,615] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:50:39,615] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 13:50:40,468] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1539
[2024-05-14 13:50:40,468] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:50:40,468] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25

 48%|████▊     | 1541/3218 [51:43<34:28,  1.23s/it]
[2024-05-14 13:50:42,032] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1540
[2024-05-14 13:50:42,033] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 48%|████▊     | 1543/3218 [51:45<32:11,  1.15s/it]
[2024-05-14 13:50:43,647] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1541
[2024-05-14 13:50:43,648] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:50:43,648] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 13:50:44,522] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1542
[2024-05-14 13:50:44,523] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 48%|████▊     | 1544/3218 [51:46<33:28,  1.20s/it]
[2024-05-14 13:50:45,665] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1543
[2024-05-14 13:50:45,665] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 48%|████▊     | 1545/3218 [51:48<37:01,  1.33s/it]
[2024-05-14 13:50:47,266] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1544
[2024-05-14 13:50:47,267] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:50:47,267] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 13:50:48,946] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1545
[2024-05-14 13:50:48,947] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 48%|████▊     | 1546/3218 [51:50<40:00,  1.44s/it]
[2024-05-14 13:50:50,864] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1546
[2024-05-14 13:50:50,865] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 48%|████▊     | 1548/3218 [51:53<44:17,  1.59s/it]
[2024-05-14 13:50:52,452] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1547
[2024-05-14 13:50:52,453] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 48%|████▊     | 1549/3218 [51:54<42:34,  1.53s/it]
[2024-05-14 13:50:53,878] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1548
[2024-05-14 13:50:53,878] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:50:53,880] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 13:50:55,195] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1549
[2024-05-14 13:50:55,196] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 48%|████▊     | 1550/3218 [51:56<40:52,  1.47s/it]
{'loss': 0.0, 'learning_rate': 0.0, 'epoch': 0.48}
[2024-05-14 13:50:56,974] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1550
[2024-05-14 13:50:56,974] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 48%|████▊     | 1552/3218 [51:59<41:19,  1.49s/it]
[2024-05-14 13:50:58,282] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1551
[2024-05-14 13:50:58,283] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 48%|████▊     | 1554/3218 [52:01<37:34,  1.36s/it]
[2024-05-14 13:50:59,552] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1552
[2024-05-14 13:50:59,552] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:50:59,553] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 13:51:00,788] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1553
[2024-05-14 13:51:00,789] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 48%|████▊     | 1555/3218 [52:03<37:41,  1.36s/it]
[2024-05-14 13:51:02,122] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1554
[2024-05-14 13:51:02,123] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 48%|████▊     | 1557/3218 [52:05<36:54,  1.33s/it]
[2024-05-14 13:51:03,429] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1555
[2024-05-14 13:51:03,430] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:51:03,433] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 13:51:04,791] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1556
[2024-05-14 13:51:04,791] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 48%|████▊     | 1558/3218 [52:07<36:24,  1.32s/it]
[2024-05-14 13:51:06,039] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1557
[2024-05-14 13:51:06,040] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 48%|████▊     | 1560/3218 [52:09<35:22,  1.28s/it]
[2024-05-14 13:51:07,405] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1558
[2024-05-14 13:51:07,406] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:51:07,407] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 13:51:08,542] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1559
[2024-05-14 13:51:08,542] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:51:08,543] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25

 49%|████▊     | 1561/3218 [52:10<34:19,  1.24s/it]
[2024-05-14 13:51:09,728] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1560
[2024-05-14 13:51:09,728] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:51:09,728] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 13:51:10,980] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1561
[2024-05-14 13:51:10,981] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 49%|████▊     | 1563/3218 [52:13<34:14,  1.24s/it]
[2024-05-14 13:51:12,185] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1562
[2024-05-14 13:51:12,186] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:51:12,186] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 13:51:13,292] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1563
[2024-05-14 13:51:13,293] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 49%|████▊     | 1565/3218 [52:15<32:07,  1.17s/it]
[2024-05-14 13:51:14,383] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1564
[2024-05-14 13:51:14,383] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 49%|████▊     | 1566/3218 [52:16<32:13,  1.17s/it]
[2024-05-14 13:51:15,527] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1565
[2024-05-14 13:51:15,528] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:51:15,528] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 13:51:16,901] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1566
[2024-05-14 13:51:16,901] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 49%|████▊     | 1568/3218 [52:19<34:15,  1.25s/it]
[2024-05-14 13:51:18,226] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1567
[2024-05-14 13:51:18,226] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:51:18,227] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 13:51:19,199] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1568
[2024-05-14 13:51:19,199] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 49%|████▉     | 1570/3218 [52:21<35:05,  1.28s/it]
[2024-05-14 13:51:20,700] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1569
[2024-05-14 13:51:20,701] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:51:20,701] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25

 49%|████▉     | 1572/3218 [52:23<31:40,  1.15s/it]
[2024-05-14 13:51:21,842] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1570
[2024-05-14 13:51:21,842] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:51:21,843] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 13:51:22,838] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1571
[2024-05-14 13:51:22,838] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 49%|████▉     | 1574/3218 [52:25<29:52,  1.09s/it]
[2024-05-14 13:51:23,838] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1572
[2024-05-14 13:51:23,839] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:51:23,839] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 13:51:24,884] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1573
[2024-05-14 13:51:24,884] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 49%|████▉     | 1575/3218 [52:26<29:09,  1.06s/it]
[2024-05-14 13:51:25,917] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1574
[2024-05-14 13:51:25,918] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 49%|████▉     | 1576/3218 [52:28<32:28,  1.19s/it]
[2024-05-14 13:51:27,376] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1575
[2024-05-14 13:51:27,377] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:51:27,377] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 13:51:28,983] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1576
[2024-05-14 13:51:28,983] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 49%|████▉     | 1578/3218 [52:30<31:01,  1.13s/it]
[2024-05-14 13:51:29,848] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1577
[2024-05-14 13:51:29,848] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 49%|████▉     | 1579/3218 [52:32<35:31,  1.30s/it]
[2024-05-14 13:51:31,407] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1578
[2024-05-14 13:51:31,408] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:51:31,413] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 13:51:33,216] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1579
[2024-05-14 13:51:33,216] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 49%|████▉     | 1581/3218 [52:35<40:07,  1.47s/it]
{'loss': 0.0, 'learning_rate': 0.0, 'epoch': 0.49}
[2024-05-14 13:51:34,757] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1580
[2024-05-14 13:51:34,758] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 49%|████▉     | 1582/3218 [52:37<39:10,  1.44s/it]
[2024-05-14 13:51:36,105] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1581
[2024-05-14 13:51:36,106] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 49%|████▉     | 1584/3218 [52:39<38:40,  1.42s/it]
[2024-05-14 13:51:37,453] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1582
[2024-05-14 13:51:37,454] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:51:37,455] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 13:51:38,895] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1583
[2024-05-14 13:51:38,896] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 49%|████▉     | 1585/3218 [52:41<40:33,  1.49s/it]
[2024-05-14 13:51:40,478] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1584
[2024-05-14 13:51:40,478] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 49%|████▉     | 1586/3218 [52:43<43:36,  1.60s/it]
[2024-05-14 13:51:42,358] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1585
[2024-05-14 13:51:42,359] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 49%|████▉     | 1588/3218 [52:45<38:33,  1.42s/it]
[2024-05-14 13:51:43,711] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1586
[2024-05-14 13:51:43,712] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:51:43,712] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 13:51:44,935] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1587
[2024-05-14 13:51:44,935] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 49%|████▉     | 1589/3218 [52:47<40:47,  1.50s/it]
[2024-05-14 13:51:46,621] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1588
[2024-05-14 13:51:46,622] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 49%|████▉     | 1590/3218 [52:49<42:54,  1.58s/it]
[2024-05-14 13:51:48,347] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1589
[2024-05-14 13:51:48,347] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:51:48,347] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25

 49%|████▉     | 1592/3218 [52:51<38:47,  1.43s/it]
[2024-05-14 13:51:49,663] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1590
[2024-05-14 13:51:49,663] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:51:49,664] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 13:51:50,978] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1591
[2024-05-14 13:51:50,979] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 50%|████▉     | 1593/3218 [52:53<40:52,  1.51s/it]
[2024-05-14 13:51:52,568] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1592
[2024-05-14 13:51:52,568] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 50%|████▉     | 1594/3218 [52:54<38:02,  1.41s/it]
[2024-05-14 13:51:53,773] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1593
[2024-05-14 13:51:53,773] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 50%|████▉     | 1595/3218 [52:56<42:04,  1.56s/it]
[2024-05-14 13:51:55,608] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1594
[2024-05-14 13:51:55,609] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:51:55,610] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 13:51:57,101] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1595
[2024-05-14 13:51:57,102] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 50%|████▉     | 1597/3218 [52:59<40:19,  1.49s/it]
[2024-05-14 13:51:58,518] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1596
[2024-05-14 13:51:58,519] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 50%|████▉     | 1598/3218 [53:00<38:06,  1.41s/it]
[2024-05-14 13:51:59,781] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1597
[2024-05-14 13:51:59,782] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:51:59,782] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 13:52:01,293] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1598
[2024-05-14 13:52:01,294] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
 50%|████▉     | 1600/3218 [53:03<35:48,  1.33s/it][INFO|trainer.py:3166] 2024-05-14 13:52:02,532 >> ***** Running Evaluation *****
[INFO|trainer.py:3168] 2024-05-14 13:52:02,532 >>   Num examples = 521
[INFO|trainer.py:3171] 2024-05-14 13:52:02,532 >>   Batch size = 8
  6%|▌         | 2/33 [00:00<00:03,  8.43it/s]
[2024-05-14 13:52:02,355] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1599
[2024-05-14 13:52:02,355] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:52:02,356] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25






 88%|████████▊ | 29/33 [00:10<00:01,  2.80it/s]
 50%|████▉     | 1600/3218 [53:15<35:48,  1.33s[INFO|trainer.py:2889] 2024-05-14 13:52:20,400 >> Saving model checkpoint to outputs/instruct_tuning/model/tmp-checkpoint-1600
[INFO|configuration_utils.py:483] 2024-05-14 13:52:20,403 >> Configuration saved in outputs/instruct_tuning/model/tmp-checkpoint-1600/config.json
[INFO|configuration_utils.py:594] 2024-05-14 13:52:20,404 >> Configuration saved in outputs/instruct_tuning/model/tmp-checkpoint-1600/generation_config.json
[INFO|modeling_utils.py:2390] 2024-05-14 13:52:44,541 >> The model is bigger than the maximum size per checkpoint (5GB) and is going to be split in 3 checkpoint shards. You can find where each parameters has been saved in the index located at outputs/instruct_tuning/model/tmp-checkpoint-1600/model.safetensors.index.json.
[INFO|tokenization_utils_base.py:2432] 2024-05-14 13:52:44,543 >> tokenizer config file saved in outputs/instruct_tuning/model/tmp-checkpoint-1600/tokenizer_config.json
[INFO|tokenization_utils_base.py:2441] 2024-05-14 13:52:44,543 >> Special tokens file saved in outputs/instruct_tuning/model/tmp-checkpoint-1600/special_tokens_map.json
[2024-05-14 13:52:45,179] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step1600 is about to be saved!
[2024-05-14 13:52:45,184] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: outputs/instruct_tuning/model/tmp-checkpoint-1600/global_step1600/mp_rank_00_model_states.pt
[2024-05-14 13:52:45,184] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving outputs/instruct_tuning/model/tmp-checkpoint-1600/global_step1600/mp_rank_00_model_states.pt...
/mounts/work/lyk/anaconda3/envs/torch2/lib/python3.8/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(