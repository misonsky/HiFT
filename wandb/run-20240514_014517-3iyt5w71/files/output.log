  0%|          | 0/6436 [00:00<?, ?it/s]/mounts/work/lyk/anaconda3/envs/torch2/lib/python3.8/site-packages/deepspeed/runtime/utils.py:274: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/conda/conda-bld/pytorch_1699449229234/work/torch/csrc/tensor/python_tensor.cpp:83.)
  overflow_gpu = get_accelerator().ByteTensor([overflow])
  0%|          | 1/6436 [00:01<2:48:18,  1.57s/it]
[2024-05-14 01:45:24,302] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 0
[2024-05-14 01:45:24,302] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2024-05-14 01:45:24,304] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0

  0%|          | 2/6436 [00:03<3:26:58,  1.93s/it]
[2024-05-14 01:45:26,672] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1
[2024-05-14 01:45:26,672] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2024-05-14 01:45:26,673] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
[2024-05-14 01:45:27,595] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 2
[2024-05-14 01:45:27,596] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0

  0%|          | 4/6436 [00:05<2:16:22,  1.27s/it]
[2024-05-14 01:45:28,475] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 3
[2024-05-14 01:45:28,475] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 8192.0 to 4096.0
[2024-05-14 01:45:28,476] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 8192.0, reducing to 4096.0
[2024-05-14 01:45:29,797] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 4
[2024-05-14 01:45:29,798] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 4096.0 to 2048.0

  0%|          | 5/6436 [00:06<2:19:51,  1.30s/it]
[2024-05-14 01:45:31,056] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 5
[2024-05-14 01:45:31,056] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 2048.0 to 1024.0

  0%|          | 7/6436 [00:09<2:13:02,  1.24s/it]
[2024-05-14 01:45:32,191] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 6
[2024-05-14 01:45:32,192] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 1024.0 to 512.0
[2024-05-14 01:45:32,194] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 1024.0, reducing to 512.0
[2024-05-14 01:45:33,595] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 7
[2024-05-14 01:45:33,595] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 512.0 to 256.0

  0%|          | 9/6436 [00:12<2:25:39,  1.36s/it]
[2024-05-14 01:45:35,170] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 8
[2024-05-14 01:45:35,171] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 256.0 to 128.0

  0%|          | 10/6436 [00:13<2:21:33,  1.32s/it]
[2024-05-14 01:45:36,416] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 9
[2024-05-14 01:45:36,417] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 128.0 to 64.0
[2024-05-14 01:45:36,417] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 128.0, reducing to 64.0

  0%|          | 12/6436 [00:16<2:20:27,  1.31s/it]
[2024-05-14 01:45:38,039] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 10
[2024-05-14 01:45:38,039] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 64.0 to 32.0
[2024-05-14 01:45:38,041] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 64.0, reducing to 32.0
[2024-05-14 01:45:39,136] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 11
[2024-05-14 01:45:39,137] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 32.0 to 16.0

  0%|          | 13/6436 [00:17<2:14:47,  1.26s/it]
[2024-05-14 01:45:40,281] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 12
[2024-05-14 01:45:40,281] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 16.0 to 8.0
[2024-05-14 01:45:40,282] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 16.0, reducing to 8.0
[2024-05-14 01:45:41,347] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 13
[2024-05-14 01:45:41,347] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 8.0 to 4.0

  0%|          | 15/6436 [00:19<2:09:42,  1.21s/it]
[2024-05-14 01:45:42,511] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 14
[2024-05-14 01:45:42,512] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 4.0 to 2.0

  0%|          | 16/6436 [00:21<2:29:08,  1.39s/it]
[2024-05-14 01:45:44,352] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 15
[2024-05-14 01:45:44,353] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 2.0 to 1.0
[2024-05-14 01:45:44,353] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 2.0, reducing to 1.0
[2024-05-14 01:45:45,331] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 16
[2024-05-14 01:45:45,332] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 1.0 to 0.5

  0%|          | 18/6436 [00:23<2:11:16,  1.23s/it]
[2024-05-14 01:45:46,536] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 17
[2024-05-14 01:45:46,537] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.5 to 0.25
[2024-05-14 01:45:46,538] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.5, reducing to 0.25
[2024-05-14 01:45:47,656] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 18
[2024-05-14 01:45:47,656] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  0%|          | 20/6436 [00:26<2:13:12,  1.25s/it]
[2024-05-14 01:45:48,965] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 19
[2024-05-14 01:45:48,965] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 01:45:48,966] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25

  0%|          | 21/6436 [00:27<2:22:18,  1.33s/it]
[2024-05-14 01:45:50,543] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 20
[2024-05-14 01:45:50,544] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  0%|          | 23/6436 [00:30<2:20:48,  1.32s/it]
[2024-05-14 01:45:51,974] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 21
[2024-05-14 01:45:51,974] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 01:45:51,977] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 01:45:53,191] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 22
[2024-05-14 01:45:53,192] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  0%|          | 24/6436 [00:31<2:16:58,  1.28s/it]
[2024-05-14 01:45:54,366] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 23
[2024-05-14 01:45:54,367] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 01:45:54,368] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 01:45:55,826] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 24
[2024-05-14 01:45:55,827] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  0%|          | 26/6436 [00:34<2:17:19,  1.29s/it]
[2024-05-14 01:45:57,022] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 25
[2024-05-14 01:45:57,023] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  0%|          | 27/6436 [00:35<2:08:43,  1.21s/it]
[2024-05-14 01:45:58,049] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 26
[2024-05-14 01:45:58,049] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 01:45:58,050] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 01:45:59,343] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 27
[2024-05-14 01:45:59,344] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  0%|          | 29/6436 [00:37<2:17:36,  1.29s/it]
[2024-05-14 01:46:00,673] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 28
[2024-05-14 01:46:00,673] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 01:46:00,674] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 01:46:01,879] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 29
[2024-05-14 01:46:01,879] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  0%|          | 31/6436 [00:40<2:13:00,  1.25s/it]
{'loss': 0.0, 'learning_rate': 2e-05, 'epoch': 0.0}
[2024-05-14 01:46:03,133] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 30
[2024-05-14 01:46:03,133] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  0%|          | 32/6436 [00:41<2:05:21,  1.17s/it]
[2024-05-14 01:46:04,136] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 31
[2024-05-14 01:46:04,137] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  1%|          | 34/6436 [00:44<2:17:30,  1.29s/it]
[2024-05-14 01:46:06,077] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 32
[2024-05-14 01:46:06,077] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 01:46:06,078] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 01:46:07,104] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 33
[2024-05-14 01:46:07,105] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  1%|          | 35/6436 [00:45<2:04:37,  1.17s/it]
[2024-05-14 01:46:08,058] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 34
[2024-05-14 01:46:08,059] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 01:46:08,060] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 01:46:09,269] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 35
[2024-05-14 01:46:09,269] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  1%|          | 37/6436 [00:48<2:20:19,  1.32s/it]
[2024-05-14 01:46:10,733] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 36
[2024-05-14 01:46:10,733] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 01:46:10,734] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 01:46:11,624] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 37
[2024-05-14 01:46:11,624] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  1%|          | 39/6436 [00:49<1:57:18,  1.10s/it]
[2024-05-14 01:46:12,377] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 38
[2024-05-14 01:46:12,378] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  1%|          | 40/6436 [00:51<2:09:48,  1.22s/it]
[2024-05-14 01:46:13,938] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 39
[2024-05-14 01:46:13,939] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 01:46:13,939] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
{'loss': 0.0, 'learning_rate': 2e-05, 'epoch': 0.01}
[2024-05-14 01:46:15,714] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 40
[2024-05-14 01:46:15,715] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  1%|          | 41/6436 [00:53<2:30:02,  1.41s/it]
[2024-05-14 01:46:17,328] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 41
[2024-05-14 01:46:17,328] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  1%|          | 43/6436 [00:56<2:43:14,  1.53s/it]
[2024-05-14 01:46:19,012] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 42
[2024-05-14 01:46:19,012] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  1%|          | 44/6436 [00:57<2:43:22,  1.53s/it]
[2024-05-14 01:46:20,517] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 43
[2024-05-14 01:46:20,518] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  1%|          | 45/6436 [00:59<2:58:14,  1.67s/it]
[2024-05-14 01:46:22,358] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 44
[2024-05-14 01:46:22,359] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  1%|          | 46/6436 [01:01<3:10:43,  1.79s/it]
[2024-05-14 01:46:24,440] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 45
[2024-05-14 01:46:24,442] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
