  0%|          | 0/3218 [00:00<?, ?it/s]/mounts/work/lyk/anaconda3/envs/torch2/lib/python3.8/site-packages/deepspeed/runtime/utils.py:274: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/conda/conda-bld/pytorch_1699449229234/work/torch/csrc/tensor/python_tensor.cpp:83.)
  overflow_gpu = get_accelerator().ByteTensor([overflow])
  0%|          | 1/3218 [00:01<1:24:56,  1.58s/it]
[2024-05-14 13:55:33,506] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 0
[2024-05-14 13:55:33,507] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2024-05-14 13:55:33,508] [INFO] [replace_operation.py:230:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
{'loss': 0.0, 'learning_rate': 0.0, 'epoch': 0.0}
[2024-05-14 13:55:34,963] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1
[2024-05-14 13:55:34,964] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0

  0%|          | 3/3218 [00:03<59:53,  1.12s/it]
[2024-05-14 13:55:35,864] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 2
[2024-05-14 13:55:35,864] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2024-05-14 13:55:35,864] [INFO] [replace_operation.py:230:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
[2024-05-14 13:55:36,893] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 3
[2024-05-14 13:55:36,893] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 8192.0 to 4096.0

  0%|          | 4/3218 [00:05<1:04:25,  1.20s/it]
[2024-05-14 13:55:38,686] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 4
[2024-05-14 13:55:38,686] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 4096.0 to 2048.0

  0%|          | 5/3218 [00:06<1:15:48,  1.42s/it]
[2024-05-14 13:55:40,582] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 5
[2024-05-14 13:55:40,582] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 2048.0 to 1024.0

  0%|          | 6/3218 [00:08<1:24:18,  1.57s/it]
[2024-05-14 13:55:41,898] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 6
[2024-05-14 13:55:41,898] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 1024.0 to 512.0

  0%|          | 8/3218 [00:12<1:31:39,  1.71s/it]
[2024-05-14 13:55:44,068] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 7
[2024-05-14 13:55:44,069] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 512.0 to 256.0

  0%|          | 9/3218 [00:13<1:26:44,  1.62s/it]
[2024-05-14 13:55:45,502] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 8
[2024-05-14 13:55:45,503] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 256.0 to 128.0

  0%|          | 10/3218 [00:15<1:26:42,  1.62s/it]
[2024-05-14 13:55:47,105] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 9
[2024-05-14 13:55:47,105] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 128.0 to 64.0
[2024-05-14 13:55:47,108] [INFO] [replace_operation.py:230:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 128.0, reducing to 64.0
{'loss': 0.0, 'learning_rate': 0.0, 'epoch': 0.0}
[2024-05-14 13:55:48,969] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 10
[2024-05-14 13:55:48,969] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 64.0 to 32.0

  0%|          | 11/3218 [00:17<1:30:38,  1.70s/it]
[2024-05-14 13:55:50,534] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 11
[2024-05-14 13:55:50,535] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 32.0 to 16.0

  0%|          | 12/3218 [00:18<1:28:34,  1.66s/it]
[2024-05-14 13:55:52,641] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 12
[2024-05-14 13:55:52,641] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 16.0 to 8.0

  0%|          | 13/3218 [00:20<1:35:14,  1.78s/it]
[2024-05-14 13:55:54,388] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 13
[2024-05-14 13:55:54,388] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 8.0 to 4.0

  0%|          | 14/3218 [00:22<1:34:51,  1.78s/it]
[2024-05-14 13:55:56,259] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 14
[2024-05-14 13:55:56,260] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 4.0 to 2.0

  0%|          | 16/3218 [00:26<1:33:31,  1.75s/it]
[2024-05-14 13:55:57,905] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 15
[2024-05-14 13:55:57,905] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 2.0 to 1.0

  1%|          | 17/3218 [00:27<1:30:21,  1.69s/it]
[2024-05-14 13:55:59,462] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 16
[2024-05-14 13:55:59,462] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 1.0 to 0.5
[2024-05-14 13:55:59,463] [INFO] [replace_operation.py:230:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 1.0, reducing to 0.5
[2024-05-14 13:56:00,976] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 17
[2024-05-14 13:56:00,976] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.5 to 0.25

  1%|          | 18/3218 [00:29<1:27:36,  1.64s/it]
[2024-05-14 13:56:02,506] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 18
[2024-05-14 13:56:02,506] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  1%|          | 20/3218 [00:32<1:23:10,  1.56s/it]
[2024-05-14 13:56:03,963] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 19
[2024-05-14 13:56:03,964] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:56:03,964] [INFO] [replace_operation.py:230:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25

  1%|          | 21/3218 [00:33<1:24:05,  1.58s/it]
[2024-05-14 13:56:05,583] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 20
[2024-05-14 13:56:05,583] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:56:05,583] [INFO] [replace_operation.py:230:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 13:56:06,980] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 21
[2024-05-14 13:56:06,981] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  1%|          | 22/3218 [00:35<1:21:12,  1.52s/it]
[2024-05-14 13:56:08,270] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 22
[2024-05-14 13:56:08,271] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  1%|          | 24/3218 [00:37<1:16:15,  1.43s/it]
[2024-05-14 13:56:09,639] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 23
[2024-05-14 13:56:09,639] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  1%|          | 25/3218 [00:39<1:19:39,  1.50s/it]
[2024-05-14 13:56:11,291] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 24
[2024-05-14 13:56:11,292] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:56:11,292] [INFO] [replace_operation.py:230:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 13:56:12,620] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 25
[2024-05-14 13:56:12,620] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  1%|          | 27/3218 [00:42<1:15:35,  1.42s/it]
[2024-05-14 13:56:14,001] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 26
[2024-05-14 13:56:14,001] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  1%|          | 28/3218 [00:43<1:18:29,  1.48s/it]
[2024-05-14 13:56:15,580] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 27
[2024-05-14 13:56:15,581] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:56:15,581] [INFO] [replace_operation.py:230:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 13:56:16,832] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 28
[2024-05-14 13:56:16,833] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  1%|          | 30/3218 [00:46<1:12:06,  1.36s/it]
[2024-05-14 13:56:18,076] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 29
[2024-05-14 13:56:18,077] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:56:18,077] [INFO] [replace_operation.py:230:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25

  1%|          | 31/3218 [00:47<1:11:00,  1.34s/it]
[2024-05-14 13:56:19,360] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 30
[2024-05-14 13:56:19,360] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:56:19,361] [INFO] [replace_operation.py:230:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 13:56:20,507] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 31
[2024-05-14 13:56:20,507] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  1%|          | 33/3218 [00:49<1:04:14,  1.21s/it]
[2024-05-14 13:56:21,547] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 32
[2024-05-14 13:56:21,547] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:56:21,548] [INFO] [replace_operation.py:230:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 13:56:22,671] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 33
[2024-05-14 13:56:22,672] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  1%|          | 35/3218 [00:51<59:23,  1.12s/it]
[2024-05-14 13:56:23,757] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 34
[2024-05-14 13:56:23,757] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  1%|          | 36/3218 [00:54<1:17:35,  1.46s/it]
[2024-05-14 13:56:25,905] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 35
[2024-05-14 13:56:25,905] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  1%|          | 37/3218 [00:55<1:21:58,  1.55s/it]
[2024-05-14 13:56:27,539] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 36
[2024-05-14 13:56:27,540] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:56:27,540] [INFO] [replace_operation.py:230:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 13:56:28,811] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 37
[2024-05-14 13:56:28,811] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  1%|          | 38/3218 [00:56<1:11:07,  1.34s/it]
[2024-05-14 13:56:30,190] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 38
[2024-05-14 13:56:30,190] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  1%|          | 39/3218 [00:58<1:18:17,  1.48s/it]
[2024-05-14 13:56:32,164] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 39
[2024-05-14 13:56:32,165] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:56:32,165] [INFO] [replace_operation.py:230:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25

  1%|▏         | 41/3218 [01:02<1:28:11,  1.67s/it]
[2024-05-14 13:56:33,893] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 40
[2024-05-14 13:56:33,893] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:56:33,894] [INFO] [replace_operation.py:230:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 13:56:35,961] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 41
[2024-05-14 13:56:35,961] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  1%|▏         | 42/3218 [01:04<1:37:02,  1.83s/it]
[2024-05-14 13:56:38,620] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 42
[2024-05-14 13:56:38,620] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  1%|▏         | 43/3218 [01:07<1:51:06,  2.10s/it]
[2024-05-14 13:56:40,802] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 43
[2024-05-14 13:56:40,803] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  1%|▏         | 44/3218 [01:09<1:49:40,  2.07s/it]
[2024-05-14 13:56:42,519] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 44
[2024-05-14 13:56:42,519] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  1%|▏         | 45/3218 [01:10<1:43:24,  1.96s/it]
[2024-05-14 13:56:44,604] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 45
[2024-05-14 13:56:44,604] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  1%|▏         | 46/3218 [01:13<1:47:44,  2.04s/it]
[2024-05-14 13:56:46,560] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 46
[2024-05-14 13:56:46,560] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  1%|▏         | 47/3218 [01:15<1:45:46,  2.00s/it]
[2024-05-14 13:56:48,527] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 47
[2024-05-14 13:56:48,527] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  1%|▏         | 48/3218 [01:16<1:44:56,  1.99s/it]
[2024-05-14 13:56:50,259] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 48
[2024-05-14 13:56:50,259] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  2%|▏         | 49/3218 [01:18<1:40:43,  1.91s/it]
[2024-05-14 13:56:52,197] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 49
[2024-05-14 13:56:52,197] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:56:52,198] [INFO] [replace_operation.py:230:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25

  2%|▏         | 50/3218 [01:20<1:40:42,  1.91s/it]
[2024-05-14 13:56:54,116] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 50
[2024-05-14 13:56:54,116] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  2%|▏         | 52/3218 [01:24<1:38:48,  1.87s/it]
[2024-05-14 13:56:55,881] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 51
[2024-05-14 13:56:55,882] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  2%|▏         | 53/3218 [01:26<1:39:28,  1.89s/it]
[2024-05-14 13:56:57,780] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 52
[2024-05-14 13:56:57,780] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  2%|▏         | 54/3218 [01:28<1:39:55,  1.90s/it]
[2024-05-14 13:56:59,695] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 53
[2024-05-14 13:56:59,696] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  2%|▏         | 55/3218 [01:30<1:40:50,  1.91s/it]
[2024-05-14 13:57:01,695] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 54
[2024-05-14 13:57:01,695] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  2%|▏         | 56/3218 [01:31<1:40:30,  1.91s/it]
[2024-05-14 13:57:03,551] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 55
[2024-05-14 13:57:03,552] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:57:03,553] [INFO] [replace_operation.py:230:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 13:57:05,183] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 56
[2024-05-14 13:57:05,184] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  2%|▏         | 57/3218 [01:33<1:35:12,  1.81s/it]
[2024-05-14 13:57:06,723] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 57
[2024-05-14 13:57:06,723] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  2%|▏         | 58/3218 [01:35<1:30:23,  1.72s/it]
[2024-05-14 13:57:08,540] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 58
[2024-05-14 13:57:08,541] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  2%|▏         | 59/3218 [01:36<1:31:39,  1.74s/it]
[2024-05-14 13:57:10,111] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 59
[2024-05-14 13:57:10,111] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:57:10,112] [INFO] [replace_operation.py:230:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25

  2%|▏         | 61/3218 [01:39<1:25:14,  1.62s/it]
[2024-05-14 13:57:11,574] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 60
[2024-05-14 13:57:11,575] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:57:11,575] [INFO] [replace_operation.py:230:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 13:57:12,987] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 61
[2024-05-14 13:57:12,987] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  2%|▏         | 62/3218 [01:41<1:22:48,  1.57s/it]
[2024-05-14 13:57:14,432] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 62
[2024-05-14 13:57:14,432] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  2%|▏         | 64/3218 [01:44<1:20:24,  1.53s/it]
[2024-05-14 13:57:15,984] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 63
[2024-05-14 13:57:15,984] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  2%|▏         | 65/3218 [01:45<1:17:23,  1.47s/it]
[2024-05-14 13:57:17,319] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 64
[2024-05-14 13:57:17,319] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:57:17,320] [INFO] [replace_operation.py:230:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 13:57:18,813] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 65
[2024-05-14 13:57:18,813] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  2%|▏         | 66/3218 [01:47<1:17:32,  1.48s/it]
[2024-05-14 13:57:20,171] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 66
[2024-05-14 13:57:20,172] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  2%|▏         | 68/3218 [01:49<1:15:11,  1.43s/it]
[2024-05-14 13:57:21,594] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 67
[2024-05-14 13:57:21,595] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:57:21,595] [INFO] [replace_operation.py:230:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 13:57:22,921] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 68
[2024-05-14 13:57:22,921] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  2%|▏         | 69/3218 [01:51<1:12:35,  1.38s/it]
[2024-05-14 13:57:24,337] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 69
[2024-05-14 13:57:24,338] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:57:24,338] [INFO] [replace_operation.py:230:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25

  2%|▏         | 70/3218 [01:52<1:11:37,  1.37s/it]
[2024-05-14 13:57:26,217] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 70
[2024-05-14 13:57:26,217] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  2%|▏         | 71/3218 [01:54<1:22:51,  1.58s/it]
[2024-05-14 13:57:28,122] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 71
[2024-05-14 13:57:28,123] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  2%|▏         | 73/3218 [01:57<1:11:55,  1.37s/it]
[2024-05-14 13:57:29,313] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 72
[2024-05-14 13:57:29,313] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:57:29,313] [INFO] [replace_operation.py:230:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 13:57:30,367] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 73
[2024-05-14 13:57:30,367] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  2%|▏         | 74/3218 [01:58<1:15:36,  1.44s/it]
[2024-05-14 13:57:32,521] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 74
[2024-05-14 13:57:32,521] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  2%|▏         | 75/3218 [02:00<1:26:49,  1.66s/it]
[2024-05-14 13:57:34,785] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 75
[2024-05-14 13:57:34,785] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  2%|▏         | 76/3218 [02:03<1:37:03,  1.85s/it]
[2024-05-14 13:57:36,713] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 76
[2024-05-14 13:57:36,713] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  2%|▏         | 77/3218 [02:05<1:38:16,  1.88s/it]
[2024-05-14 13:57:38,749] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 77
[2024-05-14 13:57:38,750] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25


  2%|▏         | 79/3218 [02:09<1:51:18,  2.13s/it]
[2024-05-14 13:57:41,350] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 78
[2024-05-14 13:57:41,350] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  2%|▏         | 80/3218 [02:11<1:47:49,  2.06s/it]
[2024-05-14 13:57:43,293] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 79
[2024-05-14 13:57:43,294] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:57:43,294] [INFO] [replace_operation.py:230:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
{'loss': 0.0, 'learning_rate': 0.0, 'epoch': 0.02}
[2024-05-14 13:57:45,097] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 80
[2024-05-14 13:57:45,098] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  3%|▎         | 81/3218 [02:13<1:42:18,  1.96s/it]

  3%|▎         | 82/3218 [02:15<1:44:01,  1.99s/it]
[2024-05-14 13:57:47,206] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:57:47,207] [INFO] [replace_operation.py:230:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 13:57:49,188] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 82
[2024-05-14 13:57:49,189] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  3%|▎         | 83/3218 [02:17<1:44:24,  2.00s/it]
[2024-05-14 13:57:51,297] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 83
[2024-05-14 13:57:51,297] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  3%|▎         | 84/3218 [02:19<1:45:02,  2.01s/it]
[2024-05-14 13:57:53,196] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 84
[2024-05-14 13:57:53,196] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  3%|▎         | 85/3218 [02:21<1:42:56,  1.97s/it]
[2024-05-14 13:57:55,165] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 85
[2024-05-14 13:57:55,165] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  3%|▎         | 86/3218 [02:23<1:43:21,  1.98s/it]
[2024-05-14 13:57:57,254] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 86
[2024-05-14 13:57:57,254] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  3%|▎         | 87/3218 [02:25<1:45:46,  2.03s/it]
[2024-05-14 13:57:59,204] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 87
[2024-05-14 13:57:59,204] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  3%|▎         | 88/3218 [02:27<1:44:48,  2.01s/it]
[2024-05-14 13:58:01,050] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 88
[2024-05-14 13:58:01,051] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  3%|▎         | 89/3218 [02:29<1:41:28,  1.95s/it]
[2024-05-14 13:58:02,978] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 89
[2024-05-14 13:58:02,978] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  3%|▎         | 90/3218 [02:31<1:41:08,  1.94s/it]
{'loss': 0.0, 'learning_rate': 0.0, 'epoch': 0.03}
[2024-05-14 13:58:04,667] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 90
[2024-05-14 13:58:04,668] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  3%|▎         | 91/3218 [02:33<1:37:40,  1.87s/it]
[2024-05-14 13:58:06,314] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 91
[2024-05-14 13:58:06,314] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  3%|▎         | 93/3218 [02:36<1:31:33,  1.76s/it]
[2024-05-14 13:58:07,872] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 92
[2024-05-14 13:58:07,872] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  3%|▎         | 94/3218 [02:37<1:28:48,  1.71s/it]
[2024-05-14 13:58:09,417] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 93
[2024-05-14 13:58:09,417] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:58:09,418] [INFO] [replace_operation.py:230:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 13:58:10,905] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 94
[2024-05-14 13:58:10,905] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  3%|▎         | 95/3218 [02:39<1:25:21,  1.64s/it]
[2024-05-14 13:58:12,720] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 95
[2024-05-14 13:58:12,720] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  3%|▎         | 96/3218 [02:41<1:29:09,  1.71s/it]
[2024-05-14 13:58:14,495] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 96
[2024-05-14 13:58:14,496] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  3%|▎         | 97/3218 [02:42<1:29:10,  1.71s/it]
[2024-05-14 13:58:16,269] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 97
[2024-05-14 13:58:16,269] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  3%|▎         | 99/3218 [02:46<1:28:19,  1.70s/it]
[2024-05-14 13:58:17,922] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 98
[2024-05-14 13:58:17,922] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
  3%|▎         | 100/3218 [02:47<1:25:58,  1.65s/it][INFO|trainer.py:3166] 2024-05-14 13:58:20,133 >> ***** Running Evaluation *****
[INFO|trainer.py:3168] 2024-05-14 13:58:20,133 >>   Num examples = 521
[INFO|trainer.py:3171] 2024-05-14 13:58:20,133 >>   Batch size = 8
  6%|▌         | 2/33 [00:00<00:03,  8.56it/s]
[2024-05-14 13:58:19,509] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 99
[2024-05-14 13:58:19,509] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:58:19,512] [INFO] [replace_operation.py:230:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25






 88%|████████▊ | 29/33 [00:10<00:01,  2.83it/s]

  3%|▎         | 101/3218 [03:01<4:39:12,  5.37s/it]
[2024-05-14 13:58:33,534] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 100
[2024-05-14 13:58:33,534] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  3%|▎         | 102/3218 [03:03<3:45:24,  4.34s/it]
[2024-05-14 13:58:35,472] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 101
[2024-05-14 13:58:35,473] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:58:35,474] [INFO] [replace_operation.py:230:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 13:58:37,004] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 102
[2024-05-14 13:58:37,005] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  3%|▎         | 103/3218 [03:05<3:01:36,  3.50s/it]
[2024-05-14 13:58:38,899] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 103
[2024-05-14 13:58:38,899] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  3%|▎         | 104/3218 [03:07<2:36:36,  3.02s/it]
[2024-05-14 13:58:40,335] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 104
[2024-05-14 13:58:40,336] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  3%|▎         | 106/3218 [03:10<1:56:28,  2.25s/it]
[2024-05-14 13:58:42,021] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 105
[2024-05-14 13:58:42,021] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  3%|▎         | 107/3218 [03:12<1:48:50,  2.10s/it]
[2024-05-14 13:58:43,606] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 106
[2024-05-14 13:58:43,606] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:58:43,607] [INFO] [replace_operation.py:230:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 13:58:45,026] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 107
[2024-05-14 13:58:45,026] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  3%|▎         | 108/3218 [03:12<1:29:42,  1.73s/it]
[2024-05-14 13:58:46,326] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 108
[2024-05-14 13:58:46,327] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  3%|▎         | 109/3218 [03:14<1:31:24,  1.76s/it]
[2024-05-14 13:58:48,545] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 109
[2024-05-14 13:58:48,545] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:58:48,546] [INFO] [replace_operation.py:230:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25

  3%|▎         | 110/3218 [03:17<1:39:04,  1.91s/it]
[2024-05-14 13:58:50,843] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 110
[2024-05-14 13:58:50,844] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  3%|▎         | 111/3218 [03:19<1:44:48,  2.02s/it]
[2024-05-14 13:58:53,049] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 111
[2024-05-14 13:58:53,049] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  3%|▎         | 112/3218 [03:21<1:48:53,  2.10s/it]
[2024-05-14 13:58:55,054] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 112
[2024-05-14 13:58:55,055] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  4%|▎         | 113/3218 [03:23<1:45:58,  2.05s/it]
[2024-05-14 13:58:57,071] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 113
[2024-05-14 13:58:57,072] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  4%|▎         | 114/3218 [03:25<1:46:10,  2.05s/it]
[2024-05-14 13:58:59,166] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 114
[2024-05-14 13:58:59,166] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  4%|▎         | 115/3218 [03:27<1:47:14,  2.07s/it]
[2024-05-14 13:59:01,258] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 115
[2024-05-14 13:59:01,258] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25


  4%|▎         | 117/3218 [03:32<1:53:25,  2.19s/it]
[2024-05-14 13:59:03,700] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 116
[2024-05-14 13:59:03,700] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:59:03,702] [INFO] [replace_operation.py:230:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 13:59:06,139] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 117
[2024-05-14 13:59:06,140] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  4%|▎         | 118/3218 [03:34<1:56:27,  2.25s/it]
[2024-05-14 13:59:08,262] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 118
[2024-05-14 13:59:08,262] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  4%|▎         | 119/3218 [03:36<1:53:56,  2.21s/it]
[2024-05-14 13:59:10,456] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 119
[2024-05-14 13:59:10,457] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:59:10,457] [INFO] [replace_operation.py:230:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25

  4%|▎         | 120/3218 [03:38<1:53:55,  2.21s/it]
[2024-05-14 13:59:12,520] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 120
[2024-05-14 13:59:12,521] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  4%|▍         | 121/3218 [03:40<1:51:22,  2.16s/it]
[2024-05-14 13:59:14,727] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 121
[2024-05-14 13:59:14,728] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  4%|▍         | 122/3218 [03:43<1:52:16,  2.18s/it]
[2024-05-14 13:59:16,814] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 122
[2024-05-14 13:59:16,814] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  4%|▍         | 123/3218 [03:45<1:50:42,  2.15s/it]
[2024-05-14 13:59:18,815] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 123
[2024-05-14 13:59:18,816] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  4%|▍         | 124/3218 [03:47<1:47:45,  2.09s/it]
[2024-05-14 13:59:20,845] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 124
[2024-05-14 13:59:20,846] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  4%|▍         | 125/3218 [03:49<1:47:02,  2.08s/it]
[2024-05-14 13:59:22,987] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 125
[2024-05-14 13:59:22,987] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  4%|▍         | 126/3218 [03:51<1:48:55,  2.11s/it]
[2024-05-14 13:59:24,756] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 126
[2024-05-14 13:59:24,756] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  4%|▍         | 127/3218 [03:53<1:43:48,  2.01s/it]
[2024-05-14 13:59:26,424] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 127
[2024-05-14 13:59:26,424] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  4%|▍         | 128/3218 [03:55<1:40:13,  1.95s/it]
[2024-05-14 13:59:28,195] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 128
[2024-05-14 13:59:28,195] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  4%|▍         | 130/3218 [03:58<1:30:27,  1.76s/it]
[2024-05-14 13:59:29,727] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 129
[2024-05-14 13:59:29,728] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:59:29,728] [INFO] [replace_operation.py:230:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
{'loss': 0.0, 'learning_rate': 0.0, 'epoch': 0.04}
[2024-05-14 13:59:31,419] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 130
[2024-05-14 13:59:31,419] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  4%|▍         | 131/3218 [03:59<1:29:16,  1.74s/it]
[2024-05-14 13:59:33,122] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 131
[2024-05-14 13:59:33,122] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  4%|▍         | 132/3218 [04:01<1:29:06,  1.73s/it]
[2024-05-14 13:59:34,683] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 132
[2024-05-14 13:59:34,684] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  4%|▍         | 133/3218 [04:03<1:25:32,  1.66s/it]
[2024-05-14 13:59:36,490] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 133
[2024-05-14 13:59:36,490] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  4%|▍         | 134/3218 [04:04<1:28:19,  1.72s/it]
[2024-05-14 13:59:38,152] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 134
[2024-05-14 13:59:38,152] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  4%|▍         | 136/3218 [04:08<1:30:17,  1.76s/it]
[2024-05-14 13:59:40,102] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 135
[2024-05-14 13:59:40,103] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  4%|▍         | 137/3218 [04:10<1:26:43,  1.69s/it]
[2024-05-14 13:59:41,625] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 136
[2024-05-14 13:59:41,626] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:59:41,626] [INFO] [replace_operation.py:230:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 13:59:43,174] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 137
[2024-05-14 13:59:43,175] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  4%|▍         | 138/3218 [04:11<1:24:38,  1.65s/it]
[2024-05-14 13:59:44,792] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 138
[2024-05-14 13:59:44,792] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  4%|▍         | 140/3218 [04:14<1:18:42,  1.53s/it]
[2024-05-14 13:59:46,297] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 139
[2024-05-14 13:59:46,298] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:59:46,298] [INFO] [replace_operation.py:230:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25

  4%|▍         | 141/3218 [04:16<1:23:42,  1.63s/it]
[2024-05-14 13:59:48,120] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 140
[2024-05-14 13:59:48,121] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  4%|▍         | 142/3218 [04:18<1:31:23,  1.78s/it]
[2024-05-14 13:59:50,031] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 141
[2024-05-14 13:59:50,032] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:59:50,032] [INFO] [replace_operation.py:230:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 13:59:51,300] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 142
[2024-05-14 13:59:51,300] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  4%|▍         | 143/3218 [04:19<1:14:00,  1.44s/it]
[2024-05-14 13:59:53,134] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 143
[2024-05-14 13:59:53,135] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  4%|▍         | 144/3218 [04:21<1:28:11,  1.72s/it]
[2024-05-14 13:59:55,265] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 144
[2024-05-14 13:59:55,265] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25


  5%|▍         | 146/3218 [04:26<1:47:00,  2.09s/it]
[2024-05-14 13:59:57,877] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 145
[2024-05-14 13:59:57,878] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  5%|▍         | 147/3218 [04:28<1:47:02,  2.09s/it]
[2024-05-14 13:59:59,988] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 146
[2024-05-14 13:59:59,988] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 13:59:59,988] [INFO] [replace_operation.py:230:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 14:00:02,039] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 147
[2024-05-14 14:00:02,039] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  5%|▍         | 148/3218 [04:30<1:48:42,  2.12s/it]
[2024-05-14 14:00:04,177] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 148
[2024-05-14 14:00:04,177] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  5%|▍         | 149/3218 [04:32<1:46:40,  2.09s/it]
[2024-05-14 14:00:06,303] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 149
[2024-05-14 14:00:06,303] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 14:00:06,304] [INFO] [replace_operation.py:230:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25

  5%|▍         | 150/3218 [04:34<1:48:06,  2.11s/it]
[2024-05-14 14:00:08,663] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 150
[2024-05-14 14:00:08,664] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  5%|▍         | 151/3218 [04:37<1:50:53,  2.17s/it]
[2024-05-14 14:00:11,108] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 151
[2024-05-14 14:00:11,108] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  5%|▍         | 152/3218 [04:39<1:55:39,  2.26s/it]
[2024-05-14 14:00:13,324] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 152
[2024-05-14 14:00:13,324] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25


  5%|▍         | 154/3218 [04:44<1:59:04,  2.33s/it]
[2024-05-14 14:00:15,850] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 153
[2024-05-14 14:00:15,850] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 14:00:15,850] [INFO] [replace_operation.py:230:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 14:00:18,327] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 154
[2024-05-14 14:00:18,327] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  5%|▍         | 155/3218 [04:46<2:00:42,  2.36s/it]
[2024-05-14 14:00:20,645] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 155
[2024-05-14 14:00:20,646] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  5%|▍         | 156/3218 [04:49<1:59:15,  2.34s/it]
[2024-05-14 14:00:22,687] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 156
[2024-05-14 14:00:22,687] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  5%|▍         | 157/3218 [04:51<1:55:32,  2.26s/it]
[2024-05-14 14:00:24,973] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 157
[2024-05-14 14:00:24,974] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  5%|▍         | 158/3218 [04:53<1:57:10,  2.30s/it]
[2024-05-14 14:00:27,179] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 158
[2024-05-14 14:00:27,179] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  5%|▍         | 159/3218 [04:55<1:52:27,  2.21s/it]
[2024-05-14 14:00:29,255] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 159
[2024-05-14 14:00:29,255] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  5%|▍         | 160/3218 [04:57<1:52:06,  2.20s/it]
{'loss': 0.0, 'learning_rate': 0.0, 'epoch': 0.05}
[2024-05-14 14:00:31,062] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 160
[2024-05-14 14:00:31,062] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  5%|▌         | 161/3218 [04:59<1:47:18,  2.11s/it]
[2024-05-14 14:00:32,823] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 161
[2024-05-14 14:00:32,823] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  5%|▌         | 162/3218 [05:01<1:41:24,  1.99s/it]
[2024-05-14 14:00:35,033] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 162
[2024-05-14 14:00:35,034] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  5%|▌         | 163/3218 [05:03<1:45:56,  2.08s/it]
[2024-05-14 14:00:36,755] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 163
[2024-05-14 14:00:36,755] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  5%|▌         | 164/3218 [05:05<1:41:10,  1.99s/it]
[2024-05-14 14:00:38,746] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 164
[2024-05-14 14:00:38,747] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  5%|▌         | 165/3218 [05:07<1:39:11,  1.95s/it]
[2024-05-14 14:00:40,337] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 165
[2024-05-14 14:00:40,337] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  5%|▌         | 167/3218 [05:10<1:31:27,  1.80s/it]
[2024-05-14 14:00:41,971] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 166
[2024-05-14 14:00:41,971] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  5%|▌         | 168/3218 [05:12<1:31:12,  1.79s/it]
[2024-05-14 14:00:43,772] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 167
[2024-05-14 14:00:43,772] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 14:00:43,773] [INFO] [replace_operation.py:230:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 14:00:45,593] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 168
[2024-05-14 14:00:45,593] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  5%|▌         | 169/3218 [05:14<1:30:29,  1.78s/it]
[2024-05-14 14:00:47,176] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 169
[2024-05-14 14:00:47,177] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  5%|▌         | 170/3218 [05:15<1:27:41,  1.73s/it]
{'loss': 0.0, 'learning_rate': 0.0, 'epoch': 0.05}
[2024-05-14 14:00:49,174] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 170
[2024-05-14 14:00:49,174] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  5%|▌         | 171/3218 [05:17<1:31:17,  1.80s/it]
[2024-05-14 14:00:50,697] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 171
[2024-05-14 14:00:50,698] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  5%|▌         | 172/3218 [05:19<1:27:19,  1.72s/it]
[2024-05-14 14:00:52,639] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 172
[2024-05-14 14:00:52,639] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  5%|▌         | 174/3218 [05:22<1:27:17,  1.72s/it]
[2024-05-14 14:00:54,216] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 173
[2024-05-14 14:00:54,216] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  5%|▌         | 175/3218 [05:24<1:21:47,  1.61s/it]
[2024-05-14 14:00:55,706] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 174
[2024-05-14 14:00:55,706] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 14:00:55,706] [INFO] [replace_operation.py:230:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 14:00:57,571] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 175
[2024-05-14 14:00:57,571] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  5%|▌         | 176/3218 [05:25<1:24:39,  1.67s/it]
[2024-05-14 14:00:59,416] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 176
[2024-05-14 14:00:59,416] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  6%|▌         | 178/3218 [05:28<1:11:25,  1.41s/it]
[2024-05-14 14:01:00,528] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 177
[2024-05-14 14:01:00,528] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  6%|▌         | 179/3218 [05:30<1:16:15,  1.51s/it]
[2024-05-14 14:01:01,732] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 178
[2024-05-14 14:01:01,733] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  6%|▌         | 180/3218 [05:32<1:27:33,  1.73s/it]
[2024-05-14 14:01:03,877] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 179
[2024-05-14 14:01:03,878] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 14:01:03,878] [INFO] [replace_operation.py:230:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25

  6%|▌         | 181/3218 [05:34<1:35:38,  1.89s/it]
[2024-05-14 14:01:06,190] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 180
[2024-05-14 14:01:06,190] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 14:01:06,191] [INFO] [replace_operation.py:230:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 14:01:08,321] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 181
[2024-05-14 14:01:08,322] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  6%|▌         | 182/3218 [05:36<1:39:07,  1.96s/it]
[2024-05-14 14:01:10,996] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 182
[2024-05-14 14:01:10,996] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  6%|▌         | 183/3218 [05:39<1:52:52,  2.23s/it]
[2024-05-14 14:01:13,240] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 183
[2024-05-14 14:01:13,241] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  6%|▌         | 184/3218 [05:41<1:51:03,  2.20s/it]
[2024-05-14 14:01:15,163] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 184
[2024-05-14 14:01:15,163] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  6%|▌         | 185/3218 [05:43<1:46:47,  2.11s/it]
[2024-05-14 14:01:17,264] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 185
[2024-05-14 14:01:17,264] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  6%|▌         | 186/3218 [05:45<1:46:20,  2.10s/it]
[2024-05-14 14:01:19,582] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 186
[2024-05-14 14:01:19,582] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  6%|▌         | 187/3218 [05:48<1:49:45,  2.17s/it]
[2024-05-14 14:01:22,346] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 187
[2024-05-14 14:01:22,347] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  6%|▌         | 188/3218 [05:50<1:58:51,  2.35s/it]
[2024-05-14 14:01:24,320] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 188
[2024-05-14 14:01:24,320] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  6%|▌         | 189/3218 [05:52<1:51:44,  2.21s/it]
[2024-05-14 14:01:26,342] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 189
[2024-05-14 14:01:26,342] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 14:01:26,343] [INFO] [replace_operation.py:230:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25

  6%|▌         | 190/3218 [05:54<1:49:20,  2.17s/it]
[2024-05-14 14:01:28,570] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 190
[2024-05-14 14:01:28,570] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  6%|▌         | 191/3218 [05:57<1:50:08,  2.18s/it]
[2024-05-14 14:01:30,863] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 191
[2024-05-14 14:01:30,863] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  6%|▌         | 192/3218 [05:59<1:51:30,  2.21s/it]
[2024-05-14 14:01:33,310] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 192
[2024-05-14 14:01:33,311] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  6%|▌         | 193/3218 [06:01<1:55:27,  2.29s/it]
[2024-05-14 14:01:35,215] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 193
[2024-05-14 14:01:35,215] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  6%|▌         | 194/3218 [06:03<1:49:13,  2.17s/it]
[2024-05-14 14:01:37,272] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 194
[2024-05-14 14:01:37,273] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  6%|▌         | 195/3218 [06:05<1:46:19,  2.11s/it]
[2024-05-14 14:01:39,082] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 195
[2024-05-14 14:01:39,082] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  6%|▌         | 196/3218 [06:07<1:43:08,  2.05s/it]
[2024-05-14 14:01:41,089] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 196
[2024-05-14 14:01:41,089] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  6%|▌         | 197/3218 [06:09<1:43:07,  2.05s/it]
[2024-05-14 14:01:42,728] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 197
[2024-05-14 14:01:42,728] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  6%|▌         | 198/3218 [06:11<1:36:54,  1.93s/it]
[2024-05-14 14:01:44,335] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 198
[2024-05-14 14:01:44,336] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
  6%|▌         | 200/3218 [06:14<1:28:52,  1.77s/it][INFO|trainer.py:3166] 2024-05-14 14:01:46,715 >> ***** Running Evaluation *****
[INFO|trainer.py:3168] 2024-05-14 14:01:46,715 >>   Num examples = 521
[INFO|trainer.py:3171] 2024-05-14 14:01:46,715 >>   Batch size = 8
  0%|          | 0/33 [00:00<?, ?it/s]
[2024-05-14 14:01:45,925] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 199
[2024-05-14 14:01:45,926] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 14:01:45,926] [INFO] [replace_operation.py:230:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25






100%|██████████| 33/33 [00:11<00:00,  2.32it/s]

{'eval_loss': nan, 'eval_runtime': 12.358, 'eval_samples_per_second': 42.159, 'eval_steps_per_second': 2.67, 'epoch': 0.06}
[INFO|trainer.py:2889] 2024-05-14 14:02:05,713 >> Saving model checkpoint to outputs/instruct_tuning/model/checkpoint-200
[INFO|configuration_utils.py:483] 2024-05-14 14:02:05,721 >> Configuration saved in outputs/instruct_tuning/model/checkpoint-200/config.json
[INFO|configuration_utils.py:594] 2024-05-14 14:02:05,727 >> Configuration saved in outputs/instruct_tuning/model/checkpoint-200/generation_config.json
[INFO|modeling_utils.py:2390] 2024-05-14 14:02:30,388 >> The model is bigger than the maximum size per checkpoint (5GB) and is going to be split in 3 checkpoint shards. You can find where each parameters has been saved in the index located at outputs/instruct_tuning/model/checkpoint-200/model.safetensors.index.json.
[INFO|tokenization_utils_base.py:2432] 2024-05-14 14:02:30,393 >> tokenizer config file saved in outputs/instruct_tuning/model/checkpoint-200/tokenizer_config.json
[INFO|tokenization_utils_base.py:2441] 2024-05-14 14:02:30,394 >> Special tokens file saved in outputs/instruct_tuning/model/checkpoint-200/special_tokens_map.json
[2024-05-14 14:02:31,355] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step200 is about to be saved!
[2024-05-14 14:02:31,414] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: outputs/instruct_tuning/model/checkpoint-200/global_step200/mp_rank_00_model_states.pt
[2024-05-14 14:02:31,414] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving outputs/instruct_tuning/model/checkpoint-200/global_step200/mp_rank_00_model_states.pt...
/mounts/work/lyk/anaconda3/envs/torch2/lib/python3.8/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
[2024-05-14 14:04:06,677] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved outputs/instruct_tuning/model/checkpoint-200/global_step200/mp_rank_00_model_states.pt.
[2024-05-14 14:04:06,680] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step200 is ready now!
[2024-05-14 14:04:07,820] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 200
[2024-05-14 14:04:07,821] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 14:04:07,821] [INFO] [replace_operation.py:230:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
  6%|▌         | 201/3218 [08:36<36:40:08, 43.75s/it]
[2024-05-14 14:04:09,409] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 201
[2024-05-14 14:04:09,409] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  6%|▋         | 202/3218 [08:37<26:03:24, 31.10s/it]
[2024-05-14 14:04:11,526] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 202
[2024-05-14 14:04:11,527] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  6%|▋         | 203/3218 [08:39<18:45:59, 22.41s/it]
[2024-05-14 14:04:13,060] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 203
[2024-05-14 14:04:13,060] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  6%|▋         | 204/3218 [08:41<13:30:51, 16.14s/it]
[2024-05-14 14:04:15,236] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 204
[2024-05-14 14:04:15,237] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  6%|▋         | 205/3218 [08:43<10:01:40, 11.98s/it]
[2024-05-14 14:04:16,871] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 205
[2024-05-14 14:04:16,871] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  6%|▋         | 207/3218 [08:46<5:33:09,  6.64s/it]
[2024-05-14 14:04:18,369] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 206
[2024-05-14 14:04:18,369] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  6%|▋         | 208/3218 [08:48<4:16:45,  5.12s/it]
[2024-05-14 14:04:19,925] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 207
[2024-05-14 14:04:19,925] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 14:04:19,926] [INFO] [replace_operation.py:230:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 14:04:21,381] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 208
[2024-05-14 14:04:21,381] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  7%|▋         | 210/3218 [08:50<2:37:18,  3.14s/it]
[2024-05-14 14:04:22,624] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 209
[2024-05-14 14:04:22,624] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 14:04:22,625] [INFO] [replace_operation.py:230:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25

  7%|▋         | 211/3218 [08:52<2:17:36,  2.75s/it]
[2024-05-14 14:04:24,373] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 210
[2024-05-14 14:04:24,374] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  7%|▋         | 212/3218 [08:54<2:10:04,  2.60s/it]
[2024-05-14 14:04:26,454] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 211
[2024-05-14 14:04:26,454] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 14:04:26,454] [INFO] [replace_operation.py:230:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 14:04:27,647] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 212
[2024-05-14 14:04:27,647] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  7%|▋         | 213/3218 [08:55<1:39:54,  1.99s/it]
[2024-05-14 14:04:29,129] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 213
[2024-05-14 14:04:29,129] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  7%|▋         | 214/3218 [08:57<1:40:51,  2.01s/it]
[2024-05-14 14:04:31,178] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 214
[2024-05-14 14:04:31,178] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  7%|▋         | 215/3218 [08:59<1:41:08,  2.02s/it]
[2024-05-14 14:04:33,129] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 215
[2024-05-14 14:04:33,129] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  7%|▋         | 216/3218 [09:01<1:40:04,  2.00s/it]
[2024-05-14 14:04:34,945] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 216
[2024-05-14 14:04:34,945] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  7%|▋         | 217/3218 [09:03<1:38:02,  1.96s/it]
[2024-05-14 14:04:36,740] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 217
[2024-05-14 14:04:36,741] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  7%|▋         | 218/3218 [09:05<1:35:19,  1.91s/it]
[2024-05-14 14:04:39,088] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 218
[2024-05-14 14:04:39,089] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  7%|▋         | 219/3218 [09:07<1:42:32,  2.05s/it]
[2024-05-14 14:04:41,076] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 219
[2024-05-14 14:04:41,076] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 14:04:41,077] [INFO] [replace_operation.py:230:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25

  7%|▋         | 220/3218 [09:09<1:41:04,  2.02s/it]
[2024-05-14 14:04:43,107] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 220
[2024-05-14 14:04:43,108] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  7%|▋         | 221/3218 [09:11<1:40:18,  2.01s/it]
[2024-05-14 14:04:45,026] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 221
[2024-05-14 14:04:45,027] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  7%|▋         | 222/3218 [09:13<1:39:01,  1.98s/it]
[2024-05-14 14:04:46,968] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 222
[2024-05-14 14:04:46,969] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  7%|▋         | 223/3218 [09:15<1:39:24,  1.99s/it]
[2024-05-14 14:04:48,936] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 223
[2024-05-14 14:04:48,936] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  7%|▋         | 224/3218 [09:17<1:39:48,  2.00s/it]
[2024-05-14 14:04:51,002] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 224
[2024-05-14 14:04:51,003] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  7%|▋         | 225/3218 [09:19<1:40:48,  2.02s/it]
[2024-05-14 14:04:52,922] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 225
[2024-05-14 14:04:52,922] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  7%|▋         | 226/3218 [09:21<1:38:22,  1.97s/it]
[2024-05-14 14:04:55,316] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 226
[2024-05-14 14:04:55,317] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  7%|▋         | 227/3218 [09:23<1:44:54,  2.10s/it]
[2024-05-14 14:04:57,240] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 227
[2024-05-14 14:04:57,241] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  7%|▋         | 228/3218 [09:25<1:40:59,  2.03s/it]
[2024-05-14 14:04:59,099] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 228
[2024-05-14 14:04:59,100] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  7%|▋         | 229/3218 [09:27<1:37:55,  1.97s/it]
[2024-05-14 14:05:01,604] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 229
[2024-05-14 14:05:01,605] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  7%|▋         | 230/3218 [09:30<1:47:06,  2.15s/it]
{'loss': 0.0, 'learning_rate': 0.0, 'epoch': 0.07}
[2024-05-14 14:05:03,535] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 230
[2024-05-14 14:05:03,536] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  7%|▋         | 231/3218 [09:31<1:43:40,  2.08s/it]
[2024-05-14 14:05:05,253] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 231
[2024-05-14 14:05:05,254] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  7%|▋         | 232/3218 [09:33<1:38:30,  1.98s/it]
[2024-05-14 14:05:07,387] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 232
[2024-05-14 14:05:07,388] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  7%|▋         | 233/3218 [09:35<1:41:01,  2.03s/it]
[2024-05-14 14:05:09,636] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 233
[2024-05-14 14:05:09,636] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  7%|▋         | 234/3218 [09:38<1:44:54,  2.11s/it]
[2024-05-14 14:05:11,817] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 234
[2024-05-14 14:05:11,818] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  7%|▋         | 235/3218 [09:40<1:45:35,  2.12s/it]
[2024-05-14 14:05:13,798] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 235
[2024-05-14 14:05:13,799] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  7%|▋         | 236/3218 [09:42<1:42:19,  2.06s/it]
[2024-05-14 14:05:15,406] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 236
[2024-05-14 14:05:15,407] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  7%|▋         | 237/3218 [09:43<1:35:42,  1.93s/it]
[2024-05-14 14:05:16,975] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 237
[2024-05-14 14:05:16,975] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  7%|▋         | 238/3218 [09:45<1:29:59,  1.81s/it]
[2024-05-14 14:05:18,596] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 238
[2024-05-14 14:05:18,597] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  7%|▋         | 240/3218 [09:48<1:21:57,  1.65s/it]
[2024-05-14 14:05:20,012] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 239
[2024-05-14 14:05:20,012] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 14:05:20,013] [INFO] [replace_operation.py:230:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
{'loss': 0.0, 'learning_rate': 0.0, 'epoch': 0.07}
[2024-05-14 14:05:21,488] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 240
[2024-05-14 14:05:21,488] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  7%|▋         | 241/3218 [09:49<1:19:04,  1.59s/it]
[2024-05-14 14:05:22,942] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 241
[2024-05-14 14:05:22,942] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  8%|▊         | 243/3218 [09:52<1:13:41,  1.49s/it]
[2024-05-14 14:05:24,284] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 242
[2024-05-14 14:05:24,284] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 14:05:24,285] [INFO] [replace_operation.py:230:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 14:05:25,613] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 243
[2024-05-14 14:05:25,613] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  8%|▊         | 244/3218 [09:54<1:11:13,  1.44s/it]
[2024-05-14 14:05:26,857] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 244
[2024-05-14 14:05:26,857] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  8%|▊         | 246/3218 [09:56<1:11:22,  1.44s/it]
[2024-05-14 14:05:28,518] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 245
[2024-05-14 14:05:28,519] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 14:05:28,519] [INFO] [replace_operation.py:230:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 14:05:30,668] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 246
[2024-05-14 14:05:30,669] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 14:05:30,669] [INFO] [replace_operation.py:230:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 14:05:31,912] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 247
[2024-05-14 14:05:31,912] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25


  8%|▊         | 249/3218 [10:02<1:31:29,  1.85s/it]
[2024-05-14 14:05:34,273] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 248
[2024-05-14 14:05:34,273] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  8%|▊         | 250/3218 [10:04<1:33:13,  1.88s/it]
[2024-05-14 14:05:36,236] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 249
[2024-05-14 14:05:36,236] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 14:05:36,237] [INFO] [replace_operation.py:230:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
{'loss': 0.0, 'learning_rate': 0.0, 'epoch': 0.08}
[2024-05-14 14:05:38,589] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 250
[2024-05-14 14:05:38,589] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  8%|▊         | 252/3218 [10:08<1:37:41,  1.98s/it]
[2024-05-14 14:05:40,438] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 251
[2024-05-14 14:05:40,438] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  8%|▊         | 253/3218 [10:10<1:36:49,  1.96s/it]
[2024-05-14 14:05:42,328] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 252
[2024-05-14 14:05:42,328] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  8%|▊         | 254/3218 [10:12<1:34:28,  1.91s/it]
[2024-05-14 14:05:44,158] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 253
[2024-05-14 14:05:44,158] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  8%|▊         | 255/3218 [10:14<1:34:23,  1.91s/it]
[2024-05-14 14:05:46,055] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 254
[2024-05-14 14:05:46,055] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
