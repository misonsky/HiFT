  0%|          | 0/3218 [00:00<?, ?it/s]/mounts/work/lyk/anaconda3/envs/torch2/lib/python3.8/site-packages/deepspeed/runtime/utils.py:274: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/conda/conda-bld/pytorch_1699449229234/work/torch/csrc/tensor/python_tensor.cpp:83.)
  overflow_gpu = get_accelerator().ByteTensor([overflow])
  0%|          | 1/3218 [00:01<1:33:52,  1.75s/it]
[2024-05-14 14:24:56,733] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 0
[2024-05-14 14:24:56,734] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2024-05-14 14:24:56,775] [INFO] [replace_operation.py:230:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0

  0%|          | 2/3218 [00:03<1:35:01,  1.77s/it]
[2024-05-14 14:24:58,375] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1
[2024-05-14 14:24:58,375] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2024-05-14 14:24:58,375] [INFO] [replace_operation.py:230:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
[2024-05-14 14:25:00,174] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 2
[2024-05-14 14:25:00,175] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0

  0%|          | 3/3218 [00:05<1:34:56,  1.77s/it]
[2024-05-14 14:25:02,021] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 3
[2024-05-14 14:25:02,021] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 8192.0 to 4096.0

  0%|          | 4/3218 [00:07<1:36:30,  1.80s/it]
[2024-05-14 14:25:03,889] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 4
[2024-05-14 14:25:03,889] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 4096.0 to 2048.0

  0%|          | 5/3218 [00:09<1:37:47,  1.83s/it]
[2024-05-14 14:25:06,118] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 5
[2024-05-14 14:25:06,118] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 2048.0 to 1024.0

  0%|          | 6/3218 [00:11<1:45:11,  1.96s/it]
[2024-05-14 14:25:07,904] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 6
[2024-05-14 14:25:07,905] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 1024.0 to 512.0

  0%|          | 7/3218 [00:13<1:41:50,  1.90s/it]
[2024-05-14 14:25:10,185] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 7
[2024-05-14 14:25:10,186] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 512.0 to 256.0

  0%|          | 8/3218 [00:15<1:48:20,  2.03s/it]
[2024-05-14 14:25:11,730] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 8
[2024-05-14 14:25:11,730] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 256.0 to 128.0

  0%|          | 9/3218 [00:16<1:40:08,  1.87s/it]
[2024-05-14 14:25:13,387] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 9
[2024-05-14 14:25:13,387] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 128.0 to 64.0
[2024-05-14 14:25:13,387] [INFO] [replace_operation.py:230:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 128.0, reducing to 64.0

  0%|          | 10/3218 [00:18<1:36:45,  1.81s/it]
[2024-05-14 14:25:15,437] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 10
[2024-05-14 14:25:15,437] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 64.0 to 32.0

  0%|          | 11/3218 [00:20<1:40:31,  1.88s/it]
[2024-05-14 14:25:17,553] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 11
[2024-05-14 14:25:17,553] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 32.0 to 16.0

  0%|          | 12/3218 [00:22<1:44:49,  1.96s/it]
[2024-05-14 14:25:19,841] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 12
[2024-05-14 14:25:19,842] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 16.0 to 8.0

  0%|          | 13/3218 [00:25<1:50:06,  2.06s/it]
[2024-05-14 14:25:21,866] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 13
[2024-05-14 14:25:21,866] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 8.0 to 4.0

  0%|          | 14/3218 [00:27<1:49:07,  2.04s/it]
[2024-05-14 14:25:23,918] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 14
[2024-05-14 14:25:23,919] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 4.0 to 2.0

  0%|          | 15/3218 [00:29<1:49:22,  2.05s/it]
[2024-05-14 14:25:25,749] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 15
[2024-05-14 14:25:25,749] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 2.0 to 1.0

  0%|          | 16/3218 [00:30<1:45:55,  1.98s/it]
[2024-05-14 14:25:27,505] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 16
[2024-05-14 14:25:27,506] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 1.0 to 0.5

  1%|          | 18/3218 [00:34<1:38:28,  1.85s/it]
[2024-05-14 14:25:29,200] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 17
[2024-05-14 14:25:29,200] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.5 to 0.25

  1%|          | 19/3218 [00:36<1:39:02,  1.86s/it]
[2024-05-14 14:25:31,020] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 18
[2024-05-14 14:25:31,020] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  1%|          | 20/3218 [00:38<1:42:17,  1.92s/it]
[2024-05-14 14:25:32,914] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 19
[2024-05-14 14:25:32,914] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 14:25:32,915] [INFO] [replace_operation.py:230:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
{'loss': 0.0, 'learning_rate': 0.0, 'epoch': 0.01}
[2024-05-14 14:25:35,175] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 20
[2024-05-14 14:25:35,176] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  1%|          | 21/3218 [00:40<1:47:35,  2.02s/it]
[2024-05-14 14:25:37,247] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 21
[2024-05-14 14:25:37,248] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  1%|          | 22/3218 [00:42<1:48:25,  2.04s/it]
[2024-05-14 14:25:39,261] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 22
[2024-05-14 14:25:39,261] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  1%|          | 23/3218 [00:44<1:48:35,  2.04s/it]
[2024-05-14 14:25:41,482] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 23
[2024-05-14 14:25:41,482] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  1%|          | 24/3218 [00:46<1:50:45,  2.08s/it]
[2024-05-14 14:25:43,812] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 24
[2024-05-14 14:25:43,812] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  1%|          | 25/3218 [00:49<1:54:47,  2.16s/it]
[2024-05-14 14:25:45,848] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 25
[2024-05-14 14:25:45,848] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  1%|          | 26/3218 [00:51<1:52:58,  2.12s/it]
[2024-05-14 14:25:47,940] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 26
[2024-05-14 14:25:47,940] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  1%|          | 27/3218 [00:53<1:52:17,  2.11s/it]
[2024-05-14 14:25:49,852] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 27
[2024-05-14 14:25:49,853] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  1%|          | 28/3218 [00:55<1:48:49,  2.05s/it]
[2024-05-14 14:25:51,731] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 28
[2024-05-14 14:25:51,732] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  1%|          | 29/3218 [00:57<1:46:14,  2.00s/it]
[2024-05-14 14:25:53,678] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 29
[2024-05-14 14:25:53,679] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  1%|          | 30/3218 [00:59<1:46:29,  2.00s/it]
{'loss': 0.0, 'learning_rate': 0.0, 'epoch': 0.01}
[2024-05-14 14:25:55,793] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 30
[2024-05-14 14:25:55,794] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  1%|          | 31/3218 [01:01<1:48:32,  2.04s/it]
[2024-05-14 14:25:57,482] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 31
[2024-05-14 14:25:57,482] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  1%|          | 32/3218 [01:02<1:41:02,  1.90s/it]
[2024-05-14 14:25:59,070] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 32
[2024-05-14 14:25:59,070] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  1%|          | 35/3218 [01:06<1:12:51,  1.37s/it]
[2024-05-14 14:26:00,674] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 33
[2024-05-14 14:26:00,674] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 14:26:00,674] [INFO] [replace_operation.py:230:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 14:26:01,211] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 34
[2024-05-14 14:26:01,211] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 14:26:01,211] [INFO] [replace_operation.py:230:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 14:26:03,789] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 35
[2024-05-14 14:26:03,790] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  1%|          | 36/3218 [01:09<1:34:12,  1.78s/it]
[2024-05-14 14:26:05,755] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 36
[2024-05-14 14:26:05,756] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25


  1%|          | 38/3218 [01:14<1:56:23,  2.20s/it]
[2024-05-14 14:26:08,508] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 37
[2024-05-14 14:26:08,508] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 14:26:08,508] [INFO] [replace_operation.py:230:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 14:26:11,384] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 38
[2024-05-14 14:26:11,384] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  1%|          | 39/3218 [01:16<2:06:46,  2.39s/it]
[2024-05-14 14:26:14,047] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 39
[2024-05-14 14:26:14,048] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  1%|          | 40/3218 [01:19<2:11:36,  2.48s/it]

  1%|▏         | 41/3218 [01:22<2:11:14,  2.48s/it]
[2024-05-14 14:26:16,522] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 40
[2024-05-14 14:26:16,522] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 14:26:16,522] [INFO] [replace_operation.py:230:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 14:26:19,383] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 41
[2024-05-14 14:26:19,383] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  1%|▏         | 42/3218 [01:25<2:19:34,  2.64s/it]
[2024-05-14 14:26:22,768] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 42
[2024-05-14 14:26:22,769] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  1%|▏         | 43/3218 [01:28<2:34:11,  2.91s/it]
[2024-05-14 14:26:25,732] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 43
[2024-05-14 14:26:25,733] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  1%|▏         | 44/3218 [01:31<2:29:45,  2.83s/it]
[2024-05-14 14:26:28,143] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 44
[2024-05-14 14:26:28,143] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  1%|▏         | 45/3218 [01:33<2:22:38,  2.70s/it]
[2024-05-14 14:26:30,905] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 45
[2024-05-14 14:26:30,905] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  1%|▏         | 46/3218 [01:36<2:25:58,  2.76s/it]
[2024-05-14 14:26:33,491] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 46
[2024-05-14 14:26:33,491] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  1%|▏         | 47/3218 [01:39<2:22:22,  2.69s/it]
[2024-05-14 14:26:35,833] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 47
[2024-05-14 14:26:35,833] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  1%|▏         | 48/3218 [01:41<2:15:34,  2.57s/it]
[2024-05-14 14:26:37,968] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 48
[2024-05-14 14:26:37,969] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  2%|▏         | 49/3218 [01:43<2:08:41,  2.44s/it]
[2024-05-14 14:26:40,352] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 49
[2024-05-14 14:26:40,353] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  2%|▏         | 50/3218 [01:46<2:07:15,  2.41s/it]

  2%|▏         | 51/3218 [01:48<2:06:26,  2.40s/it]
[2024-05-14 14:26:42,669] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 50
[2024-05-14 14:26:42,670] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 14:26:42,670] [INFO] [replace_operation.py:230:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 14:26:44,881] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 51
[2024-05-14 14:26:44,882] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  2%|▏         | 52/3218 [01:50<2:02:54,  2.33s/it]
[2024-05-14 14:26:47,122] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 52
[2024-05-14 14:26:47,123] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  2%|▏         | 53/3218 [01:52<2:02:31,  2.32s/it]
[2024-05-14 14:26:49,630] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 53
[2024-05-14 14:26:49,631] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  2%|▏         | 54/3218 [01:55<2:08:26,  2.44s/it]
[2024-05-14 14:26:52,306] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 54
[2024-05-14 14:26:52,306] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  2%|▏         | 55/3218 [01:58<2:15:18,  2.57s/it]
[2024-05-14 14:26:54,916] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 55
[2024-05-14 14:26:54,916] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  2%|▏         | 56/3218 [02:00<2:14:32,  2.55s/it]
[2024-05-14 14:26:57,558] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 56
[2024-05-14 14:26:57,559] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  2%|▏         | 57/3218 [02:03<2:15:43,  2.58s/it]
[2024-05-14 14:27:00,201] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 57
[2024-05-14 14:27:00,201] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  2%|▏         | 58/3218 [02:06<2:16:06,  2.58s/it]
[2024-05-14 14:27:03,039] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 58
[2024-05-14 14:27:03,039] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  2%|▏         | 59/3218 [02:08<2:20:12,  2.66s/it]
[2024-05-14 14:27:05,653] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 59
[2024-05-14 14:27:05,654] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  2%|▏         | 60/3218 [02:11<2:18:45,  2.64s/it]
{'loss': 0.0, 'learning_rate': 0.0, 'epoch': 0.02}
[2024-05-14 14:27:08,160] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 60
[2024-05-14 14:27:08,160] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  2%|▏         | 61/3218 [02:14<2:16:37,  2.60s/it]
[2024-05-14 14:27:10,789] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 61
[2024-05-14 14:27:10,790] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  2%|▏         | 62/3218 [02:16<2:18:01,  2.62s/it]
[2024-05-14 14:27:13,482] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 62
[2024-05-14 14:27:13,483] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  2%|▏         | 63/3218 [02:19<2:16:37,  2.60s/it]
[2024-05-14 14:27:15,966] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 63
[2024-05-14 14:27:15,966] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  2%|▏         | 64/3218 [02:21<2:15:41,  2.58s/it]
[2024-05-14 14:27:18,409] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 64
[2024-05-14 14:27:18,410] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  2%|▏         | 65/3218 [02:24<2:15:51,  2.59s/it]
[2024-05-14 14:27:21,124] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 65
[2024-05-14 14:27:21,125] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  2%|▏         | 66/3218 [02:27<2:19:40,  2.66s/it]
[2024-05-14 14:27:23,717] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 66
[2024-05-14 14:27:23,718] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  2%|▏         | 67/3218 [02:29<2:15:26,  2.58s/it]
[2024-05-14 14:27:26,495] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 67
[2024-05-14 14:27:26,495] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  2%|▏         | 69/3218 [02:33<1:51:20,  2.12s/it]
[2024-05-14 14:27:28,800] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 68
[2024-05-14 14:27:28,801] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 14:27:28,801] [INFO] [replace_operation.py:230:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 14:27:29,403] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 69
[2024-05-14 14:27:29,403] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 14:27:29,403] [INFO] [replace_operation.py:230:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25

  2%|▏         | 70/3218 [02:34<1:43:23,  1.97s/it]
[2024-05-14 14:27:32,058] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 70
[2024-05-14 14:27:32,058] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  2%|▏         | 71/3218 [02:37<1:57:06,  2.23s/it]
[2024-05-14 14:27:35,092] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 71
[2024-05-14 14:27:35,093] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  2%|▏         | 72/3218 [02:40<2:09:17,  2.47s/it]
[2024-05-14 14:27:37,734] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 72
[2024-05-14 14:27:37,735] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  2%|▏         | 73/3218 [02:43<2:10:55,  2.50s/it]
[2024-05-14 14:27:40,216] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 73
[2024-05-14 14:27:40,217] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  2%|▏         | 74/3218 [02:45<2:11:48,  2.52s/it]
[2024-05-14 14:27:43,023] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 74
[2024-05-14 14:27:43,023] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

