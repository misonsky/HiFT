  0%|          | 0/6436 [00:00<?, ?it/s]/mounts/work/lyk/anaconda3/envs/torch2/lib/python3.8/site-packages/deepspeed/runtime/utils.py:274: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/conda/conda-bld/pytorch_1699449229234/work/torch/csrc/tensor/python_tensor.cpp:83.)
  overflow_gpu = get_accelerator().ByteTensor([overflow])
  0%|          | 1/6436 [00:01<2:14:41,  1.26s/it]
[2024-05-14 01:56:23,531] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 0
[2024-05-14 01:56:23,532] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2024-05-14 01:56:23,534] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0

  0%|          | 3/6436 [00:03<2:04:10,  1.16s/it]
[2024-05-14 01:56:25,407] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1
[2024-05-14 01:56:25,407] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2024-05-14 01:56:25,409] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
[2024-05-14 01:56:26,104] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 2
[2024-05-14 01:56:26,105] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2024-05-14 01:56:26,106] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
[2024-05-14 01:56:26,879] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 3
[2024-05-14 01:56:26,880] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 8192.0 to 4096.0

  0%|          | 4/6436 [00:04<1:52:00,  1.04s/it]
[2024-05-14 01:56:28,193] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 4
[2024-05-14 01:56:28,193] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 4096.0 to 2048.0

  0%|          | 6/6436 [00:07<2:02:41,  1.14s/it]
[2024-05-14 01:56:29,356] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 5
[2024-05-14 01:56:29,356] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 2048.0 to 1024.0
[2024-05-14 01:56:29,357] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 2048.0, reducing to 1024.0
[2024-05-14 01:56:30,380] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 6
[2024-05-14 01:56:30,381] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 1024.0 to 512.0

  0%|          | 8/6436 [00:09<2:00:30,  1.12s/it]
[2024-05-14 01:56:31,543] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 7
[2024-05-14 01:56:31,544] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 512.0 to 256.0
[2024-05-14 01:56:31,551] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 512.0, reducing to 256.0
[2024-05-14 01:56:32,870] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 8
[2024-05-14 01:56:32,871] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 256.0 to 128.0

  0%|          | 10/6436 [00:11<2:05:51,  1.18s/it]
[2024-05-14 01:56:34,023] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 9
[2024-05-14 01:56:34,023] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 128.0 to 64.0
[2024-05-14 01:56:34,024] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 128.0, reducing to 64.0

  0%|          | 11/6436 [00:13<2:15:21,  1.26s/it]
[2024-05-14 01:56:35,492] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 10
[2024-05-14 01:56:35,492] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 64.0 to 32.0
[2024-05-14 01:56:35,493] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 64.0, reducing to 32.0
[2024-05-14 01:56:36,475] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 11
[2024-05-14 01:56:36,475] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 32.0 to 16.0

  0%|          | 13/6436 [00:15<1:59:45,  1.12s/it]
[2024-05-14 01:56:37,453] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 12
[2024-05-14 01:56:37,454] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 16.0 to 8.0
[2024-05-14 01:56:37,456] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 16.0, reducing to 8.0
[2024-05-14 01:56:38,338] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 13
[2024-05-14 01:56:38,339] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 8.0 to 4.0

  0%|          | 15/6436 [00:17<1:49:09,  1.02s/it]
[2024-05-14 01:56:39,293] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 14
[2024-05-14 01:56:39,293] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 4.0 to 2.0
[2024-05-14 01:56:39,295] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 4.0, reducing to 2.0
[2024-05-14 01:56:40,887] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 15
[2024-05-14 01:56:40,888] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 2.0 to 1.0

  0%|          | 17/6436 [00:19<1:58:41,  1.11s/it]
[2024-05-14 01:56:41,792] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 16
[2024-05-14 01:56:41,793] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 1.0 to 0.5
[2024-05-14 01:56:41,793] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 1.0, reducing to 0.5
[2024-05-14 01:56:42,861] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 17
[2024-05-14 01:56:42,861] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.5 to 0.25

  0%|          | 19/6436 [00:21<1:54:11,  1.07s/it]
[2024-05-14 01:56:43,855] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 18
[2024-05-14 01:56:43,855] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 01:56:43,857] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 01:56:45,079] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 19
[2024-05-14 01:56:45,079] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  0%|          | 20/6436 [00:22<1:59:17,  1.12s/it]
{'loss': 0.0, 'learning_rate': 2e-05, 'epoch': 0.0}
[2024-05-14 01:56:46,467] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 20
[2024-05-14 01:56:46,468] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  0%|          | 22/6436 [00:25<2:10:23,  1.22s/it]
[2024-05-14 01:56:47,751] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 21
[2024-05-14 01:56:47,751] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 01:56:47,752] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 01:56:48,767] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 22
[2024-05-14 01:56:48,767] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  0%|          | 24/6436 [00:27<1:56:53,  1.09s/it]
[2024-05-14 01:56:49,701] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 23
[2024-05-14 01:56:49,701] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 01:56:49,703] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 01:56:50,933] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 24
[2024-05-14 01:56:50,933] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  0%|          | 26/6436 [00:29<1:55:52,  1.08s/it]
[2024-05-14 01:56:51,902] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 25
[2024-05-14 01:56:51,902] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 01:56:51,903] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 01:56:52,648] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 26
[2024-05-14 01:56:52,648] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  0%|          | 28/6436 [00:31<1:47:24,  1.01s/it]
[2024-05-14 01:56:53,706] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 27
[2024-05-14 01:56:53,706] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 01:56:53,707] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 01:56:54,754] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 28
[2024-05-14 01:56:54,754] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  0%|          | 30/6436 [00:33<1:40:30,  1.06it/s]
[2024-05-14 01:56:55,524] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 29
[2024-05-14 01:56:55,524] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 01:56:55,525] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
{'loss': 0.0, 'learning_rate': 2e-05, 'epoch': 0.0}
[2024-05-14 01:56:56,521] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 30
[2024-05-14 01:56:56,522] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  0%|          | 32/6436 [00:35<2:00:59,  1.13s/it]
[2024-05-14 01:56:57,848] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 31
[2024-05-14 01:56:57,849] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 01:56:57,849] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 01:56:59,073] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 32
[2024-05-14 01:56:59,073] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  1%|          | 34/6436 [00:37<1:53:07,  1.06s/it]
[2024-05-14 01:57:00,029] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 33
[2024-05-14 01:57:00,030] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 01:57:00,031] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 01:57:00,969] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 34
[2024-05-14 01:57:00,970] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  1%|          | 35/6436 [00:38<1:47:39,  1.01s/it]
[2024-05-14 01:57:02,096] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 35
[2024-05-14 01:57:02,096] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  1%|          | 36/6436 [00:39<1:52:57,  1.06s/it]
[2024-05-14 01:57:03,463] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 36
[2024-05-14 01:57:03,463] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 01:57:03,464] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 01:57:04,818] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 37
[2024-05-14 01:57:04,818] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  1%|          | 39/6436 [00:43<1:51:36,  1.05s/it]
[2024-05-14 01:57:05,544] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 38
[2024-05-14 01:57:05,544] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 01:57:05,546] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 01:57:06,675] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 39
[2024-05-14 01:57:06,676] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 01:57:06,677] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25

  1%|          | 40/6436 [00:44<1:54:34,  1.07s/it]
[2024-05-14 01:57:08,226] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 40
[2024-05-14 01:57:08,227] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  1%|          | 42/6436 [00:47<2:11:57,  1.24s/it]
[2024-05-14 01:57:09,500] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 41
[2024-05-14 01:57:09,501] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 01:57:09,503] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 01:57:10,811] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 42
[2024-05-14 01:57:10,811] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  1%|          | 43/6436 [00:48<2:14:53,  1.27s/it]
[2024-05-14 01:57:12,020] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 43
[2024-05-14 01:57:12,020] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

