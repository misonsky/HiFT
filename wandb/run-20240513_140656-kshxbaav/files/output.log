  0%|          | 0/3218 [00:00<?, ?it/s]/mounts/work/lyk/anaconda3/envs/torch2/lib/python3.8/site-packages/deepspeed/runtime/utils.py:274: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/conda/conda-bld/pytorch_1699449229234/work/torch/csrc/tensor/python_tensor.cpp:83.)
  overflow_gpu = get_accelerator().ByteTensor([overflow])
  0%|          | 1/3218 [00:01<1:28:37,  1.65s/it]
[2024-05-13 14:07:01,124] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 0
[2024-05-13 14:07:01,124] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2024-05-13 14:07:01,126] [INFO] [replace_operation.py:192:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0

  0%|          | 2/3218 [00:03<1:31:51,  1.71s/it]
[2024-05-13 14:07:02,757] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1
[2024-05-13 14:07:02,757] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2024-05-13 14:07:02,758] [INFO] [replace_operation.py:192:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
[2024-05-13 14:07:03,663] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 2
[2024-05-13 14:07:03,663] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0

  0%|          | 4/3218 [00:05<1:08:22,  1.28s/it]
[2024-05-13 14:07:04,742] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 3
[2024-05-13 14:07:04,743] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 8192.0 to 4096.0

  0%|          | 5/3218 [00:07<1:19:52,  1.49s/it]
[2024-05-13 14:07:06,637] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 4
[2024-05-13 14:07:06,638] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 4096.0 to 2048.0
[2024-05-13 14:07:06,638] [INFO] [replace_operation.py:192:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 4096.0, reducing to 2048.0
[2024-05-13 14:07:08,590] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 5
[2024-05-13 14:07:08,591] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 2048.0 to 1024.0

  0%|          | 6/3218 [00:09<1:28:14,  1.65s/it]
[2024-05-13 14:07:10,048] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 6
[2024-05-13 14:07:10,049] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 1024.0 to 512.0

  0%|          | 7/3218 [00:10<1:24:45,  1.58s/it]
[2024-05-13 14:07:12,226] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 7
[2024-05-13 14:07:12,227] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 512.0 to 256.0

  0%|          | 8/3218 [00:12<1:35:25,  1.78s/it]
[2024-05-13 14:07:13,620] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 8
[2024-05-13 14:07:13,620] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 256.0 to 128.0

  0%|          | 9/3218 [00:14<1:28:49,  1.66s/it]
[2024-05-13 14:07:15,433] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 9
[2024-05-13 14:07:15,434] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 128.0 to 64.0
[2024-05-13 14:07:15,435] [INFO] [replace_operation.py:192:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 128.0, reducing to 64.0

  0%|          | 11/3218 [00:18<1:36:49,  1.81s/it]
[2024-05-13 14:07:17,507] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 10
[2024-05-13 14:07:17,507] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 64.0 to 32.0

  0%|          | 12/3218 [00:19<1:35:43,  1.79s/it]
[2024-05-13 14:07:19,249] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 11
[2024-05-13 14:07:19,249] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 32.0 to 16.0

  0%|          | 13/3218 [00:21<1:38:34,  1.85s/it]
[2024-05-13 14:07:21,212] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 12
[2024-05-13 14:07:21,213] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 16.0 to 8.0

  0%|          | 14/3218 [00:23<1:41:22,  1.90s/it]
[2024-05-13 14:07:23,262] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 13
[2024-05-13 14:07:23,262] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 8.0 to 4.0

  0%|          | 15/3218 [00:25<1:41:25,  1.90s/it]
[2024-05-13 14:07:25,162] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 14
[2024-05-13 14:07:25,163] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 4.0 to 2.0

  0%|          | 16/3218 [00:27<1:39:34,  1.87s/it]
[2024-05-13 14:07:26,934] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 15
[2024-05-13 14:07:26,934] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 2.0 to 1.0
[2024-05-13 14:07:26,934] [INFO] [replace_operation.py:192:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 2.0, reducing to 1.0
[2024-05-13 14:07:28,520] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 16
[2024-05-13 14:07:28,520] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 1.0 to 0.5

  1%|          | 17/3218 [00:29<1:34:59,  1.78s/it]
[2024-05-13 14:07:30,091] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 17
[2024-05-13 14:07:30,091] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.5 to 0.25

  1%|          | 18/3218 [00:30<1:31:29,  1.72s/it]
[2024-05-13 14:07:31,629] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 18
[2024-05-13 14:07:31,629] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  1%|          | 20/3218 [00:33<1:24:52,  1.59s/it]
[2024-05-13 14:07:33,046] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 19
[2024-05-13 14:07:33,047] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-13 14:07:33,047] [INFO] [replace_operation.py:192:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25

  1%|          | 21/3218 [00:35<1:25:34,  1.61s/it]
[2024-05-13 14:07:34,710] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 20
[2024-05-13 14:07:34,710] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-13 14:07:34,711] [INFO] [replace_operation.py:192:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-13 14:07:36,242] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 21
[2024-05-13 14:07:36,243] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  1%|          | 22/3218 [00:36<1:24:30,  1.59s/it]
[2024-05-13 14:07:37,708] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 22
[2024-05-13 14:07:37,708] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  1%|          | 24/3218 [00:39<1:22:54,  1.56s/it]
[2024-05-13 14:07:39,267] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 23
[2024-05-13 14:07:39,268] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  1%|          | 25/3218 [00:41<1:27:13,  1.64s/it]
[2024-05-13 14:07:41,112] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 24
[2024-05-13 14:07:41,112] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-13 14:07:41,112] [INFO] [replace_operation.py:192:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-13 14:07:42,640] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 25
[2024-05-13 14:07:42,641] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  1%|          | 26/3218 [00:43<1:25:33,  1.61s/it]
[2024-05-13 14:07:44,225] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 26
[2024-05-13 14:07:44,225] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  1%|          | 27/3218 [00:44<1:25:09,  1.60s/it]
[2024-05-13 14:07:46,454] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 27
[2024-05-13 14:07:46,454] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  1%|          | 28/3218 [00:47<1:35:04,  1.79s/it]
[2024-05-13 14:07:47,753] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 28
[2024-05-13 14:07:47,753] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  1%|          | 30/3218 [00:49<1:22:10,  1.55s/it]
[2024-05-13 14:07:49,060] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 29
[2024-05-13 14:07:49,060] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-13 14:07:49,060] [INFO] [replace_operation.py:192:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
{'loss': 0.0, 'learning_rate': 1e-06, 'epoch': 0.01}
[2024-05-13 14:07:50,586] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 30
[2024-05-13 14:07:50,586] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  1%|          | 31/3218 [00:51<1:21:28,  1.53s/it]
[2024-05-13 14:07:51,854] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 31
[2024-05-13 14:07:51,854] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  1%|          | 33/3218 [00:53<1:13:32,  1.39s/it]
[2024-05-13 14:07:53,090] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 32
[2024-05-13 14:07:53,090] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-13 14:07:53,090] [INFO] [replace_operation.py:192:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-13 14:07:54,423] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 33
[2024-05-13 14:07:54,423] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  1%|          | 35/3218 [00:56<1:08:28,  1.29s/it]
[2024-05-13 14:07:55,641] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 34
[2024-05-13 14:07:55,641] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-13 14:07:55,642] [INFO] [replace_operation.py:192:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-13 14:07:57,810] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 35
[2024-05-13 14:07:57,811] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  1%|          | 36/3218 [00:58<1:24:16,  1.59s/it]
[2024-05-13 14:07:59,618] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 36
[2024-05-13 14:07:59,618] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  1%|          | 38/3218 [01:01<1:16:29,  1.44s/it]
[2024-05-13 14:08:00,906] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 37
[2024-05-13 14:08:00,906] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-13 14:08:00,906] [INFO] [replace_operation.py:192:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-13 14:08:02,421] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 38
[2024-05-13 14:08:02,422] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  1%|          | 39/3218 [01:03<1:24:54,  1.60s/it]
[2024-05-13 14:08:04,434] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 39
[2024-05-13 14:08:04,434] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  1%|          | 40/3218 [01:05<1:31:53,  1.73s/it]
{'loss': 0.0, 'learning_rate': 1e-06, 'epoch': 0.01}
[2024-05-13 14:08:06,297] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 40
[2024-05-13 14:08:06,297] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  1%|▏         | 41/3218 [01:07<1:34:40,  1.79s/it]
[2024-05-13 14:08:08,654] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 41
[2024-05-13 14:08:08,655] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  1%|▏         | 42/3218 [01:09<1:44:59,  1.98s/it]
[2024-05-13 14:08:11,520] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 42
[2024-05-13 14:08:11,521] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  1%|▏         | 43/3218 [01:12<1:59:41,  2.26s/it]
[2024-05-13 14:08:13,679] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 43
[2024-05-13 14:08:13,680] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  1%|▏         | 44/3218 [01:14<1:56:51,  2.21s/it]
[2024-05-13 14:08:15,812] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 44
[2024-05-13 14:08:15,813] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  1%|▏         | 45/3218 [01:16<1:54:50,  2.17s/it]
[2024-05-13 14:08:18,433] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 45
[2024-05-13 14:08:18,434] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  1%|▏         | 46/3218 [01:19<2:02:30,  2.32s/it]
[2024-05-13 14:08:20,670] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 46
[2024-05-13 14:08:20,670] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25


  1%|▏         | 48/3218 [01:23<2:03:14,  2.33s/it]
[2024-05-13 14:08:23,092] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 47
[2024-05-13 14:08:23,092] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  2%|▏         | 49/3218 [01:25<1:53:39,  2.15s/it]
[2024-05-13 14:08:24,852] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 48
[2024-05-13 14:08:24,853] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  2%|▏         | 50/3218 [01:27<1:50:44,  2.10s/it]
[2024-05-13 14:08:26,870] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 49
[2024-05-13 14:08:26,870] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-13 14:08:26,870] [INFO] [replace_operation.py:192:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25

  2%|▏         | 51/3218 [01:29<1:49:54,  2.08s/it]
[2024-05-13 14:08:28,868] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 50
[2024-05-13 14:08:28,868] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-13 14:08:28,868] [INFO] [replace_operation.py:192:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-13 14:08:30,625] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 51
[2024-05-13 14:08:30,625] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  2%|▏         | 52/3218 [01:31<1:45:29,  2.00s/it]
[2024-05-13 14:08:32,630] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 52
[2024-05-13 14:08:32,630] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  2%|▏         | 53/3218 [01:33<1:45:14,  1.99s/it]
[2024-05-13 14:08:34,688] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 53
[2024-05-13 14:08:34,688] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

