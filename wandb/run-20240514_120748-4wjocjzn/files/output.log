  0%|          | 0/3218 [00:00<?, ?it/s]/mounts/work/lyk/anaconda3/envs/torch2/lib/python3.8/site-packages/deepspeed/runtime/utils.py:274: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/conda/conda-bld/pytorch_1699449229234/work/torch/csrc/tensor/python_tensor.cpp:83.)
  overflow_gpu = get_accelerator().ByteTensor([overflow])
[2024-05-14 12:07:57,357] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 0
[2024-05-14 12:07:57,357] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2024-05-14 12:07:57,425] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
{'loss': 0.0, 'learning_rate': 2e-05, 'epoch': 0.0}
  0%|          | 2/3218 [00:03<1:30:04,  1.68s/it]
[2024-05-14 12:07:59,098] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1
[2024-05-14 12:07:59,099] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2024-05-14 12:07:59,099] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
[2024-05-14 12:07:59,832] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 2
[2024-05-14 12:07:59,832] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0

  0%|          | 4/3218 [00:05<1:04:42,  1.21s/it]
[2024-05-14 12:08:00,957] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 3
[2024-05-14 12:08:00,957] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 8192.0 to 4096.0
[2024-05-14 12:08:00,958] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 8192.0, reducing to 4096.0
[2024-05-14 12:08:02,497] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 4
[2024-05-14 12:08:02,498] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 4096.0 to 2048.0

  0%|          | 5/3218 [00:06<1:11:16,  1.33s/it]
[2024-05-14 12:08:04,216] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 5
[2024-05-14 12:08:04,216] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 2048.0 to 1024.0

  0%|          | 6/3218 [00:08<1:17:58,  1.46s/it]
[2024-05-14 12:08:05,440] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 6
[2024-05-14 12:08:05,440] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 1024.0 to 512.0

  0%|          | 8/3218 [00:11<1:23:15,  1.56s/it]
[2024-05-14 12:08:07,359] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 7
[2024-05-14 12:08:07,360] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 512.0 to 256.0
[2024-05-14 12:08:07,361] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 512.0, reducing to 256.0
[2024-05-14 12:08:08,881] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 8
[2024-05-14 12:08:08,882] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 256.0 to 128.0

  0%|          | 9/3218 [00:13<1:24:28,  1.58s/it]
[2024-05-14 12:08:10,642] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 9
[2024-05-14 12:08:10,643] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 128.0 to 64.0

  0%|          | 10/3218 [00:15<1:25:46,  1.60s/it]
{'loss': 0.0, 'learning_rate': 2e-05, 'epoch': 0.0}
[2024-05-14 12:08:12,458] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 10
[2024-05-14 12:08:12,459] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 64.0 to 32.0

  0%|          | 11/3218 [00:16<1:30:26,  1.69s/it]
[2024-05-14 12:08:14,069] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 11
[2024-05-14 12:08:14,070] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 32.0 to 16.0

  0%|          | 12/3218 [00:18<1:28:50,  1.66s/it]
[2024-05-14 12:08:16,166] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 12
[2024-05-14 12:08:16,167] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 16.0 to 8.0

  0%|          | 13/3218 [00:20<1:35:52,  1.79s/it]
[2024-05-14 12:08:17,975] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 13
[2024-05-14 12:08:17,976] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 8.0 to 4.0

  0%|          | 14/3218 [00:22<1:35:10,  1.78s/it]
[2024-05-14 12:08:19,918] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 14
[2024-05-14 12:08:19,919] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 4.0 to 2.0

  0%|          | 15/3218 [00:24<1:38:45,  1.85s/it]
[2024-05-14 12:08:21,726] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 15
[2024-05-14 12:08:21,726] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 2.0 to 1.0

  1%|          | 17/3218 [00:27<1:34:31,  1.77s/it]
[2024-05-14 12:08:23,354] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 16
[2024-05-14 12:08:23,354] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 1.0 to 0.5

  1%|          | 18/3218 [00:29<1:33:27,  1.75s/it]
[2024-05-14 12:08:25,113] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 17
[2024-05-14 12:08:25,113] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.5 to 0.25
[2024-05-14 12:08:25,114] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.5, reducing to 0.25
[2024-05-14 12:08:26,697] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 18
[2024-05-14 12:08:26,698] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  1%|          | 20/3218 [00:32<1:22:41,  1.55s/it]
[2024-05-14 12:08:27,888] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 19
[2024-05-14 12:08:27,888] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:08:27,889] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25

  1%|          | 21/3218 [00:33<1:22:16,  1.54s/it]
[2024-05-14 12:08:29,422] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 20
[2024-05-14 12:08:29,422] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:08:29,423] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 12:08:30,680] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 21
[2024-05-14 12:08:30,680] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  1%|          | 23/3218 [00:36<1:13:06,  1.37s/it]
[2024-05-14 12:08:31,850] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 22
[2024-05-14 12:08:31,850] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  1%|          | 24/3218 [00:37<1:14:19,  1.40s/it]
[2024-05-14 12:08:33,273] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 23
[2024-05-14 12:08:33,273] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:08:33,273] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 12:08:34,873] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 24
[2024-05-14 12:08:34,873] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  1%|          | 25/3218 [00:39<1:17:19,  1.45s/it]
[2024-05-14 12:08:36,183] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 25
[2024-05-14 12:08:36,184] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  1%|          | 27/3218 [00:41<1:10:58,  1.33s/it]
[2024-05-14 12:08:37,374] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 26
[2024-05-14 12:08:37,374] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:08:37,374] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 12:08:38,436] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 27
[2024-05-14 12:08:38,437] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  1%|          | 29/3218 [00:43<1:00:59,  1.15s/it]
[2024-05-14 12:08:39,327] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 28
[2024-05-14 12:08:39,327] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:08:39,328] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 12:08:40,524] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 29
[2024-05-14 12:08:40,524] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  1%|          | 31/3218 [00:46<1:03:13,  1.19s/it]
{'loss': 0.0, 'learning_rate': 2e-05, 'epoch': 0.01}
[2024-05-14 12:08:41,794] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 30
[2024-05-14 12:08:41,795] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:08:41,795] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 12:08:42,934] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 31
[2024-05-14 12:08:42,934] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  1%|          | 32/3218 [00:47<1:02:48,  1.18s/it]
[2024-05-14 12:08:44,144] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 32
[2024-05-14 12:08:44,145] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  1%|          | 34/3218 [00:49<1:02:38,  1.18s/it]
[2024-05-14 12:08:45,307] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 33
[2024-05-14 12:08:45,307] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:08:45,308] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 12:08:46,439] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 34
[2024-05-14 12:08:46,439] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  1%|          | 35/3218 [00:50<1:01:01,  1.15s/it]
[2024-05-14 12:08:48,428] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 35
[2024-05-14 12:08:48,429] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  1%|          | 36/3218 [00:52<1:16:57,  1.45s/it]
[2024-05-14 12:08:49,929] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 36
[2024-05-14 12:08:49,929] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  1%|          | 38/3218 [00:55<1:08:18,  1.29s/it]
[2024-05-14 12:08:51,050] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 37
[2024-05-14 12:08:51,051] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:08:51,051] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 12:08:52,737] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 38
[2024-05-14 12:08:52,738] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  1%|          | 39/3218 [00:57<1:18:30,  1.48s/it]
[2024-05-14 12:08:54,669] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 39
[2024-05-14 12:08:54,669] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:08:54,670] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25

  1%|          | 40/3218 [00:59<1:25:46,  1.62s/it]
[2024-05-14 12:08:56,162] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 40
[2024-05-14 12:08:56,162] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  1%|▏         | 41/3218 [01:00<1:23:53,  1.58s/it]
[2024-05-14 12:08:58,095] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 41
[2024-05-14 12:08:58,096] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  1%|▏         | 42/3218 [01:02<1:28:54,  1.68s/it]
[2024-05-14 12:09:00,501] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 42
[2024-05-14 12:09:00,502] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  1%|▏         | 43/3218 [01:05<1:41:45,  1.92s/it]
[2024-05-14 12:09:02,591] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 43
[2024-05-14 12:09:02,592] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  1%|▏         | 44/3218 [01:07<1:43:17,  1.95s/it]
[2024-05-14 12:09:04,450] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 44
[2024-05-14 12:09:04,450] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  1%|▏         | 45/3218 [01:08<1:42:17,  1.93s/it]
[2024-05-14 12:09:06,763] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 45
[2024-05-14 12:09:06,764] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  1%|▏         | 46/3218 [01:11<1:47:51,  2.04s/it]
[2024-05-14 12:09:08,625] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 46
[2024-05-14 12:09:08,625] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  1%|▏         | 47/3218 [01:13<1:44:36,  1.98s/it]
[2024-05-14 12:09:10,286] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 47
[2024-05-14 12:09:10,287] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  2%|▏         | 49/3218 [01:16<1:33:31,  1.77s/it]
[2024-05-14 12:09:11,720] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 48
[2024-05-14 12:09:11,720] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  2%|▏         | 50/3218 [01:18<1:33:01,  1.76s/it]
[2024-05-14 12:09:13,562] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 49
[2024-05-14 12:09:13,562] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:09:13,563] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
{'loss': 0.0, 'learning_rate': 2e-05, 'epoch': 0.02}
[2024-05-14 12:09:14,990] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 50
[2024-05-14 12:09:14,990] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  2%|▏         | 51/3218 [01:19<1:27:12,  1.65s/it]
[2024-05-14 12:09:16,178] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 51
[2024-05-14 12:09:16,178] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  2%|▏         | 53/3218 [01:22<1:20:25,  1.52s/it]
[2024-05-14 12:09:17,697] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 52
[2024-05-14 12:09:17,697] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  2%|▏         | 54/3218 [01:23<1:21:16,  1.54s/it]
[2024-05-14 12:09:19,270] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 53
[2024-05-14 12:09:19,271] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:09:19,271] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 12:09:20,740] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 54
[2024-05-14 12:09:20,741] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  2%|▏         | 56/3218 [01:26<1:12:04,  1.37s/it]
[2024-05-14 12:09:21,806] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 55
[2024-05-14 12:09:21,807] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:09:21,807] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 12:09:22,931] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 56
[2024-05-14 12:09:22,931] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  2%|▏         | 57/3218 [01:27<1:08:23,  1.30s/it]
[2024-05-14 12:09:24,506] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 57
[2024-05-14 12:09:24,506] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  2%|▏         | 59/3218 [01:30<1:13:47,  1.40s/it]
[2024-05-14 12:09:25,954] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 58
[2024-05-14 12:09:25,954] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  2%|▏         | 60/3218 [01:31<1:08:55,  1.31s/it]
[2024-05-14 12:09:27,067] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 59
[2024-05-14 12:09:27,067] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:09:27,067] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
{'loss': 0.0, 'learning_rate': 2e-05, 'epoch': 0.02}
[2024-05-14 12:09:28,278] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 60
[2024-05-14 12:09:28,279] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  2%|▏         | 62/3218 [01:34<1:12:25,  1.38s/it]
[2024-05-14 12:09:29,853] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 61
[2024-05-14 12:09:29,853] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  2%|▏         | 63/3218 [01:35<1:16:57,  1.46s/it]
[2024-05-14 12:09:31,505] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 62
[2024-05-14 12:09:31,506] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  2%|▏         | 64/3218 [01:37<1:21:13,  1.55s/it]
[2024-05-14 12:09:33,256] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 63
[2024-05-14 12:09:33,257] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:09:33,257] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 12:09:34,924] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 64
[2024-05-14 12:09:34,925] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  2%|▏         | 65/3218 [01:39<1:25:27,  1.63s/it]
[2024-05-14 12:09:36,992] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 65
[2024-05-14 12:09:36,993] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  2%|▏         | 66/3218 [01:41<1:32:03,  1.75s/it]
[2024-05-14 12:09:38,443] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 66
[2024-05-14 12:09:38,444] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  2%|▏         | 67/3218 [01:42<1:26:54,  1.65s/it]
[2024-05-14 12:09:40,232] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 67
[2024-05-14 12:09:40,233] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  2%|▏         | 69/3218 [01:46<1:27:44,  1.67s/it]
[2024-05-14 12:09:41,849] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 68
[2024-05-14 12:09:41,850] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:09:41,850] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 12:09:43,112] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 69
[2024-05-14 12:09:43,113] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  2%|▏         | 70/3218 [01:47<1:19:00,  1.51s/it]
{'loss': 0.0, 'learning_rate': 2e-05, 'epoch': 0.02}
[2024-05-14 12:09:44,944] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 70
[2024-05-14 12:09:44,945] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  2%|▏         | 71/3218 [01:49<1:24:39,  1.61s/it]
[2024-05-14 12:09:46,708] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 71
[2024-05-14 12:09:46,709] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  2%|▏         | 73/3218 [01:52<1:14:06,  1.41s/it]
[2024-05-14 12:09:47,771] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 72
[2024-05-14 12:09:47,771] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:09:47,771] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 12:09:48,710] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 73
[2024-05-14 12:09:48,710] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  2%|▏         | 74/3218 [01:53<1:09:45,  1.33s/it]
[2024-05-14 12:09:50,900] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 74
[2024-05-14 12:09:50,900] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  2%|▏         | 75/3218 [01:55<1:23:41,  1.60s/it]
[2024-05-14 12:09:52,849] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 75
[2024-05-14 12:09:52,849] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  2%|▏         | 76/3218 [01:57<1:29:12,  1.70s/it]
[2024-05-14 12:09:54,853] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 76
[2024-05-14 12:09:54,853] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  2%|▏         | 77/3218 [01:59<1:33:31,  1.79s/it]
[2024-05-14 12:09:56,299] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 77
[2024-05-14 12:09:56,300] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  2%|▏         | 78/3218 [02:00<1:27:58,  1.68s/it]
[2024-05-14 12:09:58,440] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 78
[2024-05-14 12:09:58,440] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  2%|▏         | 79/3218 [02:02<1:35:42,  1.83s/it]
[2024-05-14 12:10:00,622] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 79
[2024-05-14 12:10:00,623] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:10:00,623] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25

  2%|▏         | 80/3218 [02:05<1:40:45,  1.93s/it]
[2024-05-14 12:10:02,157] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 80
[2024-05-14 12:10:02,158] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  3%|▎         | 82/3218 [02:08<1:32:23,  1.77s/it]
[2024-05-14 12:10:03,796] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 81
[2024-05-14 12:10:03,797] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  3%|▎         | 83/3218 [02:09<1:30:35,  1.73s/it]
[2024-05-14 12:10:05,464] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 82
[2024-05-14 12:10:05,465] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:10:05,465] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 12:10:07,101] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 83
[2024-05-14 12:10:07,102] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  3%|▎         | 84/3218 [02:11<1:28:47,  1.70s/it]
[2024-05-14 12:10:08,680] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 84
[2024-05-14 12:10:08,680] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  3%|▎         | 85/3218 [02:13<1:26:49,  1.66s/it]
[2024-05-14 12:10:10,456] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 85
[2024-05-14 12:10:10,456] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  3%|▎         | 86/3218 [02:14<1:29:43,  1.72s/it]
[2024-05-14 12:10:12,297] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 86
[2024-05-14 12:10:12,298] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  3%|▎         | 88/3218 [02:18<1:26:55,  1.67s/it]
[2024-05-14 12:10:13,797] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 87
[2024-05-14 12:10:13,798] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:10:13,800] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 12:10:15,145] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 88
[2024-05-14 12:10:15,146] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  3%|▎         | 89/3218 [02:19<1:21:13,  1.56s/it]
[2024-05-14 12:10:16,544] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 89
[2024-05-14 12:10:16,545] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:10:16,545] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25

  3%|▎         | 91/3218 [02:22<1:16:42,  1.47s/it]
[2024-05-14 12:10:17,955] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 90
[2024-05-14 12:10:17,955] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  3%|▎         | 92/3218 [02:23<1:18:29,  1.51s/it]
[2024-05-14 12:10:19,515] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 91
[2024-05-14 12:10:19,516] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:10:19,516] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 12:10:20,731] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 92
[2024-05-14 12:10:20,731] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  3%|▎         | 94/3218 [02:26<1:11:15,  1.37s/it]
[2024-05-14 12:10:21,979] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 93
[2024-05-14 12:10:21,979] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  3%|▎         | 95/3218 [02:27<1:09:22,  1.33s/it]
[2024-05-14 12:10:23,247] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 94
[2024-05-14 12:10:23,247] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:10:23,248] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 12:10:25,072] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 95
[2024-05-14 12:10:25,072] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  3%|▎         | 96/3218 [02:29<1:17:46,  1.49s/it]
[2024-05-14 12:10:26,312] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 96
[2024-05-14 12:10:26,313] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  3%|▎         | 98/3218 [02:32<1:12:06,  1.39s/it]
[2024-05-14 12:10:27,647] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 97
[2024-05-14 12:10:27,648] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:10:27,648] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 12:10:28,848] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 98
[2024-05-14 12:10:28,849] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
  3%|▎         | 100/3218 [02:34<1:04:42,  1.25s/it][INFO|trainer.py:3166] 2024-05-14 12:10:30,202 >> ***** Running Evaluation *****
[INFO|trainer.py:3168] 2024-05-14 12:10:30,203 >>   Num examples = 521
[INFO|trainer.py:3171] 2024-05-14 12:10:30,203 >>   Batch size = 8
[2024-05-14 12:10:29,934] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 99
[2024-05-14 12:10:29,935] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:10:29,935] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
{'loss': 0.0, 'learning_rate': 2e-05, 'epoch': 0.03}





100%|██████████| 33/33 [00:11<00:00,  2.33it/s]

{'eval_loss': nan, 'eval_runtime': 12.3618, 'eval_samples_per_second': 42.146, 'eval_steps_per_second': 2.67, 'epoch': 0.03}
[2024-05-14 12:10:43,531] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 100
[2024-05-14 12:10:43,532] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:10:43,532] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 12:10:45,073] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 101
[2024-05-14 12:10:45,074] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  3%|▎         | 102/3218 [02:49<3:24:28,  3.94s/it]
[2024-05-14 12:10:46,148] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 102
[2024-05-14 12:10:46,148] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  3%|▎         | 104/3218 [02:52<2:15:10,  2.60s/it]
[2024-05-14 12:10:47,662] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 103
[2024-05-14 12:10:47,665] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:10:47,665] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 12:10:48,626] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 104
[2024-05-14 12:10:48,627] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  3%|▎         | 106/3218 [02:54<1:38:06,  1.89s/it]
[2024-05-14 12:10:50,063] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 105
[2024-05-14 12:10:50,063] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:10:50,063] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 12:10:51,220] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 106
[2024-05-14 12:10:51,220] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  3%|▎         | 108/3218 [02:56<1:13:54,  1.43s/it]
[2024-05-14 12:10:52,182] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 107
[2024-05-14 12:10:52,183] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  3%|▎         | 109/3218 [02:57<1:14:33,  1.44s/it]
[2024-05-14 12:10:53,443] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 108
[2024-05-14 12:10:53,444] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  3%|▎         | 110/3218 [02:59<1:22:39,  1.60s/it]
[2024-05-14 12:10:55,383] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 109
[2024-05-14 12:10:55,383] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:10:55,386] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25

  3%|▎         | 111/3218 [03:01<1:27:33,  1.69s/it]
[2024-05-14 12:10:57,309] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 110
[2024-05-14 12:10:57,310] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:10:57,311] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 12:10:58,992] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 111
[2024-05-14 12:10:58,993] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  3%|▎         | 112/3218 [03:03<1:26:59,  1.68s/it]
[2024-05-14 12:11:00,442] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 112
[2024-05-14 12:11:00,443] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  4%|▎         | 113/3218 [03:04<1:23:31,  1.61s/it]
[2024-05-14 12:11:02,173] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 113
[2024-05-14 12:11:02,174] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  4%|▎         | 115/3218 [03:08<1:28:07,  1.70s/it]
[2024-05-14 12:11:03,985] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 114
[2024-05-14 12:11:03,986] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  4%|▎         | 116/3218 [03:10<1:26:08,  1.67s/it]
[2024-05-14 12:11:05,584] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 115
[2024-05-14 12:11:05,585] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  4%|▎         | 117/3218 [03:11<1:29:19,  1.73s/it]
[2024-05-14 12:11:07,433] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 116
[2024-05-14 12:11:07,433] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  4%|▎         | 118/3218 [03:13<1:31:07,  1.76s/it]
[2024-05-14 12:11:09,281] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 117
[2024-05-14 12:11:09,281] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:11:09,282] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 12:11:10,834] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 118
[2024-05-14 12:11:10,834] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  4%|▎         | 119/3218 [03:15<1:27:11,  1.69s/it]
[2024-05-14 12:11:12,465] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 119
[2024-05-14 12:11:12,465] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:11:12,466] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25

  4%|▍         | 121/3218 [03:18<1:21:06,  1.57s/it]
[2024-05-14 12:11:13,808] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 120
[2024-05-14 12:11:13,808] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  4%|▍         | 122/3218 [03:19<1:22:26,  1.60s/it]
[2024-05-14 12:11:15,448] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 121
[2024-05-14 12:11:15,449] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:11:15,450] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 12:11:16,825] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 122
[2024-05-14 12:11:16,826] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  4%|▍         | 123/3218 [03:21<1:18:36,  1.52s/it]
[2024-05-14 12:11:18,160] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 123
[2024-05-14 12:11:18,160] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  4%|▍         | 125/3218 [03:23<1:13:51,  1.43s/it]
[2024-05-14 12:11:19,521] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 124
[2024-05-14 12:11:19,522] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:11:19,522] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 12:11:21,272] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 125
[2024-05-14 12:11:21,273] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  4%|▍         | 126/3218 [03:25<1:18:40,  1.53s/it]
[2024-05-14 12:11:22,842] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 126
[2024-05-14 12:11:22,843] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  4%|▍         | 127/3218 [03:27<1:19:25,  1.54s/it]
[2024-05-14 12:11:24,189] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 127
[2024-05-14 12:11:24,190] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  4%|▍         | 129/3218 [03:29<1:13:33,  1.43s/it]
[2024-05-14 12:11:25,475] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 128
[2024-05-14 12:11:25,475] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:11:25,476] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 12:11:26,667] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 129
[2024-05-14 12:11:26,668] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:11:26,668] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25

  4%|▍         | 130/3218 [03:31<1:09:39,  1.35s/it]
[2024-05-14 12:11:28,202] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 130
[2024-05-14 12:11:28,202] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  4%|▍         | 132/3218 [03:34<1:13:00,  1.42s/it]
[2024-05-14 12:11:29,629] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 131
[2024-05-14 12:11:29,629] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:11:29,630] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 12:11:30,810] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 132
[2024-05-14 12:11:30,811] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  4%|▍         | 133/3218 [03:35<1:08:57,  1.34s/it]
[2024-05-14 12:11:32,214] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 133
[2024-05-14 12:11:32,215] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  4%|▍         | 135/3218 [03:37<1:06:25,  1.29s/it]
[2024-05-14 12:11:33,342] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 134
[2024-05-14 12:11:33,343] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:11:33,343] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 12:11:34,691] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 135
[2024-05-14 12:11:34,691] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  4%|▍         | 137/3218 [03:40<1:05:12,  1.27s/it]
[2024-05-14 12:11:35,793] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 136
[2024-05-14 12:11:35,794] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:11:35,876] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 12:11:36,923] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 137
[2024-05-14 12:11:36,923] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  4%|▍         | 139/3218 [03:42<58:07,  1.13s/it]
[2024-05-14 12:11:37,905] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 138
[2024-05-14 12:11:37,906] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:11:37,906] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 12:11:39,164] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 139
[2024-05-14 12:11:39,164] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:11:39,164] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25

  4%|▍         | 140/3218 [03:43<58:58,  1.15s/it]
[2024-05-14 12:11:40,631] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 140
[2024-05-14 12:11:40,632] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  4%|▍         | 142/3218 [03:46<1:08:34,  1.34s/it]
[2024-05-14 12:11:42,118] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 141
[2024-05-14 12:11:42,119] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:11:42,119] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 12:11:43,079] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 142
[2024-05-14 12:11:43,079] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  4%|▍         | 143/3218 [03:47<59:56,  1.17s/it]
[2024-05-14 12:11:44,771] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 143
[2024-05-14 12:11:44,771] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  4%|▍         | 144/3218 [03:49<1:10:57,  1.38s/it]
[2024-05-14 12:11:46,391] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 144
[2024-05-14 12:11:46,391] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  5%|▍         | 145/3218 [03:50<1:14:47,  1.46s/it]
[2024-05-14 12:11:48,588] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 145
[2024-05-14 12:11:48,589] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  5%|▍         | 146/3218 [03:53<1:26:07,  1.68s/it]
[2024-05-14 12:11:50,326] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 146
[2024-05-14 12:11:50,327] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  5%|▍         | 148/3218 [03:56<1:22:37,  1.61s/it]
[2024-05-14 12:11:51,786] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 147
[2024-05-14 12:11:51,787] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:11:51,787] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 12:11:53,173] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 148
[2024-05-14 12:11:53,173] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  5%|▍         | 149/3218 [03:57<1:18:37,  1.54s/it]
[2024-05-14 12:11:54,879] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 149
[2024-05-14 12:11:54,880] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:11:54,880] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25

  5%|▍         | 150/3218 [03:59<1:21:20,  1.59s/it]
[2024-05-14 12:11:56,747] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 150
[2024-05-14 12:11:56,748] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  5%|▍         | 151/3218 [04:01<1:26:02,  1.68s/it]
[2024-05-14 12:11:58,757] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 151
[2024-05-14 12:11:58,757] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  5%|▍         | 152/3218 [04:03<1:31:46,  1.80s/it]
[2024-05-14 12:12:00,438] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 152
[2024-05-14 12:12:00,439] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  5%|▍         | 153/3218 [04:04<1:29:49,  1.76s/it]
[2024-05-14 12:12:02,423] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 153
[2024-05-14 12:12:02,424] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  5%|▍         | 154/3218 [04:06<1:33:24,  1.83s/it]
[2024-05-14 12:12:04,276] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 154
[2024-05-14 12:12:04,277] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  5%|▍         | 156/3218 [04:10<1:29:09,  1.75s/it]
[2024-05-14 12:12:05,865] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 155
[2024-05-14 12:12:05,866] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:12:05,866] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 12:12:07,281] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 156
[2024-05-14 12:12:07,282] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  5%|▍         | 157/3218 [04:11<1:24:15,  1.65s/it]
[2024-05-14 12:12:09,017] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 157
[2024-05-14 12:12:09,018] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  5%|▍         | 158/3218 [04:13<1:26:05,  1.69s/it]
[2024-05-14 12:12:10,606] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 158
[2024-05-14 12:12:10,607] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  5%|▍         | 160/3218 [04:16<1:21:35,  1.60s/it]
[2024-05-14 12:12:12,091] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 159
[2024-05-14 12:12:12,091] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:12:12,093] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25

  5%|▌         | 161/3218 [04:17<1:17:16,  1.52s/it]
[2024-05-14 12:12:13,457] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 160
[2024-05-14 12:12:13,458] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:12:13,458] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 12:12:14,964] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 161
[2024-05-14 12:12:14,965] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  5%|▌         | 162/3218 [04:19<1:17:20,  1.52s/it]
[2024-05-14 12:12:17,047] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 162
[2024-05-14 12:12:17,047] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  5%|▌         | 163/3218 [04:21<1:26:05,  1.69s/it]
[2024-05-14 12:12:18,317] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 163
[2024-05-14 12:12:18,317] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  5%|▌         | 165/3218 [04:24<1:17:08,  1.52s/it]
[2024-05-14 12:12:19,722] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 164
[2024-05-14 12:12:19,723] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:12:19,723] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 12:12:21,081] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 165
[2024-05-14 12:12:21,082] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  5%|▌         | 166/3218 [04:25<1:14:35,  1.47s/it]
[2024-05-14 12:12:22,348] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 166
[2024-05-14 12:12:22,349] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  5%|▌         | 168/3218 [04:28<1:09:44,  1.37s/it]
[2024-05-14 12:12:23,634] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 167
[2024-05-14 12:12:23,635] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:12:23,635] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 12:12:24,980] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 168
[2024-05-14 12:12:24,980] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  5%|▌         | 170/3218 [04:30<1:06:31,  1.31s/it]
[2024-05-14 12:12:26,162] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 169
[2024-05-14 12:12:26,163] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:12:26,163] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25

  5%|▌         | 171/3218 [04:32<1:10:48,  1.39s/it]
[2024-05-14 12:12:27,749] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 170
[2024-05-14 12:12:27,750] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:12:27,752] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 12:12:28,880] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 171
[2024-05-14 12:12:28,881] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  5%|▌         | 172/3218 [04:33<1:06:25,  1.31s/it]
[2024-05-14 12:12:30,397] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 172
[2024-05-14 12:12:30,398] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  5%|▌         | 174/3218 [04:35<1:06:32,  1.31s/it]
[2024-05-14 12:12:31,589] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 173
[2024-05-14 12:12:31,589] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:12:31,589] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 12:12:32,660] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 174
[2024-05-14 12:12:32,660] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  5%|▌         | 176/3218 [04:38<1:07:42,  1.34s/it]
[2024-05-14 12:12:34,211] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 175
[2024-05-14 12:12:34,212] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  6%|▌         | 178/3218 [04:40<57:01,  1.13s/it]
[2024-05-14 12:12:35,543] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 176
[2024-05-14 12:12:35,543] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:12:35,544] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 12:12:36,364] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 177
[2024-05-14 12:12:36,365] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:12:36,365] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 12:12:37,378] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 178
[2024-05-14 12:12:37,378] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  6%|▌         | 179/3218 [04:41<58:01,  1.15s/it]
[2024-05-14 12:12:38,996] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 179
[2024-05-14 12:12:38,996] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:12:38,997] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25

  6%|▌         | 180/3218 [04:43<1:05:43,  1.30s/it]
[2024-05-14 12:12:40,908] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 180
[2024-05-14 12:12:40,908] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  6%|▌         | 181/3218 [04:45<1:14:54,  1.48s/it]
[2024-05-14 12:12:42,739] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 181
[2024-05-14 12:12:42,740] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  6%|▌         | 182/3218 [04:47<1:20:17,  1.59s/it]
[2024-05-14 12:12:45,053] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 182
[2024-05-14 12:12:45,054] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  6%|▌         | 183/3218 [04:49<1:31:24,  1.81s/it]
[2024-05-14 12:12:46,810] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 183
[2024-05-14 12:12:46,811] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  6%|▌         | 184/3218 [04:51<1:30:29,  1.79s/it]
[2024-05-14 12:12:48,317] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 184
[2024-05-14 12:12:48,317] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  6%|▌         | 186/3218 [04:54<1:25:50,  1.70s/it]
[2024-05-14 12:12:49,969] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 185
[2024-05-14 12:12:49,969] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  6%|▌         | 187/3218 [04:56<1:26:13,  1.71s/it]
[2024-05-14 12:12:51,726] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 186
[2024-05-14 12:12:51,726] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  6%|▌         | 188/3218 [04:58<1:29:13,  1.77s/it]
[2024-05-14 12:12:53,656] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 187
[2024-05-14 12:12:53,657] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:12:53,657] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 12:12:55,070] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 188
[2024-05-14 12:12:55,071] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  6%|▌         | 189/3218 [04:59<1:23:42,  1.66s/it]
[2024-05-14 12:12:56,598] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 189
[2024-05-14 12:12:56,599] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:12:56,599] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25

  6%|▌         | 191/3218 [05:02<1:22:18,  1.63s/it]
[2024-05-14 12:12:58,269] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 190
[2024-05-14 12:12:58,270] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  6%|▌         | 192/3218 [05:04<1:19:42,  1.58s/it]
[2024-05-14 12:12:59,733] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 191
[2024-05-14 12:12:59,734] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:12:59,734] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 12:13:01,329] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 192
[2024-05-14 12:13:01,330] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  6%|▌         | 193/3218 [05:05<1:19:52,  1.58s/it]
[2024-05-14 12:13:02,638] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 193
[2024-05-14 12:13:02,639] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  6%|▌         | 195/3218 [05:08<1:12:40,  1.44s/it]
[2024-05-14 12:13:03,931] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 194
[2024-05-14 12:13:03,932] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:13:03,932] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 12:13:05,161] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 195
[2024-05-14 12:13:05,162] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  6%|▌         | 196/3218 [05:09<1:09:05,  1.37s/it]
[2024-05-14 12:13:06,669] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 196
[2024-05-14 12:13:06,670] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  6%|▌         | 198/3218 [05:12<1:11:12,  1.41s/it]
[2024-05-14 12:13:08,080] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 197
[2024-05-14 12:13:08,081] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:13:08,082] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 12:13:09,295] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 198
[2024-05-14 12:13:09,296] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  6%|▌         | 199/3218 [05:13<1:08:13,  1.36s/it]
[2024-05-14 12:13:10,564] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 199
[2024-05-14 12:13:10,565] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:13:10,566] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
  6%|▌         | 200/3218 [05:15<1:06:38,  1.32s/it][INFO|trainer.py:3166] 2024-05-14 12:13:10,796 >> ***** Running Evaluation *****
[INFO|trainer.py:3168] 2024-05-14 12:13:10,796 >>   Num examples = 521
[INFO|trainer.py:3171] 2024-05-14 12:13:10,796 >>   Batch size = 8





 97%|█████████▋| 32/33 [00:11<00:00,  2.78it/s]

  6%|▌         | 200/3218 [05:27<1:06:38,  1.32[INFO|trainer.py:2889] 2024-05-14 12:13:28,825 >> Saving model checkpoint to outputs/instruct_tuning/model/tmp-checkpoint-200
[INFO|configuration_utils.py:483] 2024-05-14 12:13:28,828 >> Configuration saved in outputs/instruct_tuning/model/tmp-checkpoint-200/config.json
[INFO|configuration_utils.py:594] 2024-05-14 12:13:28,830 >> Configuration saved in outputs/instruct_tuning/model/tmp-checkpoint-200/generation_config.json
[INFO|modeling_utils.py:2390] 2024-05-14 12:13:53,653 >> The model is bigger than the maximum size per checkpoint (5GB) and is going to be split in 3 checkpoint shards. You can find where each parameters has been saved in the index located at outputs/instruct_tuning/model/tmp-checkpoint-200/model.safetensors.index.json.
[INFO|tokenization_utils_base.py:2432] 2024-05-14 12:13:53,655 >> tokenizer config file saved in outputs/instruct_tuning/model/tmp-checkpoint-200/tokenizer_config.json
[INFO|tokenization_utils_base.py:2441] 2024-05-14 12:13:53,656 >> Special tokens file saved in outputs/instruct_tuning/model/tmp-checkpoint-200/special_tokens_map.json
[2024-05-14 12:13:54,494] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step200 is about to be saved!
[2024-05-14 12:13:54,516] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: outputs/instruct_tuning/model/tmp-checkpoint-200/global_step200/mp_rank_00_model_states.pt
[2024-05-14 12:13:54,516] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving outputs/instruct_tuning/model/tmp-checkpoint-200/global_step200/mp_rank_00_model_states.pt...
/mounts/work/lyk/anaconda3/envs/torch2/lib/python3.8/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
[2024-05-14 12:15:15,881] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved outputs/instruct_tuning/model/tmp-checkpoint-200/global_step200/mp_rank_00_model_states.pt.
[2024-05-14 12:15:15,885] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step200 is ready now!
[2024-05-14 12:15:17,114] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 200
[2024-05-14 12:15:17,115] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:15:17,115] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
  6%|▌         | 201/3218 [07:21<32:36:19, 38.91s/it]
[2024-05-14 12:15:18,590] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 201
[2024-05-14 12:15:18,591] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  6%|▋         | 203/3218 [07:24<16:31:07, 19.72s/it]
[2024-05-14 12:15:19,777] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 202
[2024-05-14 12:15:19,778] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:15:19,778] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 12:15:21,050] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 203
[2024-05-14 12:15:21,051] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  6%|▋         | 204/3218 [07:25<11:52:46, 14.19s/it]
[2024-05-14 12:15:22,748] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 204
[2024-05-14 12:15:22,749] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  6%|▋         | 205/3218 [07:27<8:45:00, 10.45s/it]
[2024-05-14 12:15:24,059] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 205
[2024-05-14 12:15:24,059] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:15:24,059] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 12:15:25,138] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 206
[2024-05-14 12:15:25,138] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  6%|▋         | 207/3218 [07:29<4:46:46,  5.71s/it]
[2024-05-14 12:15:26,239] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 207
[2024-05-14 12:15:26,239] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:15:26,239] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 12:15:27,244] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 208
[2024-05-14 12:15:27,244] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  6%|▋         | 209/3218 [07:31<2:47:03,  3.33s/it]
[2024-05-14 12:15:28,161] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 209
[2024-05-14 12:15:28,162] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:15:28,162] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
{'loss': 0.0, 'learning_rate': 2e-05, 'epoch': 0.07}
[2024-05-14 12:15:29,619] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 210
[2024-05-14 12:15:29,619] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  7%|▋         | 211/3218 [07:34<1:53:05,  2.26s/it]
[2024-05-14 12:15:31,504] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 211
[2024-05-14 12:15:31,504] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  7%|▋         | 212/3218 [07:35<1:48:40,  2.17s/it]
[2024-05-14 12:15:32,384] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 212
[2024-05-14 12:15:32,384] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  7%|▋         | 214/3218 [07:38<1:22:51,  1.65s/it]
[2024-05-14 12:15:33,696] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 213
[2024-05-14 12:15:33,697] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:15:33,697] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 12:15:35,380] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 214
[2024-05-14 12:15:35,380] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  7%|▋         | 215/3218 [07:39<1:23:32,  1.67s/it]
[2024-05-14 12:15:37,011] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 215
[2024-05-14 12:15:37,011] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  7%|▋         | 216/3218 [07:41<1:22:44,  1.65s/it]
[2024-05-14 12:15:38,447] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 216
[2024-05-14 12:15:38,448] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  7%|▋         | 218/3218 [07:44<1:16:37,  1.53s/it]
[2024-05-14 12:15:39,861] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 217
[2024-05-14 12:15:39,862] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  7%|▋         | 219/3218 [07:46<1:23:54,  1.68s/it]
[2024-05-14 12:15:41,864] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 218
[2024-05-14 12:15:41,864] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:15:41,865] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 12:15:43,468] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 219
[2024-05-14 12:15:43,469] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:15:43,469] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25

  7%|▋         | 220/3218 [07:47<1:22:35,  1.65s/it]
[2024-05-14 12:15:45,243] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 220
[2024-05-14 12:15:45,244] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  7%|▋         | 221/3218 [07:49<1:24:18,  1.69s/it]
[2024-05-14 12:15:46,497] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 221
[2024-05-14 12:15:46,498] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  7%|▋         | 222/3218 [07:50<1:17:25,  1.55s/it]
[2024-05-14 12:15:48,053] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 222
[2024-05-14 12:15:48,054] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:15:48,054] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 12:15:49,450] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 223
[2024-05-14 12:15:49,451] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  7%|▋         | 224/3218 [07:53<1:15:26,  1.51s/it]
[2024-05-14 12:15:51,130] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 224
[2024-05-14 12:15:51,130] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  7%|▋         | 225/3218 [07:55<1:19:55,  1.60s/it]
[2024-05-14 12:15:52,603] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 225
[2024-05-14 12:15:52,604] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  7%|▋         | 226/3218 [07:57<1:15:58,  1.52s/it]
[2024-05-14 12:15:54,454] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 226
[2024-05-14 12:15:54,455] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  7%|▋         | 228/3218 [08:00<1:16:52,  1.54s/it]
[2024-05-14 12:15:55,834] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 227
[2024-05-14 12:15:55,835] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:15:55,835] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 12:15:57,160] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 228
[2024-05-14 12:15:57,160] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  7%|▋         | 229/3218 [08:01<1:13:35,  1.48s/it]
[2024-05-14 12:15:59,036] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 229
[2024-05-14 12:15:59,036] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:15:59,037] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25

  7%|▋         | 230/3218 [08:03<1:20:02,  1.61s/it]
[2024-05-14 12:16:00,457] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 230
[2024-05-14 12:16:00,458] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  7%|▋         | 232/3218 [08:06<1:14:47,  1.50s/it]
[2024-05-14 12:16:01,883] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 231
[2024-05-14 12:16:01,883] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:16:01,883] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 12:16:03,714] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 232
[2024-05-14 12:16:03,715] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  7%|▋         | 233/3218 [08:08<1:20:23,  1.62s/it]
[2024-05-14 12:16:05,681] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 233
[2024-05-14 12:16:05,681] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  7%|▋         | 234/3218 [08:10<1:25:44,  1.72s/it]
[2024-05-14 12:16:07,531] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 234
[2024-05-14 12:16:07,531] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  7%|▋         | 235/3218 [08:12<1:27:51,  1.77s/it]
[2024-05-14 12:16:09,234] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 235
[2024-05-14 12:16:09,234] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  7%|▋         | 236/3218 [08:13<1:25:37,  1.72s/it]
[2024-05-14 12:16:10,533] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 236
[2024-05-14 12:16:10,534] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:16:10,534] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 12:16:11,743] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 237
[2024-05-14 12:16:11,743] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  7%|▋         | 238/3218 [08:16<1:13:13,  1.47s/it]
[2024-05-14 12:16:13,127] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 238
[2024-05-14 12:16:13,127] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  7%|▋         | 239/3218 [08:17<1:12:13,  1.45s/it]
[2024-05-14 12:16:14,257] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 239
[2024-05-14 12:16:14,257] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:16:14,258] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
{'loss': 0.0, 'learning_rate': 2e-05, 'epoch': 0.07}
[2024-05-14 12:16:15,450] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 240
[2024-05-14 12:16:15,450] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  7%|▋         | 241/3218 [08:19<1:04:45,  1.31s/it]
[2024-05-14 12:16:16,616] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 241
[2024-05-14 12:16:16,617] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:16:16,619] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 12:16:17,702] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 242
[2024-05-14 12:16:17,702] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  8%|▊         | 243/3218 [08:22<59:35,  1.20s/it]
[2024-05-14 12:16:18,717] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 243
[2024-05-14 12:16:18,718] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:16:18,718] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 12:16:19,695] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 244
[2024-05-14 12:16:19,695] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  8%|▊         | 245/3218 [08:24<53:47,  1.09s/it]
[2024-05-14 12:16:21,173] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 245
[2024-05-14 12:16:21,173] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  8%|▊         | 246/3218 [08:25<1:00:02,  1.21s/it]
[2024-05-14 12:16:23,099] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 246
[2024-05-14 12:16:23,099] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  8%|▊         | 248/3218 [08:28<1:00:46,  1.23s/it]
[2024-05-14 12:16:24,042] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 247
[2024-05-14 12:16:24,043] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:16:24,043] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 12:16:25,588] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 248
[2024-05-14 12:16:25,589] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  8%|▊         | 249/3218 [08:30<1:08:44,  1.39s/it]
[2024-05-14 12:16:27,296] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 249
[2024-05-14 12:16:27,297] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:16:27,298] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25

  8%|▊         | 250/3218 [08:31<1:13:11,  1.48s/it]
[2024-05-14 12:16:29,396] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 250
[2024-05-14 12:16:29,396] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  8%|▊         | 251/3218 [08:33<1:22:41,  1.67s/it]
[2024-05-14 12:16:31,072] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 251
[2024-05-14 12:16:31,072] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  8%|▊         | 252/3218 [08:35<1:22:33,  1.67s/it]
[2024-05-14 12:16:32,656] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 252
[2024-05-14 12:16:32,656] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  8%|▊         | 254/3218 [08:38<1:17:06,  1.56s/it]
[2024-05-14 12:16:34,056] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 253
[2024-05-14 12:16:34,057] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:16:34,057] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 12:16:35,465] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 254
[2024-05-14 12:16:35,465] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  8%|▊         | 255/3218 [08:39<1:14:49,  1.52s/it]
[2024-05-14 12:16:36,923] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 255
[2024-05-14 12:16:36,924] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  8%|▊         | 256/3218 [08:41<1:13:41,  1.49s/it]
[2024-05-14 12:16:38,478] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 256
[2024-05-14 12:16:38,479] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:16:38,479] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 12:16:39,729] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 257
[2024-05-14 12:16:39,730] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  8%|▊         | 258/3218 [08:44<1:10:27,  1.43s/it]
[2024-05-14 12:16:41,122] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 258
[2024-05-14 12:16:41,122] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  8%|▊         | 259/3218 [08:45<1:10:13,  1.42s/it]
[2024-05-14 12:16:42,575] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 259
[2024-05-14 12:16:42,576] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:16:42,577] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25

  8%|▊         | 260/3218 [08:47<1:10:19,  1.43s/it]
[2024-05-14 12:16:44,175] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 260
[2024-05-14 12:16:44,175] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:16:44,176] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 12:16:45,634] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 261
[2024-05-14 12:16:45,634] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  8%|▊         | 262/3218 [08:50<1:12:55,  1.48s/it]
[2024-05-14 12:16:47,533] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 262
[2024-05-14 12:16:47,533] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  8%|▊         | 263/3218 [08:52<1:19:48,  1.62s/it]
[2024-05-14 12:16:49,160] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 263
[2024-05-14 12:16:49,160] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  8%|▊         | 264/3218 [08:53<1:19:01,  1.61s/it]
[2024-05-14 12:16:50,436] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 264
[2024-05-14 12:16:50,436] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  8%|▊         | 266/3218 [08:56<1:15:59,  1.54s/it]
[2024-05-14 12:16:52,038] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 265
[2024-05-14 12:16:52,039] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:16:52,042] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 12:16:53,218] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 266
[2024-05-14 12:16:53,218] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  8%|▊         | 267/3218 [08:57<1:09:16,  1.41s/it]
[2024-05-14 12:16:54,501] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 267
[2024-05-14 12:16:54,501] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  8%|▊         | 268/3218 [08:58<1:07:57,  1.38s/it]
[2024-05-14 12:16:56,229] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 268
[2024-05-14 12:16:56,230] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:16:56,231] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 12:16:57,425] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 269
[2024-05-14 12:16:57,425] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:16:57,426] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25

  8%|▊         | 270/3218 [09:01<1:08:46,  1.40s/it]
[2024-05-14 12:16:58,853] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 270
[2024-05-14 12:16:58,853] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  8%|▊         | 271/3218 [09:03<1:09:20,  1.41s/it]
[2024-05-14 12:17:00,169] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 271
[2024-05-14 12:17:00,169] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:17:00,169] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 12:17:01,694] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 272
[2024-05-14 12:17:01,694] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  8%|▊         | 273/3218 [09:06<1:09:35,  1.42s/it]
[2024-05-14 12:17:02,938] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 273
[2024-05-14 12:17:02,938] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  9%|▊         | 274/3218 [09:07<1:07:19,  1.37s/it]
[2024-05-14 12:17:04,205] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 274
[2024-05-14 12:17:04,206] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:17:04,207] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 12:17:05,375] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 275
[2024-05-14 12:17:05,375] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  9%|▊         | 276/3218 [09:09<1:03:19,  1.29s/it]
[2024-05-14 12:17:06,937] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 276
[2024-05-14 12:17:06,937] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  9%|▊         | 278/3218 [09:12<1:04:00,  1.31s/it]
[2024-05-14 12:17:08,080] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 277
[2024-05-14 12:17:08,080] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:17:08,081] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 12:17:09,162] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 278
[2024-05-14 12:17:09,162] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  9%|▊         | 279/3218 [09:13<1:00:23,  1.23s/it]
[2024-05-14 12:17:10,152] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 279
[2024-05-14 12:17:10,152] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:17:10,152] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
{'loss': 0.0, 'learning_rate': 2e-05, 'epoch': 0.09}
[2024-05-14 12:17:11,594] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 280
[2024-05-14 12:17:11,594] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  9%|▊         | 281/3218 [09:15<1:00:25,  1.23s/it]
[2024-05-14 12:17:13,503] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 281
[2024-05-14 12:17:13,503] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  9%|▉         | 282/3218 [09:17<1:11:13,  1.46s/it]
[2024-05-14 12:17:14,673] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 282
[2024-05-14 12:17:14,673] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  9%|▉         | 284/3218 [09:20<1:07:26,  1.38s/it]
[2024-05-14 12:17:15,983] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 283
[2024-05-14 12:17:15,984] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:17:15,984] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 12:17:17,500] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 284
[2024-05-14 12:17:17,500] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  9%|▉         | 285/3218 [09:21<1:08:30,  1.40s/it]
[2024-05-14 12:17:19,569] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 285
[2024-05-14 12:17:19,570] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  9%|▉         | 286/3218 [09:24<1:19:13,  1.62s/it]
[2024-05-14 12:17:21,162] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 286
[2024-05-14 12:17:21,163] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  9%|▉         | 287/3218 [09:25<1:18:08,  1.60s/it]
[2024-05-14 12:17:22,566] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 287
[2024-05-14 12:17:22,567] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:17:22,567] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 12:17:23,897] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 288
[2024-05-14 12:17:23,897] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  9%|▉         | 289/3218 [09:28<1:11:56,  1.47s/it]
[2024-05-14 12:17:25,506] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 289
[2024-05-14 12:17:25,507] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:17:25,507] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25

  9%|▉         | 290/3218 [09:29<1:14:00,  1.52s/it]
[2024-05-14 12:17:27,389] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 290
[2024-05-14 12:17:27,389] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  9%|▉         | 291/3218 [09:31<1:19:37,  1.63s/it]
[2024-05-14 12:17:28,977] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 291
[2024-05-14 12:17:28,977] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  9%|▉         | 292/3218 [09:33<1:18:51,  1.62s/it]
[2024-05-14 12:17:30,362] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 292
[2024-05-14 12:17:30,363] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:17:30,365] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 12:17:31,671] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 293
[2024-05-14 12:17:31,672] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  9%|▉         | 294/3218 [09:36<1:12:05,  1.48s/it]
[2024-05-14 12:17:33,179] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 294
[2024-05-14 12:17:33,179] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  9%|▉         | 295/3218 [09:37<1:12:19,  1.48s/it]
[2024-05-14 12:17:34,907] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 295
[2024-05-14 12:17:34,908] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  9%|▉         | 296/3218 [09:39<1:16:22,  1.57s/it]
[2024-05-14 12:17:36,248] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 296
[2024-05-14 12:17:36,249] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:17:36,249] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 12:17:37,583] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 297
[2024-05-14 12:17:37,583] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  9%|▉         | 298/3218 [09:41<1:10:00,  1.44s/it]
[2024-05-14 12:17:39,390] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 298
[2024-05-14 12:17:39,391] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  9%|▉         | 299/3218 [09:43<1:15:19,  1.55s/it]
[2024-05-14 12:17:40,498] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 299
[2024-05-14 12:17:40,498] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:17:40,499] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
  9%|▉         | 300/3218 [09:44<1:08:30,  1.41s/it][INFO|trainer.py:3166] 2024-05-14 12:17:40,654 >> ***** Running Evaluation *****
[INFO|trainer.py:3168] 2024-05-14 12:17:40,654 >>   Num examples = 521
[INFO|trainer.py:3171] 2024-05-14 12:17:40,654 >>   Batch size = 8





 97%|█████████▋| 32/33 [00:11<00:00,  2.78it/s]

{'eval_loss': nan, 'eval_runtime': 12.4038, 'eval_samples_per_second': 42.003, 'eval_steps_per_second': 2.66, 'epoch': 0.09}
[2024-05-14 12:17:54,363] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 300
[2024-05-14 12:17:54,363] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:17:54,365] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 12:17:55,784] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 301
[2024-05-14 12:17:55,784] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  9%|▉         | 302/3218 [10:00<3:16:06,  4.04s/it]
[2024-05-14 12:17:57,033] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 302
[2024-05-14 12:17:57,034] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  9%|▉         | 303/3218 [10:01<2:34:44,  3.18s/it]
[2024-05-14 12:17:58,461] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 303
[2024-05-14 12:17:58,462] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:17:58,462] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 12:17:59,707] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 304
[2024-05-14 12:17:59,707] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  9%|▉         | 305/3218 [10:04<1:48:53,  2.24s/it]
[2024-05-14 12:18:01,029] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 305
[2024-05-14 12:18:01,030] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 10%|▉         | 306/3218 [10:05<1:35:08,  1.96s/it]
[2024-05-14 12:18:02,724] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 306
[2024-05-14 12:18:02,725] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 10%|▉         | 307/3218 [10:07<1:31:56,  1.90s/it]
[2024-05-14 12:18:04,185] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 307
[2024-05-14 12:18:04,186] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:18:04,186] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 12:18:05,414] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 308
[2024-05-14 12:18:05,415] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 10%|▉         | 309/3218 [10:09<1:19:08,  1.63s/it]
[2024-05-14 12:18:06,755] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 309
[2024-05-14 12:18:06,756] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:18:06,756] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
{'loss': 0.0, 'learning_rate': 2e-05, 'epoch': 0.1}
[2024-05-14 12:18:07,985] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 310
[2024-05-14 12:18:07,985] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 10%|▉         | 311/3218 [10:12<1:09:11,  1.43s/it]
[2024-05-14 12:18:09,293] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 311
[2024-05-14 12:18:09,293] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 10%|▉         | 312/3218 [10:13<1:06:52,  1.38s/it]
[2024-05-14 12:18:10,376] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 312
[2024-05-14 12:18:10,377] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:18:10,377] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 12:18:11,624] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 313
[2024-05-14 12:18:11,624] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 10%|▉         | 314/3218 [10:16<1:01:46,  1.28s/it]
[2024-05-14 12:18:12,493] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 314
[2024-05-14 12:18:12,494] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 10%|▉         | 315/3218 [10:16<56:12,  1.16s/it]
[2024-05-14 12:18:14,797] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 315
[2024-05-14 12:18:14,798] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 10%|▉         | 316/3218 [10:19<1:13:01,  1.51s/it]
[2024-05-14 12:18:17,161] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 316
[2024-05-14 12:18:17,161] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 10%|▉         | 318/3218 [10:22<1:10:04,  1.45s/it]
[2024-05-14 12:18:18,076] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 317
[2024-05-14 12:18:18,077] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:18:18,077] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 12:18:19,224] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 318
[2024-05-14 12:18:19,225] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 10%|▉         | 319/3218 [10:23<1:08:26,  1.42s/it]
[2024-05-14 12:18:21,038] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 319
[2024-05-14 12:18:21,039] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:18:21,039] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25

 10%|▉         | 320/3218 [10:25<1:15:30,  1.56s/it]
[2024-05-14 12:18:22,604] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 320
[2024-05-14 12:18:22,604] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 10%|█         | 322/3218 [10:28<1:13:09,  1.52s/it]
[2024-05-14 12:18:24,092] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 321
[2024-05-14 12:18:24,093] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:18:24,093] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 12:18:25,901] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 322
[2024-05-14 12:18:25,901] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 10%|█         | 323/3218 [10:30<1:18:13,  1.62s/it]
[2024-05-14 12:18:27,421] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 323
[2024-05-14 12:18:27,422] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 10%|█         | 324/3218 [10:31<1:16:23,  1.58s/it]
[2024-05-14 12:18:28,871] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 324
[2024-05-14 12:18:28,871] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 10%|█         | 325/3218 [10:33<1:14:18,  1.54s/it]
[2024-05-14 12:18:30,541] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 325
[2024-05-14 12:18:30,542] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 10%|█         | 327/3218 [10:36<1:15:55,  1.58s/it]
[2024-05-14 12:18:32,143] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 326
[2024-05-14 12:18:32,143] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:18:32,145] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 12:18:33,618] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 327
[2024-05-14 12:18:33,618] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 10%|█         | 328/3218 [10:38<1:14:29,  1.55s/it]
[2024-05-14 12:18:35,156] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 328
[2024-05-14 12:18:35,157] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 10%|█         | 329/3218 [10:39<1:14:40,  1.55s/it]
[2024-05-14 12:18:36,606] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 329
[2024-05-14 12:18:36,607] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:18:36,607] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25

 10%|█         | 331/3218 [10:42<1:14:37,  1.55s/it]
[2024-05-14 12:18:38,132] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 330
[2024-05-14 12:18:38,133] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:18:38,135] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 12:18:39,642] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 331
[2024-05-14 12:18:39,642] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 10%|█         | 332/3218 [10:44<1:12:48,  1.51s/it]
[2024-05-14 12:18:41,051] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 332
[2024-05-14 12:18:41,052] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 10%|█         | 333/3218 [10:45<1:11:23,  1.48s/it]
[2024-05-14 12:18:42,488] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 333
[2024-05-14 12:18:42,489] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:18:42,489] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 12:18:43,897] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 334
[2024-05-14 12:18:43,898] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 10%|█         | 335/3218 [10:48<1:09:53,  1.45s/it]
[2024-05-14 12:18:45,249] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 335
[2024-05-14 12:18:45,250] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 10%|█         | 336/3218 [10:49<1:08:05,  1.42s/it]
[2024-05-14 12:18:46,415] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 336
[2024-05-14 12:18:46,415] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:18:46,416] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 12:18:47,745] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 337
[2024-05-14 12:18:47,745] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 11%|█         | 338/3218 [10:52<1:04:24,  1.34s/it]
[2024-05-14 12:18:48,892] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 338
[2024-05-14 12:18:48,893] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 11%|█         | 339/3218 [10:53<1:00:57,  1.27s/it]
[2024-05-14 12:18:50,332] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 339
[2024-05-14 12:18:50,333] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:18:50,333] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
{'loss': 0.0, 'learning_rate': 2e-05, 'epoch': 0.11}
[2024-05-14 12:18:51,593] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 340
[2024-05-14 12:18:51,593] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 11%|█         | 341/3218 [10:55<1:02:26,  1.30s/it]
[2024-05-14 12:18:52,544] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 341
[2024-05-14 12:18:52,544] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:18:52,544] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 12:18:53,667] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 342
[2024-05-14 12:18:53,668] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 11%|█         | 343/3218 [10:58<56:31,  1.18s/it]
[2024-05-14 12:18:54,953] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 343
[2024-05-14 12:18:54,954] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:18:54,954] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 12:18:55,974] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 344
[2024-05-14 12:18:55,975] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 11%|█         | 345/3218 [11:00<55:09,  1.15s/it]
[2024-05-14 12:18:57,066] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 345
[2024-05-14 12:18:57,066] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 11%|█         | 347/3218 [11:02<54:37,  1.14s/it]
[2024-05-14 12:18:58,233] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 346
[2024-05-14 12:18:58,234] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:18:58,234] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 12:18:59,265] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 347
[2024-05-14 12:18:59,266] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 11%|█         | 349/3218 [11:04<51:30,  1.08s/it]
[2024-05-14 12:19:00,265] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 348
[2024-05-14 12:19:00,265] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:19:00,266] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 12:19:01,239] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 349
[2024-05-14 12:19:01,239] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:19:01,239] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25

 11%|█         | 350/3218 [11:05<49:12,  1.03s/it]
[2024-05-14 12:19:03,234] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 350
[2024-05-14 12:19:03,235] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 11%|█         | 351/3218 [11:07<1:04:09,  1.34s/it]
[2024-05-14 12:19:05,144] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 351
[2024-05-14 12:19:05,144] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:19:05,144] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 12:19:06,096] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 352
[2024-05-14 12:19:06,097] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 11%|█         | 353/3218 [11:10<1:01:26,  1.29s/it]
[2024-05-14 12:19:07,301] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 353
[2024-05-14 12:19:07,302] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 11%|█         | 354/3218 [11:11<1:02:21,  1.31s/it]
[2024-05-14 12:19:09,142] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 354
[2024-05-14 12:19:09,143] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 11%|█         | 355/3218 [11:13<1:10:45,  1.48s/it]
[2024-05-14 12:19:10,614] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 355
[2024-05-14 12:19:10,615] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 11%|█         | 357/3218 [11:16<1:12:24,  1.52s/it]
[2024-05-14 12:19:12,223] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 356
[2024-05-14 12:19:12,224] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:19:12,228] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 12:19:13,889] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 357
[2024-05-14 12:19:13,889] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 11%|█         | 358/3218 [11:18<1:14:51,  1.57s/it]
[2024-05-14 12:19:15,569] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 358
[2024-05-14 12:19:15,570] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 11%|█         | 359/3218 [11:19<1:15:49,  1.59s/it]
[2024-05-14 12:19:17,126] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 359
[2024-05-14 12:19:17,126] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:19:17,133] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25

 11%|█         | 360/3218 [11:21<1:15:13,  1.58s/it]
[2024-05-14 12:19:18,811] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 360
[2024-05-14 12:19:18,812] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 11%|█         | 361/3218 [11:23<1:17:05,  1.62s/it]
[2024-05-14 12:19:20,896] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 361
[2024-05-14 12:19:20,897] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 11%|█▏        | 363/3218 [11:26<1:16:22,  1.61s/it]
[2024-05-14 12:19:22,178] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 362
[2024-05-14 12:19:22,179] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:19:22,179] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 12:19:23,791] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 363
[2024-05-14 12:19:23,792] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 11%|█▏        | 364/3218 [11:28<1:16:24,  1.61s/it]
[2024-05-14 12:19:25,108] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 364
[2024-05-14 12:19:25,109] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 11%|█▏        | 365/3218 [11:29<1:12:31,  1.53s/it]
[2024-05-14 12:19:26,607] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 365
[2024-05-14 12:19:26,607] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:19:26,608] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 12:19:27,999] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 366
[2024-05-14 12:19:28,000] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 11%|█▏        | 367/3218 [11:32<1:09:45,  1.47s/it]
[2024-05-14 12:19:29,618] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 367
[2024-05-14 12:19:29,619] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 11%|█▏        | 368/3218 [11:34<1:12:49,  1.53s/it]
[2024-05-14 12:19:31,532] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 368
[2024-05-14 12:19:31,532] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 11%|█▏        | 369/3218 [11:35<1:17:27,  1.63s/it]
[2024-05-14 12:19:33,019] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 369
[2024-05-14 12:19:33,019] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:19:33,020] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25

 11%|█▏        | 370/3218 [11:37<1:15:08,  1.58s/it]
[2024-05-14 12:19:34,483] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 370
[2024-05-14 12:19:34,484] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:19:34,484] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 12:19:35,931] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 371
[2024-05-14 12:19:35,931] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 12%|█▏        | 372/3218 [11:40<1:11:59,  1.52s/it]
[2024-05-14 12:19:37,380] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 372
[2024-05-14 12:19:37,380] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 12%|█▏        | 373/3218 [11:41<1:11:31,  1.51s/it]
[2024-05-14 12:19:38,654] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 373
[2024-05-14 12:19:38,655] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:19:38,655] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 12:19:40,050] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 374
[2024-05-14 12:19:40,050] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 12%|█▏        | 375/3218 [11:44<1:07:30,  1.42s/it]
[2024-05-14 12:19:41,476] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 375
[2024-05-14 12:19:41,477] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 12%|█▏        | 376/3218 [11:45<1:07:27,  1.42s/it]
[2024-05-14 12:19:43,023] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 376
[2024-05-14 12:19:43,024] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:19:43,024] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 12:19:44,120] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 377
[2024-05-14 12:19:44,121] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 12%|█▏        | 378/3218 [11:48<1:03:49,  1.35s/it]
[2024-05-14 12:19:45,185] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 378
[2024-05-14 12:19:45,186] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 12%|█▏        | 380/3218 [11:50<58:13,  1.23s/it]
[2024-05-14 12:19:46,332] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 379
[2024-05-14 12:19:46,333] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:19:46,333] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
{'loss': 0.0, 'learning_rate': 2e-05, 'epoch': 0.12}
[2024-05-14 12:19:47,330] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 380
[2024-05-14 12:19:47,331] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 12%|█▏        | 382/3218 [11:52<53:01,  1.12s/it]
[2024-05-14 12:19:48,363] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 381
[2024-05-14 12:19:48,364] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:19:48,364] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 12:19:49,495] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 382
[2024-05-14 12:19:49,495] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 12%|█▏        | 383/3218 [11:53<52:49,  1.12s/it]
[2024-05-14 12:19:50,588] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 383
[2024-05-14 12:19:50,589] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:19:50,589] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 12:19:51,605] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 384
[2024-05-14 12:19:51,606] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 12%|█▏        | 385/3218 [11:55<50:33,  1.07s/it]
[2024-05-14 12:19:53,075] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 385
[2024-05-14 12:19:53,076] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 12%|█▏        | 386/3218 [11:57<56:43,  1.20s/it]
[2024-05-14 12:19:54,934] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 386
[2024-05-14 12:19:54,935] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:19:54,935] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 12:19:56,069] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 387
[2024-05-14 12:19:56,070] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 12%|█▏        | 388/3218 [12:00<59:52,  1.27s/it]
[2024-05-14 12:19:57,742] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 388
[2024-05-14 12:19:57,743] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 12%|█▏        | 389/3218 [12:02<1:08:33,  1.45s/it]
[2024-05-14 12:19:59,243] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 389
[2024-05-14 12:19:59,243] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:19:59,244] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25

 12%|█▏        | 390/3218 [12:03<1:08:29,  1.45s/it]
[2024-05-14 12:20:00,930] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 390
[2024-05-14 12:20:00,930] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 12%|█▏        | 392/3218 [12:06<1:10:35,  1.50s/it]
[2024-05-14 12:20:02,365] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 391
[2024-05-14 12:20:02,366] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:20:02,367] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 12:20:03,951] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 392
[2024-05-14 12:20:03,952] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 12%|█▏        | 393/3218 [12:08<1:12:08,  1.53s/it]
[2024-05-14 12:20:05,623] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 393
[2024-05-14 12:20:05,624] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 12%|█▏        | 394/3218 [12:10<1:13:26,  1.56s/it]
[2024-05-14 12:20:07,464] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 394
[2024-05-14 12:20:07,465] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 12%|█▏        | 395/3218 [12:11<1:17:11,  1.64s/it]
[2024-05-14 12:20:09,009] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 395
[2024-05-14 12:20:09,010] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 12%|█▏        | 396/3218 [12:13<1:15:57,  1.61s/it]
[2024-05-14 12:20:10,885] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 396
[2024-05-14 12:20:10,886] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 12%|█▏        | 398/3218 [12:16<1:15:24,  1.60s/it]
[2024-05-14 12:20:12,327] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 397
[2024-05-14 12:20:12,328] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:20:12,328] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 12:20:14,126] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 398
[2024-05-14 12:20:14,126] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 12%|█▏        | 399/3218 [12:18<1:19:08,  1.68s/it]
[2024-05-14 12:20:15,843] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 399
[2024-05-14 12:20:15,844] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:20:15,844] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
 12%|█▏        | 400/3218 [12:20<1:19:04,  1.68s/it][INFO|trainer.py:3166] 2024-05-14 12:20:16,017 >> ***** Running Evaluation *****
[INFO|trainer.py:3168] 2024-05-14 12:20:16,017 >>   Num examples = 521
[INFO|trainer.py:3171] 2024-05-14 12:20:16,017 >>   Batch size = 8






 88%|████████▊ | 29/33 [00:10<00:01,  2.81it/s]
 12%|█▏        | 400/3218 [12:32<1:19:04,  1.68[INFO|trainer.py:2889] 2024-05-14 12:20:33,771 >> Saving model checkpoint to outputs/instruct_tuning/model/tmp-checkpoint-400
[INFO|configuration_utils.py:483] 2024-05-14 12:20:33,774 >> Configuration saved in outputs/instruct_tuning/model/tmp-checkpoint-400/config.json
[INFO|configuration_utils.py:594] 2024-05-14 12:20:33,775 >> Configuration saved in outputs/instruct_tuning/model/tmp-checkpoint-400/generation_config.json
[2024-05-14 12:21:02,201] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step400 is about to be saved!
[2024-05-14 12:21:02,207] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: outputs/instruct_tuning/model/tmp-checkpoint-400/global_step400/mp_rank_00_model_states.pt
[2024-05-14 12:21:02,208] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving outputs/instruct_tuning/model/tmp-checkpoint-400/global_step400/mp_rank_00_model_states.pt...
[INFO|modeling_utils.py:2390] 2024-05-14 12:21:01,602 >> The model is bigger than the maximum size per checkpoint (5GB) and is going to be split in 3 checkpoint shards. You can find where each parameters has been saved in the index located at outputs/instruct_tuning/model/tmp-checkpoint-400/model.safetensors.index.json.
[INFO|tokenization_utils_base.py:2432] 2024-05-14 12:21:01,604 >> tokenizer config file saved in outputs/instruct_tuning/model/tmp-checkpoint-400/tokenizer_config.json
[INFO|tokenization_utils_base.py:2441] 2024-05-14 12:21:01,604 >> Special tokens file saved in outputs/instruct_tuning/model/tmp-checkpoint-400/special_tokens_map.json
/mounts/work/lyk/anaconda3/envs/torch2/lib/python3.8/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
[2024-05-14 12:22:23,016] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved outputs/instruct_tuning/model/tmp-checkpoint-400/global_step400/mp_rank_00_model_states.pt.
[2024-05-14 12:22:23,019] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step400 is ready now!
[2024-05-14 12:22:24,535] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 400
[2024-05-14 12:22:24,535] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:22:24,536] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 12:22:26,205] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 401
[2024-05-14 12:22:26,205] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:22:26,205] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
 12%|█▏        | 402/3218 [14:30<22:10:59, 28.36s/it]
[2024-05-14 12:22:27,640] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 402
[2024-05-14 12:22:27,640] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 13%|█▎        | 403/3218 [14:32<15:50:46, 20.27s/it]
[2024-05-14 12:22:29,048] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 403
[2024-05-14 12:22:29,048] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 13%|█▎        | 405/3218 [14:34<8:20:25, 10.67s/it]
[2024-05-14 12:22:30,537] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 404
[2024-05-14 12:22:30,538] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:22:30,538] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 12:22:31,917] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 405
[2024-05-14 12:22:31,918] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 13%|█▎        | 406/3218 [14:36<6:10:20,  7.90s/it]
[2024-05-14 12:22:33,362] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 406
[2024-05-14 12:22:33,363] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 13%|█▎        | 407/3218 [14:37<4:38:40,  5.95s/it]
[2024-05-14 12:22:34,804] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 407
[2024-05-14 12:22:34,804] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:22:34,805] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 12:22:36,022] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 408
[2024-05-14 12:22:36,023] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 13%|█▎        | 409/3218 [14:40<2:47:18,  3.57s/it]
[2024-05-14 12:22:37,267] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 409
[2024-05-14 12:22:37,268] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:22:37,268] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25

 13%|█▎        | 410/3218 [14:41<2:15:05,  2.89s/it]
[2024-05-14 12:22:38,732] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 410
[2024-05-14 12:22:38,732] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:22:38,733] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 12:22:40,040] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 411
[2024-05-14 12:22:40,040] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 13%|█▎        | 412/3218 [14:44<1:38:22,  2.10s/it]
[2024-05-14 12:22:41,252] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 412
[2024-05-14 12:22:41,252] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:22:41,254] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 12:22:42,353] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 413
[2024-05-14 12:22:42,353] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 13%|█▎        | 414/3218 [14:46<1:16:25,  1.64s/it]
[2024-05-14 12:22:43,550] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 414
[2024-05-14 12:22:43,550] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 13%|█▎        | 415/3218 [14:47<1:09:25,  1.49s/it]
[2024-05-14 12:22:44,982] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 415
[2024-05-14 12:22:44,982] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 13%|█▎        | 416/3218 [14:49<1:09:06,  1.48s/it]
[2024-05-14 12:22:46,639] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 416
[2024-05-14 12:22:46,639] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:22:46,639] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 12:22:48,038] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 417
[2024-05-14 12:22:48,039] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 13%|█▎        | 418/3218 [14:52<1:09:15,  1.48s/it]
[2024-05-14 12:22:49,164] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 418
[2024-05-14 12:22:49,165] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:22:49,166] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 12:22:50,274] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 419
[2024-05-14 12:22:50,275] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:22:50,275] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25

 13%|█▎        | 420/3218 [14:54<59:54,  1.28s/it]
[2024-05-14 12:22:51,537] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 420
[2024-05-14 12:22:51,538] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 13%|█▎        | 421/3218 [14:55<59:32,  1.28s/it]
[2024-05-14 12:22:53,626] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 421
[2024-05-14 12:22:53,627] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 13%|█▎        | 423/3218 [14:58<1:00:15,  1.29s/it]
[2024-05-14 12:22:54,563] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 422
[2024-05-14 12:22:54,563] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:22:54,564] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 12:22:55,784] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 423
[2024-05-14 12:22:55,784] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 13%|█▎        | 424/3218 [15:00<1:01:51,  1.33s/it]
[2024-05-14 12:22:57,394] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 424
[2024-05-14 12:22:57,395] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 13%|█▎        | 425/3218 [15:01<1:06:03,  1.42s/it]
[2024-05-14 12:22:58,984] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 425
[2024-05-14 12:22:58,984] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 13%|█▎        | 426/3218 [15:03<1:09:27,  1.49s/it]
[2024-05-14 12:23:00,584] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 426
[2024-05-14 12:23:00,585] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:23:00,585] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 12:23:02,275] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 427
[2024-05-14 12:23:02,275] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 13%|█▎        | 428/3218 [15:06<1:13:13,  1.57s/it]
[2024-05-14 12:23:03,879] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 428
[2024-05-14 12:23:03,880] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 13%|█▎        | 429/3218 [15:08<1:13:18,  1.58s/it]
[2024-05-14 12:23:05,425] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 429
[2024-05-14 12:23:05,425] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:23:05,425] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25

 13%|█▎        | 430/3218 [15:09<1:13:04,  1.57s/it]
[2024-05-14 12:23:06,979] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 430
[2024-05-14 12:23:06,980] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:23:06,980] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 12:23:08,407] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 431
[2024-05-14 12:23:08,407] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 13%|█▎        | 432/3218 [15:12<1:11:38,  1.54s/it]
[2024-05-14 12:23:09,804] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 432
[2024-05-14 12:23:09,804] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 13%|█▎        | 433/3218 [15:14<1:07:28,  1.45s/it]
[2024-05-14 12:23:11,063] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 433
[2024-05-14 12:23:11,063] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 13%|█▎        | 434/3218 [15:15<1:04:48,  1.40s/it]
[2024-05-14 12:23:12,895] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 434
[2024-05-14 12:23:12,896] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:23:12,896] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 12:23:14,307] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 435
[2024-05-14 12:23:14,308] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 14%|█▎        | 436/3218 [15:18<1:10:36,  1.52s/it]
[2024-05-14 12:23:15,999] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 436
[2024-05-14 12:23:16,000] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 14%|█▎        | 437/3218 [15:20<1:11:56,  1.55s/it]
[2024-05-14 12:23:17,706] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 437
[2024-05-14 12:23:17,707] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 14%|█▎        | 438/3218 [15:22<1:13:58,  1.60s/it]
[2024-05-14 12:23:19,000] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 438
[2024-05-14 12:23:19,000] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 14%|█▎        | 440/3218 [15:25<1:10:57,  1.53s/it]
[2024-05-14 12:23:20,569] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 439
[2024-05-14 12:23:20,570] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:23:20,570] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
{'loss': 0.0, 'learning_rate': 2e-05, 'epoch': 0.14}
[2024-05-14 12:23:22,026] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 440
[2024-05-14 12:23:22,027] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 14%|█▎        | 441/3218 [15:26<1:09:43,  1.51s/it]
[2024-05-14 12:23:23,468] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 441
[2024-05-14 12:23:23,468] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 14%|█▎        | 442/3218 [15:27<1:08:58,  1.49s/it]
[2024-05-14 12:23:24,727] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 442
[2024-05-14 12:23:24,727] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:23:24,728] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 12:23:26,136] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 443
[2024-05-14 12:23:26,136] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 14%|█▍        | 444/3218 [15:30<1:04:48,  1.40s/it]
[2024-05-14 12:23:27,261] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 444
[2024-05-14 12:23:27,262] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 14%|█▍        | 445/3218 [15:31<1:01:09,  1.32s/it]
[2024-05-14 12:23:28,736] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 445
[2024-05-14 12:23:28,736] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:23:28,737] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 12:23:29,953] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 446
[2024-05-14 12:23:29,953] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 14%|█▍        | 447/3218 [15:34<1:00:22,  1.31s/it]
[2024-05-14 12:23:31,241] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 447
[2024-05-14 12:23:31,241] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:23:31,241] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 12:23:32,361] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 448
[2024-05-14 12:23:32,361] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 14%|█▍        | 449/3218 [15:36<58:16,  1.26s/it]
[2024-05-14 12:23:33,754] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 449
[2024-05-14 12:23:33,754] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:23:33,754] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25

 14%|█▍        | 450/3218 [15:38<1:00:00,  1.30s/it]
[2024-05-14 12:23:35,072] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 450
[2024-05-14 12:23:35,072] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:23:35,072] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 12:23:36,344] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 451
[2024-05-14 12:23:36,344] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 14%|█▍        | 452/3218 [15:40<1:00:16,  1.31s/it]
[2024-05-14 12:23:37,506] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 452
[2024-05-14 12:23:37,507] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 14%|█▍        | 453/3218 [15:41<57:11,  1.24s/it]
[2024-05-14 12:23:38,889] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 453
[2024-05-14 12:23:38,890] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:23:38,890] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 12:23:40,349] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 454
[2024-05-14 12:23:40,349] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 14%|█▍        | 455/3218 [15:44<1:01:46,  1.34s/it]
[2024-05-14 12:23:41,854] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 455
[2024-05-14 12:23:41,854] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 14%|█▍        | 456/3218 [15:46<1:03:35,  1.38s/it]
[2024-05-14 12:23:43,134] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 456
[2024-05-14 12:23:43,134] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:23:43,135] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 12:23:44,013] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 457
[2024-05-14 12:23:44,013] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 14%|█▍        | 458/3218 [15:48<53:17,  1.16s/it]
[2024-05-14 12:23:45,318] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 458
[2024-05-14 12:23:45,319] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 14%|█▍        | 459/3218 [15:49<58:17,  1.27s/it]
[2024-05-14 12:23:47,522] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 459
[2024-05-14 12:23:47,522] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:23:47,523] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25

 14%|█▍        | 460/3218 [15:51<1:11:18,  1.55s/it]
[2024-05-14 12:23:49,231] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 460
[2024-05-14 12:23:49,231] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 14%|█▍        | 461/3218 [15:53<1:13:38,  1.60s/it]
[2024-05-14 12:23:50,883] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 461
[2024-05-14 12:23:50,883] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 14%|█▍        | 462/3218 [15:55<1:13:30,  1.60s/it]
[2024-05-14 12:23:52,810] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 462
[2024-05-14 12:23:52,811] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 14%|█▍        | 463/3218 [15:57<1:17:47,  1.69s/it]
[2024-05-14 12:23:55,100] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 463
[2024-05-14 12:23:55,100] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 14%|█▍        | 464/3218 [15:59<1:26:43,  1.89s/it]
[2024-05-14 12:23:56,867] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 464
[2024-05-14 12:23:56,867] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:23:56,868] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 12:23:58,419] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 465
[2024-05-14 12:23:58,420] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 14%|█▍        | 466/3218 [16:02<1:21:01,  1.77s/it]
[2024-05-14 12:23:59,846] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 466
[2024-05-14 12:23:59,847] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 15%|█▍        | 467/3218 [16:04<1:15:58,  1.66s/it]
[2024-05-14 12:24:01,392] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 467
[2024-05-14 12:24:01,392] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 15%|█▍        | 468/3218 [16:05<1:13:39,  1.61s/it]
[2024-05-14 12:24:03,059] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 468
[2024-05-14 12:24:03,059] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 15%|█▍        | 469/3218 [16:07<1:14:29,  1.63s/it]
[2024-05-14 12:24:04,735] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 469
[2024-05-14 12:24:04,736] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:24:04,736] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
{'loss': 0.0, 'learning_rate': 2e-05, 'epoch': 0.15}
[2024-05-14 12:24:06,407] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 470
[2024-05-14 12:24:06,407] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 15%|█▍        | 471/3218 [16:10<1:15:42,  1.65s/it]
[2024-05-14 12:24:08,332] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 471
[2024-05-14 12:24:08,333] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 15%|█▍        | 472/3218 [16:12<1:19:56,  1.75s/it]
[2024-05-14 12:24:09,816] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 472
[2024-05-14 12:24:09,816] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 15%|█▍        | 473/3218 [16:14<1:15:49,  1.66s/it]
[2024-05-14 12:24:11,141] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 473
[2024-05-14 12:24:11,141] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 15%|█▍        | 474/3218 [16:15<1:11:04,  1.55s/it]
[2024-05-14 12:24:12,801] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 474
[2024-05-14 12:24:12,801] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:24:12,802] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 12:24:14,190] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 475
[2024-05-14 12:24:14,190] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 15%|█▍        | 476/3218 [16:18<1:10:25,  1.54s/it]
[2024-05-14 12:24:15,545] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 476
[2024-05-14 12:24:15,545] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 15%|█▍        | 477/3218 [16:19<1:07:13,  1.47s/it]
[2024-05-14 12:24:16,988] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 477
[2024-05-14 12:24:16,989] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:24:16,989] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 12:24:18,379] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 478
[2024-05-14 12:24:18,379] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 15%|█▍        | 479/3218 [16:22<1:05:17,  1.43s/it]
[2024-05-14 12:24:19,607] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 479
[2024-05-14 12:24:19,608] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:24:19,608] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25

 15%|█▍        | 480/3218 [16:24<1:03:08,  1.38s/it]
[2024-05-14 12:24:21,121] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 480
[2024-05-14 12:24:21,121] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:24:21,122] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 12:24:22,330] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 481
[2024-05-14 12:24:22,331] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 15%|█▍        | 482/3218 [16:26<1:01:07,  1.34s/it]
[2024-05-14 12:24:23,614] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 482
[2024-05-14 12:24:23,614] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 15%|█▌        | 483/3218 [16:28<1:00:31,  1.33s/it]
[2024-05-14 12:24:24,820] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 483
[2024-05-14 12:24:24,820] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:24:24,821] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 12:24:26,022] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 484
[2024-05-14 12:24:26,022] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 15%|█▌        | 485/3218 [16:30<57:46,  1.27s/it]
[2024-05-14 12:24:27,255] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 485
[2024-05-14 12:24:27,256] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:24:27,256] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 12:24:28,668] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 486
[2024-05-14 12:24:28,669] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 15%|█▌        | 486/3218 [16:31<57:55,  1.27s/it]
[2024-05-14 12:24:29,853] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 487
[2024-05-14 12:24:29,854] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 15%|█▌        | 488/3218 [16:34<56:57,  1.25s/it]
[2024-05-14 12:24:31,094] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 488
[2024-05-14 12:24:31,095] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:24:31,095] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 12:24:32,036] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 489
[2024-05-14 12:24:32,036] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:24:32,036] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25

 15%|█▌        | 490/3218 [16:36<52:24,  1.15s/it]
[2024-05-14 12:24:33,488] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 490
[2024-05-14 12:24:33,488] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 15%|█▌        | 491/3218 [16:37<56:59,  1.25s/it]
[2024-05-14 12:24:35,248] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 491
[2024-05-14 12:24:35,249] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:24:35,249] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 12:24:36,157] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 492
[2024-05-14 12:24:36,157] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 15%|█▌        | 493/3218 [16:40<54:19,  1.20s/it]
[2024-05-14 12:24:37,089] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 493
[2024-05-14 12:24:37,090] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 15%|█▌        | 494/3218 [16:41<53:51,  1.19s/it]
[2024-05-14 12:24:38,824] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 494
[2024-05-14 12:24:38,825] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:24:38,826] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 12:24:40,282] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 495
[2024-05-14 12:24:40,282] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 15%|█▌        | 496/3218 [16:44<1:02:30,  1.38s/it]
[2024-05-14 12:24:41,958] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 496
[2024-05-14 12:24:41,958] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 15%|█▌        | 497/3218 [16:46<1:07:02,  1.48s/it]
[2024-05-14 12:24:43,454] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 497
[2024-05-14 12:24:43,457] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 15%|█▌        | 498/3218 [16:47<1:07:05,  1.48s/it]
[2024-05-14 12:24:45,142] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 498
[2024-05-14 12:24:45,143] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:24:45,143] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 12:24:46,732] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 499
[2024-05-14 12:24:46,732] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 16%|█▌        | 499/3218 [16:49<1:09:22,  1.53s/it]
 16%|█▌        | 500/3218 [16:51<1:10:30,  1.56s/it][INFO|trainer.py:3166] 2024-05-14 12:24:46,957 >> ***** Running Evaluation *****
[INFO|trainer.py:3168] 2024-05-14 12:24:46,957 >>   Num examples = 521
[INFO|trainer.py:3171] 2024-05-14 12:24:46,957 >>   Batch size = 8





 97%|█████████▋| 32/33 [00:11<00:00,  2.78it/s]
{'eval_loss': nan, 'eval_runtime': 12.4075, 'eval_samples_per_second': 41.991, 'eval_steps_per_second': 2.66, 'epoch': 0.16}
[2024-05-14 12:25:00,733] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 500
[2024-05-14 12:25:00,733] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

[2024-05-14 12:25:00,734] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 12:25:02,246] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 501
[2024-05-14 12:25:02,247] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 16%|█▌        | 502/3218 [17:06<3:08:33,  4.17s/it]
[2024-05-14 12:25:03,699] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 502
[2024-05-14 12:25:03,700] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 16%|█▌        | 503/3218 [17:08<2:30:54,  3.33s/it]
[2024-05-14 12:25:05,543] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 503
[2024-05-14 12:25:05,543] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 16%|█▌        | 504/3218 [17:10<2:11:37,  2.91s/it]
[2024-05-14 12:25:07,151] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 504
[2024-05-14 12:25:07,151] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:25:07,151] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 12:25:08,664] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 505
[2024-05-14 12:25:08,664] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 16%|█▌        | 506/3218 [17:13<1:40:33,  2.22s/it]
[2024-05-14 12:25:10,081] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 506
[2024-05-14 12:25:10,082] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 16%|█▌        | 507/3218 [17:14<1:29:01,  1.97s/it]
[2024-05-14 12:25:11,392] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 507
[2024-05-14 12:25:11,392] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 16%|█▌        | 508/3218 [17:15<1:20:06,  1.77s/it]
[2024-05-14 12:25:12,936] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 508
[2024-05-14 12:25:12,937] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:25:12,937] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 12:25:14,170] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 509
[2024-05-14 12:25:14,171] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:25:14,171] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25

 16%|█▌        | 510/3218 [17:18<1:10:15,  1.56s/it]
[2024-05-14 12:25:15,603] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 510
[2024-05-14 12:25:15,604] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 16%|█▌        | 511/3218 [17:20<1:08:28,  1.52s/it]
[2024-05-14 12:25:16,861] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 511
[2024-05-14 12:25:16,861] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:25:16,861] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 12:25:18,246] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 512
[2024-05-14 12:25:18,247] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 16%|█▌        | 513/3218 [17:22<1:04:27,  1.43s/it]
[2024-05-14 12:25:19,577] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 513
[2024-05-14 12:25:19,578] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:25:19,578] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 12:25:20,677] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 514
[2024-05-14 12:25:20,677] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 16%|█▌        | 515/3218 [17:25<58:47,  1.30s/it]
[2024-05-14 12:25:22,016] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 515
[2024-05-14 12:25:22,016] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 16%|█▌        | 516/3218 [17:26<59:46,  1.33s/it]
[2024-05-14 12:25:23,165] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 516
[2024-05-14 12:25:23,165] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:25:23,165] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 12:25:24,320] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 517
[2024-05-14 12:25:24,321] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 16%|█▌        | 518/3218 [17:28<55:10,  1.23s/it]
[2024-05-14 12:25:25,590] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 518
[2024-05-14 12:25:25,590] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:25:25,591] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 12:25:26,669] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 519
[2024-05-14 12:25:26,670] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 16%|█▌        | 520/3218 [17:31<53:52,  1.20s/it]
{'loss': 0.0, 'learning_rate': 2e-05, 'epoch': 0.16}
[2024-05-14 12:25:27,871] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 520
[2024-05-14 12:25:27,871] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 16%|█▌        | 521/3218 [17:32<54:05,  1.20s/it]
[2024-05-14 12:25:28,980] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 521
[2024-05-14 12:25:28,980] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:25:28,980] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 12:25:30,244] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 522
[2024-05-14 12:25:30,244] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 16%|█▋        | 523/3218 [17:34<53:34,  1.19s/it]
[2024-05-14 12:25:31,335] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 523
[2024-05-14 12:25:31,335] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:25:31,335] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 12:25:32,410] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 524
[2024-05-14 12:25:32,411] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 16%|█▋        | 525/3218 [17:36<50:21,  1.12s/it]
[2024-05-14 12:25:33,882] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 525
[2024-05-14 12:25:33,883] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 16%|█▋        | 526/3218 [17:38<55:26,  1.24s/it]
[2024-05-14 12:25:35,388] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 526
[2024-05-14 12:25:35,389] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:25:35,389] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 12:25:36,213] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 527
[2024-05-14 12:25:36,214] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 16%|█▋        | 528/3218 [17:40<49:57,  1.11s/it]
[2024-05-14 12:25:37,790] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 528
[2024-05-14 12:25:37,790] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 16%|█▋        | 529/3218 [17:42<58:53,  1.31s/it]
[2024-05-14 12:25:39,312] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 529
[2024-05-14 12:25:39,313] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:25:39,313] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25

 16%|█▋        | 530/3218 [17:43<1:01:19,  1.37s/it]
[2024-05-14 12:25:40,959] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 530
[2024-05-14 12:25:40,959] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 17%|█▋        | 531/3218 [17:45<1:05:51,  1.47s/it]
[2024-05-14 12:25:42,971] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 531
[2024-05-14 12:25:42,971] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 17%|█▋        | 532/3218 [17:47<1:13:02,  1.63s/it]
[2024-05-14 12:25:45,152] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 532
[2024-05-14 12:25:45,152] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 17%|█▋        | 533/3218 [17:49<1:20:51,  1.81s/it]
[2024-05-14 12:25:46,994] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 533
[2024-05-14 12:25:46,995] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:25:46,996] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 12:25:48,432] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 534
[2024-05-14 12:25:48,433] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 17%|█▋        | 535/3218 [17:52<1:15:14,  1.68s/it]
[2024-05-14 12:25:49,949] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 535
[2024-05-14 12:25:49,950] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 17%|█▋        | 536/3218 [17:54<1:12:41,  1.63s/it]
[2024-05-14 12:25:51,876] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 536
[2024-05-14 12:25:51,877] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 17%|█▋        | 537/3218 [17:56<1:17:25,  1.73s/it]
[2024-05-14 12:25:53,323] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 537
[2024-05-14 12:25:53,324] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:25:53,324] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 12:25:54,733] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 538
[2024-05-14 12:25:54,733] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 17%|█▋        | 539/3218 [17:59<1:09:57,  1.57s/it]
[2024-05-14 12:25:56,954] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 539
[2024-05-14 12:25:56,955] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:25:56,957] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
{'loss': 0.0, 'learning_rate': 2e-05, 'epoch': 0.17}
[2024-05-14 12:25:58,318] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 540
[2024-05-14 12:25:58,319] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 17%|█▋        | 541/3218 [18:02<1:14:23,  1.67s/it]
[2024-05-14 12:25:59,938] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 541
[2024-05-14 12:25:59,939] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 17%|█▋        | 542/3218 [18:04<1:12:35,  1.63s/it]
[2024-05-14 12:26:01,108] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 542
[2024-05-14 12:26:01,108] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:26:01,109] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 12:26:02,479] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 543
[2024-05-14 12:26:02,480] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 17%|█▋        | 544/3218 [18:06<1:04:25,  1.45s/it]
[2024-05-14 12:26:04,001] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 544
[2024-05-14 12:26:04,002] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 17%|█▋        | 545/3218 [18:08<1:06:25,  1.49s/it]
[2024-05-14 12:26:05,910] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 545
[2024-05-14 12:26:05,911] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 17%|█▋        | 546/3218 [18:10<1:12:15,  1.62s/it]
[2024-05-14 12:26:07,161] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 546
[2024-05-14 12:26:07,161] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:26:07,163] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 12:26:08,672] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 547
[2024-05-14 12:26:08,673] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 17%|█▋        | 548/3218 [18:13<1:07:22,  1.51s/it]
[2024-05-14 12:26:09,955] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 548
[2024-05-14 12:26:09,956] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 17%|█▋        | 549/3218 [18:14<1:03:36,  1.43s/it]
[2024-05-14 12:26:11,138] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 549
[2024-05-14 12:26:11,139] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:26:11,139] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
{'loss': 0.0, 'learning_rate': 2e-05, 'epoch': 0.17}
[2024-05-14 12:26:12,313] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 550
[2024-05-14 12:26:12,313] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 17%|█▋        | 551/3218 [18:16<57:44,  1.30s/it]
[2024-05-14 12:26:13,311] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 551
[2024-05-14 12:26:13,311] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:26:13,311] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 12:26:14,335] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 552
[2024-05-14 12:26:14,335] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 17%|█▋        | 553/3218 [18:18<50:37,  1.14s/it]
[2024-05-14 12:26:15,767] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 553
[2024-05-14 12:26:15,768] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:26:15,768] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 12:26:16,904] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 554
[2024-05-14 12:26:16,904] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 17%|█▋        | 554/3218 [18:20<55:41,  1.25s/it]
[2024-05-14 12:26:18,012] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 555
[2024-05-14 12:26:18,013] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 17%|█▋        | 556/3218 [18:22<52:29,  1.18s/it]
[2024-05-14 12:26:19,113] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 556
[2024-05-14 12:26:19,113] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:26:19,114] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 12:26:20,282] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 557
[2024-05-14 12:26:20,282] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 17%|█▋        | 558/3218 [18:24<51:27,  1.16s/it]
[2024-05-14 12:26:21,362] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 558
[2024-05-14 12:26:21,362] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:26:21,363] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 12:26:22,200] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 559
[2024-05-14 12:26:22,200] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:26:22,200] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25

 17%|█▋        | 560/3218 [18:26<46:09,  1.04s/it]
[2024-05-14 12:26:23,462] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 560
[2024-05-14 12:26:23,463] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 17%|█▋        | 561/3218 [18:27<49:11,  1.11s/it]
[2024-05-14 12:26:25,044] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 561
[2024-05-14 12:26:25,044] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:26:25,044] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 12:26:25,900] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 562
[2024-05-14 12:26:25,900] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:26:25,900] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 12:26:26,942] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 563
[2024-05-14 12:26:26,943] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25


 18%|█▊        | 564/3218 [18:31<49:26,  1.12s/it]
[2024-05-14 12:26:29,458] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 564
[2024-05-14 12:26:29,459] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 18%|█▊        | 565/3218 [18:33<1:08:25,  1.55s/it]
[2024-05-14 12:26:31,231] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 565
[2024-05-14 12:26:31,232] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 18%|█▊        | 566/3218 [18:35<1:11:14,  1.61s/it]
[2024-05-14 12:26:33,704] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 566
[2024-05-14 12:26:33,705] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 18%|█▊        | 567/3218 [18:38<1:23:46,  1.90s/it]
[2024-05-14 12:26:35,170] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 567
[2024-05-14 12:26:35,171] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:26:35,171] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 12:26:36,571] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 568
[2024-05-14 12:26:36,572] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 18%|█▊        | 569/3218 [18:41<1:12:33,  1.64s/it]
[2024-05-14 12:26:37,921] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 569
[2024-05-14 12:26:37,922] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:26:37,922] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25

 18%|█▊        | 570/3218 [18:42<1:08:30,  1.55s/it]
[2024-05-14 12:26:39,507] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 570
[2024-05-14 12:26:39,508] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 18%|█▊        | 571/3218 [18:43<1:08:48,  1.56s/it]
[2024-05-14 12:26:41,106] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 571
[2024-05-14 12:26:41,110] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:26:41,110] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 12:26:42,375] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 572
[2024-05-14 12:26:42,376] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 18%|█▊        | 573/3218 [18:46<1:04:54,  1.47s/it]
[2024-05-14 12:26:43,732] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 573
[2024-05-14 12:26:43,732] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 18%|█▊        | 574/3218 [18:48<1:03:59,  1.45s/it]
[2024-05-14 12:26:45,081] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 574
[2024-05-14 12:26:45,082] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:26:45,086] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 12:26:46,520] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 575
[2024-05-14 12:26:46,521] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 18%|█▊        | 576/3218 [18:50<1:02:00,  1.41s/it]
[2024-05-14 12:26:47,672] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 576
[2024-05-14 12:26:47,673] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:26:47,685] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 12:26:48,868] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 577
[2024-05-14 12:26:48,869] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 18%|█▊        | 578/3218 [18:53<57:02,  1.30s/it]
[2024-05-14 12:26:50,905] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 578
[2024-05-14 12:26:50,906] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:26:50,906] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 12:26:52,840] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 579
[2024-05-14 12:26:52,840] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 18%|█▊        | 579/3218 [18:55<1:06:25,  1.51s/it]
{'loss': 0.0, 'learning_rate': 2e-05, 'epoch': 0.18}
[2024-05-14 12:26:54,243] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 580
[2024-05-14 12:26:54,243] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 18%|█▊        | 581/3218 [18:58<1:08:46,  1.56s/it]
[2024-05-14 12:26:56,006] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 581
[2024-05-14 12:26:56,006] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 18%|█▊        | 582/3218 [19:00<1:11:39,  1.63s/it]
[2024-05-14 12:26:57,288] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 582
[2024-05-14 12:26:57,289] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:26:57,289] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 12:26:58,492] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 583
[2024-05-14 12:26:58,492] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 18%|█▊        | 584/3218 [19:02<1:02:30,  1.42s/it]
[2024-05-14 12:26:59,846] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 584
[2024-05-14 12:26:59,847] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 18%|█▊        | 585/3218 [19:04<1:02:09,  1.42s/it]
[2024-05-14 12:27:01,202] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 585
[2024-05-14 12:27:01,202] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:27:01,203] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 12:27:02,312] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 586
[2024-05-14 12:27:02,312] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 18%|█▊        | 587/3218 [19:06<56:46,  1.29s/it]
[2024-05-14 12:27:03,463] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 587
[2024-05-14 12:27:03,463] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:27:03,463] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 12:27:04,522] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 588
[2024-05-14 12:27:04,523] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 18%|█▊        | 589/3218 [19:08<52:23,  1.20s/it]
[2024-05-14 12:27:05,883] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 589
[2024-05-14 12:27:05,883] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:27:05,884] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
{'loss': 0.0, 'learning_rate': 2e-05, 'epoch': 0.18}
[2024-05-14 12:27:06,852] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 590
[2024-05-14 12:27:06,853] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 18%|█▊        | 591/3218 [19:11<50:54,  1.16s/it]
[2024-05-14 12:27:08,048] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 591
[2024-05-14 12:27:08,048] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 18%|█▊        | 592/3218 [19:12<51:18,  1.17s/it]
[2024-05-14 12:27:09,093] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 592
[2024-05-14 12:27:09,094] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:27:09,094] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 12:27:10,125] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 593
[2024-05-14 12:27:10,126] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 18%|█▊        | 594/3218 [19:14<48:10,  1.10s/it]
[2024-05-14 12:27:11,187] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 594
[2024-05-14 12:27:11,188] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:27:11,188] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 12:27:12,470] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 595
[2024-05-14 12:27:12,470] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 19%|█▊        | 596/3218 [19:16<50:25,  1.15s/it]
[2024-05-14 12:27:14,376] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 596
[2024-05-14 12:27:14,377] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 19%|█▊        | 597/3218 [19:18<1:00:57,  1.40s/it]
[2024-05-14 12:27:15,226] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 597
[2024-05-14 12:27:15,227] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:27:15,227] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 12:27:16,426] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 598
[2024-05-14 12:27:16,426] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 19%|█▊        | 599/3218 [19:20<53:03,  1.22s/it]
[2024-05-14 12:27:18,412] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 599
[2024-05-14 12:27:18,413] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:27:18,413] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
 19%|█▊        | 600/3218 [19:22<1:03:57,  1.47s/it][INFO|trainer.py:3166] 2024-05-14 12:27:18,620 >> ***** Running Evaluation *****
[INFO|trainer.py:3168] 2024-05-14 12:27:18,621 >>   Num examples = 521
[INFO|trainer.py:3171] 2024-05-14 12:27:18,621 >>   Batch size = 8






 88%|████████▊ | 29/33 [00:10<00:01,  2.80it/s]
 19%|█▊        | 600/3218 [19:35<1:03:57,  1.47[INFO|trainer.py:2889] 2024-05-14 12:27:36,684 >> Saving model checkpoint to outputs/instruct_tuning/model/tmp-checkpoint-600
[INFO|configuration_utils.py:483] 2024-05-14 12:27:36,687 >> Configuration saved in outputs/instruct_tuning/model/tmp-checkpoint-600/config.json
[INFO|configuration_utils.py:594] 2024-05-14 12:27:36,689 >> Configuration saved in outputs/instruct_tuning/model/tmp-checkpoint-600/generation_config.json
[2024-05-14 12:28:02,165] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step600 is about to be saved!
[2024-05-14 12:28:02,172] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: outputs/instruct_tuning/model/tmp-checkpoint-600/global_step600/mp_rank_00_model_states.pt
[2024-05-14 12:28:02,172] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving outputs/instruct_tuning/model/tmp-checkpoint-600/global_step600/mp_rank_00_model_states.pt...
[INFO|modeling_utils.py:2390] 2024-05-14 12:28:01,240 >> The model is bigger than the maximum size per checkpoint (5GB) and is going to be split in 3 checkpoint shards. You can find where each parameters has been saved in the index located at outputs/instruct_tuning/model/tmp-checkpoint-600/model.safetensors.index.json.
[INFO|tokenization_utils_base.py:2432] 2024-05-14 12:28:01,242 >> tokenizer config file saved in outputs/instruct_tuning/model/tmp-checkpoint-600/tokenizer_config.json
[INFO|tokenization_utils_base.py:2441] 2024-05-14 12:28:01,243 >> Special tokens file saved in outputs/instruct_tuning/model/tmp-checkpoint-600/special_tokens_map.json
/mounts/work/lyk/anaconda3/envs/torch2/lib/python3.8/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
[2024-05-14 12:29:14,468] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved outputs/instruct_tuning/model/tmp-checkpoint-600/global_step600/mp_rank_00_model_states.pt.
[2024-05-14 12:29:14,471] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step600 is ready now!
[2024-05-14 12:29:15,849] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 600
[2024-05-14 12:29:15,850] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:29:15,850] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
 19%|█▊        | 601/3218 [21:20<26:21:29, 36.26s/it]
[2024-05-14 12:29:17,571] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 601
[2024-05-14 12:29:17,572] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:29:17,572] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 12:29:19,136] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 602
[2024-05-14 12:29:19,137] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25


 19%|█▊        | 603/3218 [21:23<13:30:16, 18.59s/it]
[2024-05-14 12:29:21,266] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 603
[2024-05-14 12:29:21,266] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:29:21,268] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 12:29:22,955] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 604
[2024-05-14 12:29:22,956] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 19%|█▉        | 605/3218 [21:27<7:18:23, 10.07s/it]
[2024-05-14 12:29:24,363] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 605
[2024-05-14 12:29:24,364] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 19%|█▉        | 606/3218 [21:28<5:25:07,  7.47s/it]
[2024-05-14 12:29:25,829] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 606
[2024-05-14 12:29:25,830] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 19%|█▉        | 607/3218 [21:30<4:06:33,  5.67s/it]
[2024-05-14 12:29:27,429] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 607
[2024-05-14 12:29:27,429] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:29:27,429] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 12:29:28,622] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 608
[2024-05-14 12:29:28,623] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 19%|█▉        | 609/3218 [21:32<2:30:04,  3.45s/it]
[2024-05-14 12:29:29,979] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 609
[2024-05-14 12:29:29,980] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:29:29,982] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25

 19%|█▉        | 610/3218 [21:34<2:03:31,  2.84s/it]
[2024-05-14 12:29:31,271] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 610
[2024-05-14 12:29:31,272] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:29:31,272] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 12:29:33,170] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 611
[2024-05-14 12:29:33,171] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 19%|█▉        | 611/3218 [21:35<1:42:34,  2.36s/it]
[2024-05-14 12:29:34,476] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 612
[2024-05-14 12:29:34,477] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 19%|█▉        | 613/3218 [21:38<1:24:55,  1.96s/it]
[2024-05-14 12:29:35,984] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 613
[2024-05-14 12:29:35,984] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 19%|█▉        | 614/3218 [21:40<1:19:22,  1.83s/it]
[2024-05-14 12:29:37,452] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 614
[2024-05-14 12:29:37,452] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:29:37,452] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 12:29:38,719] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 615
[2024-05-14 12:29:38,719] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 19%|█▉        | 616/3218 [21:43<1:08:38,  1.58s/it]
[2024-05-14 12:29:40,386] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 616
[2024-05-14 12:29:40,386] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 19%|█▉        | 617/3218 [21:44<1:10:00,  1.61s/it]
[2024-05-14 12:29:41,797] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 617
[2024-05-14 12:29:41,798] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:29:41,800] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 12:29:43,192] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 618
[2024-05-14 12:29:43,192] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 19%|█▉        | 618/3218 [21:46<1:07:18,  1.55s/it]
[2024-05-14 12:29:44,451] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 619
[2024-05-14 12:29:44,452] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:29:44,452] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25

 19%|█▉        | 620/3218 [21:48<1:01:38,  1.42s/it]
[2024-05-14 12:29:45,698] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 620
[2024-05-14 12:29:45,698] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:29:45,699] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 12:29:46,833] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 621
[2024-05-14 12:29:46,833] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 19%|█▉        | 622/3218 [21:51<56:28,  1.31s/it]
[2024-05-14 12:29:48,048] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 622
[2024-05-14 12:29:48,048] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:29:48,048] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 12:29:49,132] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 623
[2024-05-14 12:29:49,132] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 19%|█▉        | 623/3218 [21:52<54:55,  1.27s/it]
[2024-05-14 12:29:50,501] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 624
[2024-05-14 12:29:50,501] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 19%|█▉        | 625/3218 [21:54<54:31,  1.26s/it]
[2024-05-14 12:29:51,625] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 625
[2024-05-14 12:29:51,626] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:29:51,626] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 12:29:53,246] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 626
[2024-05-14 12:29:53,247] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 19%|█▉        | 626/3218 [21:56<52:41,  1.22s/it]
[2024-05-14 12:29:54,397] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 627
[2024-05-14 12:29:54,398] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 20%|█▉        | 628/3218 [21:58<55:18,  1.28s/it]
[2024-05-14 12:29:55,523] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 628
[2024-05-14 12:29:55,523] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:29:55,523] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 12:29:56,646] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 629
[2024-05-14 12:29:56,647] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:29:56,647] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25

 20%|█▉        | 630/3218 [22:01<51:18,  1.19s/it]
[2024-05-14 12:29:58,297] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 630
[2024-05-14 12:29:58,299] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 20%|█▉        | 631/3218 [22:02<57:25,  1.33s/it]
[2024-05-14 12:29:59,610] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 631
[2024-05-14 12:29:59,610] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:29:59,611] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 12:30:00,435] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 632
[2024-05-14 12:30:00,436] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 20%|█▉        | 633/3218 [22:04<48:35,  1.13s/it]
[2024-05-14 12:30:01,602] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 633
[2024-05-14 12:30:01,603] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:30:01,603] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 12:30:03,193] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 634
[2024-05-14 12:30:03,193] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 20%|█▉        | 634/3218 [22:06<51:17,  1.19s/it]
[2024-05-14 12:30:04,559] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 635
[2024-05-14 12:30:04,559] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 20%|█▉        | 636/3218 [22:08<57:19,  1.33s/it]
[2024-05-14 12:30:05,996] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 636
[2024-05-14 12:30:05,997] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 20%|█▉        | 637/3218 [22:10<58:30,  1.36s/it]
[2024-05-14 12:30:07,502] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 637
[2024-05-14 12:30:07,502] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:30:07,503] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 12:30:09,101] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 638
[2024-05-14 12:30:09,101] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 20%|█▉        | 639/3218 [22:13<1:02:45,  1.46s/it]
[2024-05-14 12:30:10,530] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 639
[2024-05-14 12:30:10,531] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:30:10,531] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25

 20%|█▉        | 640/3218 [22:14<1:02:26,  1.45s/it]
[2024-05-14 12:30:12,220] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 640
[2024-05-14 12:30:12,220] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 20%|█▉        | 641/3218 [22:16<1:06:12,  1.54s/it]
[2024-05-14 12:30:14,120] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 641
[2024-05-14 12:30:14,121] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 20%|█▉        | 642/3218 [22:18<1:10:03,  1.63s/it]
[2024-05-14 12:30:15,517] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 642
[2024-05-14 12:30:15,518] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:30:15,518] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 12:30:17,003] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 643
[2024-05-14 12:30:17,004] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 20%|██        | 644/3218 [22:21<1:05:42,  1.53s/it]
[2024-05-14 12:30:18,403] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 644
[2024-05-14 12:30:18,404] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 20%|██        | 645/3218 [22:22<1:04:02,  1.49s/it]
[2024-05-14 12:30:20,541] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 645
[2024-05-14 12:30:20,541] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 20%|██        | 646/3218 [22:24<1:12:51,  1.70s/it]
[2024-05-14 12:30:22,032] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 646
[2024-05-14 12:30:22,033] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:30:22,033] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 12:30:23,323] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 647
[2024-05-14 12:30:23,323] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 20%|██        | 647/3218 [22:26<1:09:37,  1.62s/it]
[2024-05-14 12:30:24,672] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 648
[2024-05-14 12:30:24,673] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 20%|██        | 649/3218 [22:29<1:03:24,  1.48s/it]
[2024-05-14 12:30:26,346] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 649
[2024-05-14 12:30:26,346] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:30:26,347] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25

 20%|██        | 650/3218 [22:30<1:05:50,  1.54s/it]
[2024-05-14 12:30:27,702] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 650
[2024-05-14 12:30:27,702] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:30:27,703] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 12:30:29,015] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 651
[2024-05-14 12:30:29,015] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 20%|██        | 652/3218 [22:33<1:00:52,  1.42s/it]
[2024-05-14 12:30:30,318] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 652
[2024-05-14 12:30:30,319] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 20%|██        | 653/3218 [22:34<59:43,  1.40s/it]
[2024-05-14 12:30:31,520] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 653
[2024-05-14 12:30:31,521] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:30:31,521] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 12:30:32,715] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 654
[2024-05-14 12:30:32,716] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 20%|██        | 655/3218 [22:37<54:42,  1.28s/it]
[2024-05-14 12:30:33,801] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 655
[2024-05-14 12:30:33,801] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:30:33,802] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 12:30:35,028] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 656
[2024-05-14 12:30:35,028] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 20%|██        | 657/3218 [22:39<52:50,  1.24s/it]
[2024-05-14 12:30:36,212] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 657
[2024-05-14 12:30:36,213] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:30:36,213] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 12:30:37,392] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 658
[2024-05-14 12:30:37,393] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 20%|██        | 658/3218 [22:40<51:56,  1.22s/it]
[2024-05-14 12:30:38,627] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 659
[2024-05-14 12:30:38,627] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:30:38,627] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25

 21%|██        | 660/3218 [22:43<52:07,  1.22s/it]
[2024-05-14 12:30:40,301] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 660
[2024-05-14 12:30:40,301] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:30:40,303] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 12:30:41,394] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 661
[2024-05-14 12:30:41,394] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 21%|██        | 661/3218 [22:44<57:29,  1.35s/it]
[2024-05-14 12:30:42,447] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 662
[2024-05-14 12:30:42,448] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 21%|██        | 663/3218 [22:46<51:12,  1.20s/it]
[2024-05-14 12:30:43,864] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 663
[2024-05-14 12:30:43,864] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:30:43,865] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 12:30:44,863] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 664
[2024-05-14 12:30:44,864] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 21%|██        | 665/3218 [22:49<50:09,  1.18s/it]
[2024-05-14 12:30:46,352] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 665
[2024-05-14 12:30:46,353] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 21%|██        | 666/3218 [22:50<54:26,  1.28s/it]
[2024-05-14 12:30:47,862] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 666
[2024-05-14 12:30:47,862] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:30:47,863] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 12:30:48,809] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 667
[2024-05-14 12:30:48,809] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 21%|██        | 668/3218 [22:53<49:59,  1.18s/it]
[2024-05-14 12:30:50,072] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 668
[2024-05-14 12:30:50,072] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 21%|██        | 669/3218 [22:54<53:31,  1.26s/it]
[2024-05-14 12:30:51,803] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 669
[2024-05-14 12:30:51,804] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:30:51,804] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25

 21%|██        | 670/3218 [22:56<59:48,  1.41s/it]
[2024-05-14 12:30:54,184] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 670
[2024-05-14 12:30:54,184] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 21%|██        | 671/3218 [22:58<1:12:04,  1.70s/it]
[2024-05-14 12:30:55,783] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 671
[2024-05-14 12:30:55,783] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:30:55,784] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 12:30:57,294] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 672
[2024-05-14 12:30:57,294] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 21%|██        | 672/3218 [23:00<1:10:34,  1.66s/it]
[2024-05-14 12:30:59,039] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 673
[2024-05-14 12:30:59,039] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 21%|██        | 674/3218 [23:03<1:10:38,  1.67s/it]
[2024-05-14 12:31:00,487] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 674
[2024-05-14 12:31:00,488] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 21%|██        | 675/3218 [23:05<1:08:54,  1.63s/it]
[2024-05-14 12:31:02,139] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 675
[2024-05-14 12:31:02,140] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:31:02,140] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 12:31:03,455] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 676
[2024-05-14 12:31:03,456] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 21%|██        | 676/3218 [23:06<1:07:51,  1.60s/it]
[2024-05-14 12:31:05,109] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 677
[2024-05-14 12:31:05,110] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 21%|██        | 678/3218 [23:09<1:05:42,  1.55s/it]
[2024-05-14 12:31:06,450] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 678
[2024-05-14 12:31:06,451] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 21%|██        | 679/3218 [23:10<1:02:35,  1.48s/it]
[2024-05-14 12:31:07,745] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 679
[2024-05-14 12:31:07,745] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:31:07,746] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
{'loss': 0.0, 'learning_rate': 2e-05, 'epoch': 0.21}
[2024-05-14 12:31:09,304] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 680
[2024-05-14 12:31:09,305] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 21%|██        | 680/3218 [23:12<1:00:23,  1.43s/it]
[2024-05-14 12:31:10,914] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 681
[2024-05-14 12:31:10,914] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 21%|██        | 682/3218 [23:15<1:04:17,  1.52s/it]
[2024-05-14 12:31:12,473] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 682
[2024-05-14 12:31:12,473] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 21%|██        | 683/3218 [23:16<1:04:55,  1.54s/it]
[2024-05-14 12:31:13,666] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 683
[2024-05-14 12:31:13,667] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:31:13,667] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 12:31:15,156] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 684
[2024-05-14 12:31:15,157] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 21%|██▏       | 685/3218 [23:19<1:01:28,  1.46s/it]
[2024-05-14 12:31:16,376] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 685
[2024-05-14 12:31:16,377] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 21%|██▏       | 686/3218 [23:20<57:32,  1.36s/it]
[2024-05-14 12:31:17,618] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 686
[2024-05-14 12:31:17,618] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:31:17,620] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 12:31:18,771] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 687
[2024-05-14 12:31:18,771] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 21%|██▏       | 688/3218 [23:23<53:46,  1.28s/it]
[2024-05-14 12:31:19,974] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 688
[2024-05-14 12:31:19,975] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:31:19,982] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 12:31:21,356] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 689
[2024-05-14 12:31:21,357] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 21%|██▏       | 689/3218 [23:24<53:01,  1.26s/it]
{'loss': 0.0, 'learning_rate': 2e-05, 'epoch': 0.21}
[2024-05-14 12:31:22,478] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 690
[2024-05-14 12:31:22,478] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 21%|██▏       | 691/3218 [23:26<52:26,  1.25s/it]
[2024-05-14 12:31:23,966] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 691
[2024-05-14 12:31:23,967] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:31:23,969] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 12:31:25,424] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 692
[2024-05-14 12:31:25,425] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 22%|██▏       | 692/3218 [23:28<55:55,  1.33s/it]
[2024-05-14 12:31:27,006] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 693
[2024-05-14 12:31:27,007] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 22%|██▏       | 694/3218 [23:31<1:00:12,  1.43s/it]
[2024-05-14 12:31:28,113] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 694
[2024-05-14 12:31:28,114] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:31:28,114] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 12:31:29,186] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 695
[2024-05-14 12:31:29,187] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 22%|██▏       | 696/3218 [23:33<52:19,  1.24s/it]
[2024-05-14 12:31:30,460] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 696
[2024-05-14 12:31:30,461] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 22%|██▏       | 697/3218 [23:34<53:14,  1.27s/it]
[2024-05-14 12:31:31,600] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 697
[2024-05-14 12:31:31,601] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:31:31,601] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 12:31:32,697] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 698
[2024-05-14 12:31:32,697] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 22%|██▏       | 699/3218 [23:37<49:42,  1.18s/it]
[2024-05-14 12:31:33,606] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 699
[2024-05-14 12:31:33,606] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:31:33,606] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
 22%|██▏       | 700/3218 [23:38<46:05,  1.10s/it][INFO|trainer.py:3166] 2024-05-14 12:31:33,775 >> ***** Running Evaluation *****
[INFO|trainer.py:3168] 2024-05-14 12:31:33,775 >>   Num examples = 521
[INFO|trainer.py:3171] 2024-05-14 12:31:33,775 >>   Batch size = 8





 97%|█████████▋| 32/33 [00:11<00:00,  2.78it/s]
{'eval_loss': nan, 'eval_runtime': 12.3995, 'eval_samples_per_second': 42.018, 'eval_steps_per_second': 2.661, 'epoch': 0.22}
[2024-05-14 12:31:47,481] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 700
[2024-05-14 12:31:47,481] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

[2024-05-14 12:31:47,482] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 12:31:48,959] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 701
[2024-05-14 12:31:48,959] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 22%|██▏       | 702/3218 [23:53<2:43:54,  3.91s/it]
[2024-05-14 12:31:49,885] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 702
[2024-05-14 12:31:49,885] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:31:49,886] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 12:31:51,314] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 703
[2024-05-14 12:31:51,314] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 22%|██▏       | 703/3218 [23:54<2:03:46,  2.95s/it]
[2024-05-14 12:31:52,869] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 704
[2024-05-14 12:31:52,869] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 22%|██▏       | 705/3218 [23:57<1:34:33,  2.26s/it]
[2024-05-14 12:31:54,600] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 705
[2024-05-14 12:31:54,601] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 22%|██▏       | 706/3218 [23:59<1:27:41,  2.09s/it]
[2024-05-14 12:31:56,096] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 706
[2024-05-14 12:31:56,097] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 22%|██▏       | 707/3218 [24:00<1:19:58,  1.91s/it]
[2024-05-14 12:31:57,900] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 707
[2024-05-14 12:31:57,900] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 22%|██▏       | 708/3218 [24:02<1:19:14,  1.89s/it]
[2024-05-14 12:31:59,536] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 708
[2024-05-14 12:31:59,537] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:31:59,538] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 12:32:01,097] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 709
[2024-05-14 12:32:01,097] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:32:01,098] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25

 22%|██▏       | 710/3218 [24:05<1:12:43,  1.74s/it]
[2024-05-14 12:32:02,945] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 710
[2024-05-14 12:32:02,946] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 22%|██▏       | 711/3218 [24:07<1:14:15,  1.78s/it]
[2024-05-14 12:32:05,016] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 711
[2024-05-14 12:32:05,017] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 22%|██▏       | 712/3218 [24:09<1:17:44,  1.86s/it]
[2024-05-14 12:32:06,403] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 712
[2024-05-14 12:32:06,404] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 22%|██▏       | 713/3218 [24:10<1:11:21,  1.71s/it]
[2024-05-14 12:32:07,664] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 713
[2024-05-14 12:32:07,665] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:32:07,666] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 12:32:09,217] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 714
[2024-05-14 12:32:09,217] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 22%|██▏       | 715/3218 [24:13<1:05:10,  1.56s/it]
[2024-05-14 12:32:10,600] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 715
[2024-05-14 12:32:10,601] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 22%|██▏       | 716/3218 [24:15<1:02:56,  1.51s/it]
[2024-05-14 12:32:11,959] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 716
[2024-05-14 12:32:11,959] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 22%|██▏       | 717/3218 [24:16<1:00:38,  1.45s/it]
[2024-05-14 12:32:13,705] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 717
[2024-05-14 12:32:13,706] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:32:13,706] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 12:32:15,377] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 718
[2024-05-14 12:32:15,377] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 22%|██▏       | 718/3218 [24:18<1:06:33,  1.60s/it]
[2024-05-14 12:32:16,605] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 719
[2024-05-14 12:32:16,606] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:32:16,608] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25

 22%|██▏       | 720/3218 [24:21<1:01:09,  1.47s/it]
[2024-05-14 12:32:18,202] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 720
[2024-05-14 12:32:18,203] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:32:18,204] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 12:32:19,608] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 721
[2024-05-14 12:32:19,609] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 22%|██▏       | 721/3218 [24:22<1:02:09,  1.49s/it]
[2024-05-14 12:32:21,057] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 722
[2024-05-14 12:32:21,058] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 22%|██▏       | 723/3218 [24:25<1:01:18,  1.47s/it]
[2024-05-14 12:32:22,299] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 723
[2024-05-14 12:32:22,299] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 22%|██▏       | 724/3218 [24:26<58:00,  1.40s/it]
[2024-05-14 12:32:23,701] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 724
[2024-05-14 12:32:23,702] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:32:23,702] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 12:32:24,856] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 725
[2024-05-14 12:32:24,856] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 23%|██▎       | 726/3218 [24:29<55:09,  1.33s/it]
[2024-05-14 12:32:26,109] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 726
[2024-05-14 12:32:26,110] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:32:26,110] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 12:32:27,200] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 727
[2024-05-14 12:32:27,201] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 23%|██▎       | 728/3218 [24:31<51:28,  1.24s/it]
[2024-05-14 12:32:28,309] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 728
[2024-05-14 12:32:28,309] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 23%|██▎       | 729/3218 [24:32<49:50,  1.20s/it]
[2024-05-14 12:32:29,587] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 729
[2024-05-14 12:32:29,587] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:32:29,588] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
{'loss': 0.0, 'learning_rate': 2e-05, 'epoch': 0.23}
[2024-05-14 12:32:30,713] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 730
[2024-05-14 12:32:30,713] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 23%|██▎       | 731/3218 [24:35<49:43,  1.20s/it]
[2024-05-14 12:32:31,777] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 731
[2024-05-14 12:32:31,777] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:32:31,782] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 12:32:33,028] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 732
[2024-05-14 12:32:33,028] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 23%|██▎       | 733/3218 [24:37<48:46,  1.18s/it]
[2024-05-14 12:32:34,174] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 733
[2024-05-14 12:32:34,175] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:32:34,175] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 12:32:35,140] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 734
[2024-05-14 12:32:35,140] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 23%|██▎       | 735/3218 [24:39<45:27,  1.10s/it]
[2024-05-14 12:32:36,541] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 735
[2024-05-14 12:32:36,541] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 23%|██▎       | 736/3218 [24:40<49:33,  1.20s/it]
[2024-05-14 12:32:37,942] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 736
[2024-05-14 12:32:37,943] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:32:37,943] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 12:32:38,729] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 737
[2024-05-14 12:32:38,729] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 23%|██▎       | 738/3218 [24:42<44:08,  1.07s/it]
[2024-05-14 12:32:39,973] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 738
[2024-05-14 12:32:39,974] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:32:39,976] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 12:32:41,373] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 739
[2024-05-14 12:32:41,373] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:32:41,379] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25

 23%|██▎       | 739/3218 [24:44<48:34,  1.18s/it]
[2024-05-14 12:32:43,099] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 740
[2024-05-14 12:32:43,100] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 23%|██▎       | 741/3218 [24:47<57:00,  1.38s/it]
[2024-05-14 12:32:44,723] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 741
[2024-05-14 12:32:44,724] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 23%|██▎       | 742/3218 [24:49<1:00:29,  1.47s/it]
[2024-05-14 12:32:46,500] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 742
[2024-05-14 12:32:46,500] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 23%|██▎       | 743/3218 [24:50<1:04:22,  1.56s/it]
[2024-05-14 12:32:48,425] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 743
[2024-05-14 12:32:48,425] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 23%|██▎       | 744/3218 [24:52<1:08:54,  1.67s/it]
[2024-05-14 12:32:49,882] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 744
[2024-05-14 12:32:49,883] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:32:49,884] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 12:32:51,522] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 745
[2024-05-14 12:32:51,523] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 23%|██▎       | 745/3218 [24:54<1:05:39,  1.59s/it]
[2024-05-14 12:32:52,933] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 746
[2024-05-14 12:32:52,934] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 23%|██▎       | 747/3218 [24:57<1:03:50,  1.55s/it]
[2024-05-14 12:32:55,009] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 747
[2024-05-14 12:32:55,010] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 23%|██▎       | 748/3218 [24:59<1:10:34,  1.71s/it]
[2024-05-14 12:32:56,474] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 748
[2024-05-14 12:32:56,475] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 23%|██▎       | 749/3218 [25:00<1:07:11,  1.63s/it]
[2024-05-14 12:32:57,868] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 749
[2024-05-14 12:32:57,869] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:32:57,869] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
{'loss': 0.0, 'learning_rate': 2e-05, 'epoch': 0.23}
[2024-05-14 12:32:59,418] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 750
[2024-05-14 12:32:59,419] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 23%|██▎       | 750/3218 [25:02<1:04:04,  1.56s/it]
[2024-05-14 12:33:00,736] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 751
[2024-05-14 12:33:00,737] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 23%|██▎       | 752/3218 [25:05<1:00:54,  1.48s/it]
[2024-05-14 12:33:02,263] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 752
[2024-05-14 12:33:02,263] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 23%|██▎       | 753/3218 [25:06<1:01:54,  1.51s/it]
[2024-05-14 12:33:03,757] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 753
[2024-05-14 12:33:03,757] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:33:03,761] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 12:33:05,025] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 754
[2024-05-14 12:33:05,025] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 23%|██▎       | 755/3218 [25:09<58:39,  1.43s/it]
[2024-05-14 12:33:06,416] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 755
[2024-05-14 12:33:06,417] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:33:06,418] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 12:33:07,629] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 756
[2024-05-14 12:33:07,630] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 23%|██▎       | 756/3218 [25:10<58:00,  1.41s/it]
[2024-05-14 12:33:08,958] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 757
[2024-05-14 12:33:08,959] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 24%|██▎       | 758/3218 [25:13<55:30,  1.35s/it]
[2024-05-14 12:33:10,302] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 758
[2024-05-14 12:33:10,302] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:33:10,302] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 12:33:11,486] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 759
[2024-05-14 12:33:11,486] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:33:11,486] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25

 24%|██▎       | 759/3218 [25:14<55:21,  1.35s/it]
[2024-05-14 12:33:12,709] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 760
[2024-05-14 12:33:12,709] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 24%|██▎       | 761/3218 [25:17<52:15,  1.28s/it]
[2024-05-14 12:33:13,932] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 761
[2024-05-14 12:33:13,932] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:33:13,932] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 12:33:15,053] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 762
[2024-05-14 12:33:15,054] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 24%|██▎       | 763/3218 [25:19<49:44,  1.22s/it]
[2024-05-14 12:33:16,543] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 763
[2024-05-14 12:33:16,543] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 24%|██▎       | 764/3218 [25:20<53:25,  1.31s/it]
[2024-05-14 12:33:17,833] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 764
[2024-05-14 12:33:17,834] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:33:17,834] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 12:33:19,224] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 765
[2024-05-14 12:33:19,225] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 24%|██▍       | 766/3218 [25:23<54:15,  1.33s/it]
[2024-05-14 12:33:20,479] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 766
[2024-05-14 12:33:20,480] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:33:20,480] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 12:33:21,546] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 767
[2024-05-14 12:33:21,546] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 24%|██▍       | 767/3218 [25:24<53:05,  1.30s/it]
[2024-05-14 12:33:22,665] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 768
[2024-05-14 12:33:22,666] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:33:22,666] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 12:33:23,691] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 769
[2024-05-14 12:33:23,691] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 24%|██▍       | 769/3218 [25:27<49:10,  1.20s/it]
{'loss': 0.0, 'learning_rate': 2e-05, 'epoch': 0.24}
[2024-05-14 12:33:25,231] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 770
[2024-05-14 12:33:25,232] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 24%|██▍       | 771/3218 [25:29<51:21,  1.26s/it]
[2024-05-14 12:33:26,938] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 771
[2024-05-14 12:33:26,939] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:33:26,939] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 12:33:27,716] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 772
[2024-05-14 12:33:27,717] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 24%|██▍       | 772/3218 [25:31<57:53,  1.42s/it]
[2024-05-14 12:33:29,487] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 773
[2024-05-14 12:33:29,487] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 24%|██▍       | 773/3218 [25:31<47:09,  1.16s/it]
[2024-05-14 12:33:31,153] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 774
[2024-05-14 12:33:31,154] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 24%|██▍       | 775/3218 [25:35<1:00:16,  1.48s/it]
[2024-05-14 12:33:33,276] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 775
[2024-05-14 12:33:33,277] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 24%|██▍       | 776/3218 [25:37<1:08:26,  1.68s/it]
[2024-05-14 12:33:35,153] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 776
[2024-05-14 12:33:35,153] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 24%|██▍       | 777/3218 [25:39<1:10:50,  1.74s/it]
[2024-05-14 12:33:36,739] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 777
[2024-05-14 12:33:36,740] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 24%|██▍       | 778/3218 [25:41<1:08:27,  1.68s/it]
[2024-05-14 12:33:38,166] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 778
[2024-05-14 12:33:38,167] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:33:38,167] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 12:33:39,601] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 779
[2024-05-14 12:33:39,601] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:33:39,602] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25

 24%|██▍       | 779/3218 [25:42<1:05:07,  1.60s/it]
[2024-05-14 12:33:41,110] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 780
[2024-05-14 12:33:41,111] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 24%|██▍       | 781/3218 [25:45<1:02:30,  1.54s/it]
[2024-05-14 12:33:43,535] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 781
[2024-05-14 12:33:43,536] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:33:43,537] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 12:33:45,090] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 782
[2024-05-14 12:33:45,090] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 24%|██▍       | 783/3218 [25:49<1:10:06,  1.73s/it]
[2024-05-14 12:33:46,932] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 783
[2024-05-14 12:33:46,933] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 24%|██▍       | 784/3218 [25:51<1:12:01,  1.78s/it]
[2024-05-14 12:33:48,434] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 784
[2024-05-14 12:33:48,435] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:33:48,435] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 12:33:49,763] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 785
[2024-05-14 12:33:49,763] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 24%|██▍       | 785/3218 [25:52<1:07:40,  1.67s/it]
[2024-05-14 12:33:51,515] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 786
[2024-05-14 12:33:51,515] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 24%|██▍       | 786/3218 [25:54<1:03:48,  1.57s/it]
[2024-05-14 12:33:52,854] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 787
[2024-05-14 12:33:52,855] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 24%|██▍       | 788/3218 [25:57<1:02:07,  1.53s/it]
[2024-05-14 12:33:54,184] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 788
[2024-05-14 12:33:54,184] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 25%|██▍       | 789/3218 [25:58<59:49,  1.48s/it]
[2024-05-14 12:33:55,976] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 789
[2024-05-14 12:33:55,977] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:33:55,978] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
{'loss': 0.0, 'learning_rate': 2e-05, 'epoch': 0.25}
[2024-05-14 12:33:57,320] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 790
[2024-05-14 12:33:57,320] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 25%|██▍       | 791/3218 [26:01<1:01:28,  1.52s/it]
[2024-05-14 12:33:58,778] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 791
[2024-05-14 12:33:58,778] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 25%|██▍       | 792/3218 [26:03<1:00:07,  1.49s/it]
[2024-05-14 12:33:59,952] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 792
[2024-05-14 12:33:59,952] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:33:59,953] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 12:34:01,401] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 793
[2024-05-14 12:34:01,401] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 25%|██▍       | 794/3218 [26:05<56:47,  1.41s/it]
[2024-05-14 12:34:03,123] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 794
[2024-05-14 12:34:03,124] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 25%|██▍       | 795/3218 [26:07<1:00:25,  1.50s/it]
[2024-05-14 12:34:04,404] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 795
[2024-05-14 12:34:04,405] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 25%|██▍       | 796/3218 [26:08<57:38,  1.43s/it]
[2024-05-14 12:34:06,042] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 796
[2024-05-14 12:34:06,042] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:34:06,043] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 12:34:07,422] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 797
[2024-05-14 12:34:07,423] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 25%|██▍       | 798/3218 [26:11<58:58,  1.46s/it]
[2024-05-14 12:34:08,730] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 798
[2024-05-14 12:34:08,730] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 25%|██▍       | 799/3218 [26:13<57:02,  1.41s/it]
[2024-05-14 12:34:10,436] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 799
[2024-05-14 12:34:10,436] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:34:10,438] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
 25%|██▍       | 800/3218 [26:14<1:00:37,  1.50s/it][INFO|trainer.py:3166] 2024-05-14 12:34:10,628 >> ***** Running Evaluation *****
[INFO|trainer.py:3168] 2024-05-14 12:34:10,628 >>   Num examples = 521
[INFO|trainer.py:3171] 2024-05-14 12:34:10,628 >>   Batch size = 8





 91%|█████████ | 30/33 [00:10<00:01,  2.82it/s]

 25%|██▍       | 800/3218 [26:27<1:00:37,  1.50[INFO|trainer.py:2889] 2024-05-14 12:34:28,112 >> Saving model checkpoint to outputs/instruct_tuning/model/tmp-checkpoint-800
[INFO|configuration_utils.py:483] 2024-05-14 12:34:28,115 >> Configuration saved in outputs/instruct_tuning/model/tmp-checkpoint-800/config.json
[INFO|configuration_utils.py:594] 2024-05-14 12:34:28,116 >> Configuration saved in outputs/instruct_tuning/model/tmp-checkpoint-800/generation_config.json
[2024-05-14 12:34:53,589] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step800 is about to be saved!
[2024-05-14 12:34:53,596] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: outputs/instruct_tuning/model/tmp-checkpoint-800/global_step800/mp_rank_00_model_states.pt
[2024-05-14 12:34:53,596] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving outputs/instruct_tuning/model/tmp-checkpoint-800/global_step800/mp_rank_00_model_states.pt...
[INFO|modeling_utils.py:2390] 2024-05-14 12:34:52,849 >> The model is bigger than the maximum size per checkpoint (5GB) and is going to be split in 3 checkpoint shards. You can find where each parameters has been saved in the index located at outputs/instruct_tuning/model/tmp-checkpoint-800/model.safetensors.index.json.
[INFO|tokenization_utils_base.py:2432] 2024-05-14 12:34:52,852 >> tokenizer config file saved in outputs/instruct_tuning/model/tmp-checkpoint-800/tokenizer_config.json
[INFO|tokenization_utils_base.py:2441] 2024-05-14 12:34:52,852 >> Special tokens file saved in outputs/instruct_tuning/model/tmp-checkpoint-800/special_tokens_map.json
/mounts/work/lyk/anaconda3/envs/torch2/lib/python3.8/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
[2024-05-14 12:36:10,595] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved outputs/instruct_tuning/model/tmp-checkpoint-800/global_step800/mp_rank_00_model_states.pt.
[2024-05-14 12:36:10,598] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step800 is ready now!
[INFO|trainer.py:2979] 2024-05-14 12:36:10,619 >> Deleting older checkpoint [outputs/instruct_tuning/model/checkpoint-400] due to args.save_total_limit
[2024-05-14 12:36:13,452] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 800
[2024-05-14 12:36:13,453] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:36:13,462] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
 25%|██▍       | 801/3218 [28:17<25:29:18, 37.96s/it]
[2024-05-14 12:36:14,539] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 801
[2024-05-14 12:36:14,540] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:36:14,541] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 12:36:15,601] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 802
[2024-05-14 12:36:15,602] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 25%|██▍       | 803/3218 [28:20<12:50:49, 19.15s/it]
[2024-05-14 12:36:16,733] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 803
[2024-05-14 12:36:16,734] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:36:16,734] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 12:36:17,789] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 804
[2024-05-14 12:36:17,790] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 25%|██▍       | 804/3218 [28:21<9:12:50, 13.74s/it]
[2024-05-14 12:36:19,402] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 805
[2024-05-14 12:36:19,403] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 25%|██▌       | 806/3218 [28:23<4:59:11,  7.44s/it]
[2024-05-14 12:36:21,379] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 806
[2024-05-14 12:36:21,380] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 25%|██▌       | 807/3218 [28:25<3:52:41,  5.79s/it]
[2024-05-14 12:36:22,159] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 807
[2024-05-14 12:36:22,159] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:36:22,159] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 12:36:23,510] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 808
[2024-05-14 12:36:23,511] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 25%|██▌       | 809/3218 [28:27<2:17:20,  3.42s/it]
[2024-05-14 12:36:25,109] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 809
[2024-05-14 12:36:25,110] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:36:25,110] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25

 25%|██▌       | 810/3218 [28:29<1:55:42,  2.88s/it]
[2024-05-14 12:36:27,042] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 810
[2024-05-14 12:36:27,043] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 25%|██▌       | 811/3218 [28:31<1:44:44,  2.61s/it]
[2024-05-14 12:36:28,538] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 811
[2024-05-14 12:36:28,539] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 25%|██▌       | 812/3218 [28:32<1:30:30,  2.26s/it]
[2024-05-14 12:36:30,425] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 812
[2024-05-14 12:36:30,427] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 25%|██▌       | 813/3218 [28:34<1:26:06,  2.15s/it]
[2024-05-14 12:36:32,339] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 813
[2024-05-14 12:36:32,340] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 25%|██▌       | 814/3218 [28:36<1:23:56,  2.09s/it]
[2024-05-14 12:36:34,527] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 814
[2024-05-14 12:36:34,527] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:36:34,527] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 12:36:36,000] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 815
[2024-05-14 12:36:36,001] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 25%|██▌       | 815/3218 [28:39<1:25:06,  2.12s/it]
[2024-05-14 12:36:37,546] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 816
[2024-05-14 12:36:37,547] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 25%|██▌       | 817/3218 [28:41<1:12:02,  1.80s/it]
[2024-05-14 12:36:39,406] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 817
[2024-05-14 12:36:39,407] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 25%|██▌       | 818/3218 [28:43<1:13:18,  1.83s/it]
[2024-05-14 12:36:40,973] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 818
[2024-05-14 12:36:40,973] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 25%|██▌       | 819/3218 [28:45<1:09:53,  1.75s/it]
[2024-05-14 12:36:42,473] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 819
[2024-05-14 12:36:42,474] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:36:42,474] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
{'loss': 0.0, 'learning_rate': 2e-05, 'epoch': 0.25}
[2024-05-14 12:36:43,979] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 820
[2024-05-14 12:36:43,979] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 25%|██▌       | 820/3218 [28:46<1:06:15,  1.66s/it]
[2024-05-14 12:36:45,439] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 821
[2024-05-14 12:36:45,439] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 26%|██▌       | 822/3218 [28:49<1:02:59,  1.58s/it]
[2024-05-14 12:36:47,190] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 822
[2024-05-14 12:36:47,190] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 26%|██▌       | 823/3218 [28:51<1:05:01,  1.63s/it]
[2024-05-14 12:36:48,518] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 823
[2024-05-14 12:36:48,518] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:36:48,519] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 12:36:49,772] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 824
[2024-05-14 12:36:49,773] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 26%|██▌       | 824/3218 [28:52<1:00:46,  1.52s/it]
[2024-05-14 12:36:51,176] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 825
[2024-05-14 12:36:51,177] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 26%|██▌       | 826/3218 [28:55<57:41,  1.45s/it]
[2024-05-14 12:36:52,566] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 826
[2024-05-14 12:36:52,566] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:36:52,566] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 12:36:53,861] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 827
[2024-05-14 12:36:53,861] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 26%|██▌       | 827/3218 [28:57<56:40,  1.42s/it]
[2024-05-14 12:36:55,352] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 828
[2024-05-14 12:36:55,352] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 26%|██▌       | 829/3218 [28:59<56:10,  1.41s/it]
[2024-05-14 12:36:56,465] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 829
[2024-05-14 12:36:56,465] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:36:56,465] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
{'loss': 0.0, 'learning_rate': 2e-05, 'epoch': 0.26}
[2024-05-14 12:36:57,751] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 830
[2024-05-14 12:36:57,751] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 26%|██▌       | 830/3218 [29:00<52:25,  1.32s/it]
[2024-05-14 12:36:58,980] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 831
[2024-05-14 12:36:58,981] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 26%|██▌       | 832/3218 [29:03<51:34,  1.30s/it]
[2024-05-14 12:37:00,217] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 832
[2024-05-14 12:37:00,218] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:37:00,218] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 12:37:01,569] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 833
[2024-05-14 12:37:01,570] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 26%|██▌       | 834/3218 [29:05<51:10,  1.29s/it]
[2024-05-14 12:37:02,814] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 834
[2024-05-14 12:37:02,814] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:37:02,814] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 12:37:04,026] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 835
[2024-05-14 12:37:04,026] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 26%|██▌       | 835/3218 [29:07<50:47,  1.28s/it]
[2024-05-14 12:37:05,234] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 836
[2024-05-14 12:37:05,235] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 26%|██▌       | 837/3218 [29:09<49:26,  1.25s/it]
[2024-05-14 12:37:06,514] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 837
[2024-05-14 12:37:06,515] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:37:06,515] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 12:37:07,715] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 838
[2024-05-14 12:37:07,715] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 26%|██▌       | 838/3218 [29:10<50:02,  1.26s/it]
[2024-05-14 12:37:08,925] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 839
[2024-05-14 12:37:08,925] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:37:08,925] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25

 26%|██▌       | 840/3218 [29:13<48:20,  1.22s/it]
[2024-05-14 12:37:10,632] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 840
[2024-05-14 12:37:10,633] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 26%|██▌       | 841/3218 [29:15<54:08,  1.37s/it]
[2024-05-14 12:37:13,101] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 841
[2024-05-14 12:37:13,102] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:37:13,102] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 12:37:14,049] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 842
[2024-05-14 12:37:14,050] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 26%|██▌       | 842/3218 [29:17<1:07:41,  1.71s/it]
[2024-05-14 12:37:15,658] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 843
[2024-05-14 12:37:15,659] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 26%|██▌       | 844/3218 [29:20<1:01:08,  1.55s/it]
[2024-05-14 12:37:17,493] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 844
[2024-05-14 12:37:17,493] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 26%|██▋       | 845/3218 [29:21<1:04:46,  1.64s/it]
[2024-05-14 12:37:18,882] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 845
[2024-05-14 12:37:18,883] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25


 26%|██▋       | 847/3218 [29:25<1:04:05,  1.62s/it]
[2024-05-14 12:37:20,676] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 846
[2024-05-14 12:37:20,677] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 26%|██▋       | 849/3218 [29:28<1:01:22,  1.55s/it]
[2024-05-14 12:37:22,278] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 847
[2024-05-14 12:37:22,279] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:37:22,279] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 12:37:23,699] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 848
[2024-05-14 12:37:23,699] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 26%|██▋       | 850/3218 [29:29<1:03:16,  1.60s/it]
[2024-05-14 12:37:25,344] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 849
[2024-05-14 12:37:25,345] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:37:25,346] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25

 26%|██▋       | 851/3218 [29:31<1:04:49,  1.64s/it]
[2024-05-14 12:37:27,135] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 850
[2024-05-14 12:37:27,135] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 26%|██▋       | 852/3218 [29:33<1:05:59,  1.67s/it]
[2024-05-14 12:37:28,857] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 851
[2024-05-14 12:37:28,858] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 27%|██▋       | 853/3218 [29:34<1:03:05,  1.60s/it]
[2024-05-14 12:37:30,316] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 852
[2024-05-14 12:37:30,317] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:37:30,319] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 12:37:31,730] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 853
[2024-05-14 12:37:31,731] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 27%|██▋       | 855/3218 [29:37<1:01:46,  1.57s/it]
[2024-05-14 12:37:33,418] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 854
[2024-05-14 12:37:33,419] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 27%|██▋       | 856/3218 [29:39<1:02:31,  1.59s/it]
[2024-05-14 12:37:34,936] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 855
[2024-05-14 12:37:34,936] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 27%|██▋       | 857/3218 [29:41<1:03:09,  1.60s/it]
[2024-05-14 12:37:36,580] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 856
[2024-05-14 12:37:36,580] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 27%|██▋       | 859/3218 [29:44<1:00:34,  1.54s/it]
[2024-05-14 12:37:38,360] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 857
[2024-05-14 12:37:38,360] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:37:38,360] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 12:37:39,707] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 858
[2024-05-14 12:37:39,708] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 27%|██▋       | 860/3218 [29:45<1:00:39,  1.54s/it]
[2024-05-14 12:37:41,188] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 859
[2024-05-14 12:37:41,189] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:37:41,189] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25

 27%|██▋       | 861/3218 [29:46<57:52,  1.47s/it]
[2024-05-14 12:37:42,538] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 860
[2024-05-14 12:37:42,539] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:37:42,539] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 12:37:43,821] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 861
[2024-05-14 12:37:43,821] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 27%|██▋       | 863/3218 [29:49<57:56,  1.48s/it]
[2024-05-14 12:37:45,405] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 862
[2024-05-14 12:37:45,406] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 27%|██▋       | 864/3218 [29:51<1:03:34,  1.62s/it]
[2024-05-14 12:37:47,359] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 863
[2024-05-14 12:37:47,360] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 27%|██▋       | 865/3218 [29:53<1:02:17,  1.59s/it]
[2024-05-14 12:37:48,891] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 864
[2024-05-14 12:37:48,892] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:37:48,892] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 12:37:50,028] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 865
[2024-05-14 12:37:50,028] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 27%|██▋       | 867/3218 [29:55<52:49,  1.35s/it]
[2024-05-14 12:37:51,154] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 866
[2024-05-14 12:37:51,154] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 27%|██▋       | 869/3218 [29:58<51:33,  1.32s/it]
[2024-05-14 12:37:52,377] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 867
[2024-05-14 12:37:52,377] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:37:52,378] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 12:37:53,714] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 868
[2024-05-14 12:37:53,714] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 27%|██▋       | 870/3218 [29:59<50:56,  1.30s/it]
[2024-05-14 12:37:54,984] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 869
[2024-05-14 12:37:54,985] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:37:54,985] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25

 27%|██▋       | 872/3218 [30:02<50:59,  1.30s/it]
[2024-05-14 12:37:56,355] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 870
[2024-05-14 12:37:56,355] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:37:56,356] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 12:37:57,545] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 871
[2024-05-14 12:37:57,546] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 27%|██▋       | 873/3218 [30:03<50:15,  1.29s/it]
[2024-05-14 12:37:58,840] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 872
[2024-05-14 12:37:58,840] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:37:58,841] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 12:37:59,945] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 873
[2024-05-14 12:37:59,945] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 27%|██▋       | 875/3218 [30:05<44:18,  1.13s/it]
[2024-05-14 12:38:00,919] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 874
[2024-05-14 12:38:00,919] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 27%|██▋       | 876/3218 [30:07<51:48,  1.33s/it]
[2024-05-14 12:38:02,669] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 875
[2024-05-14 12:38:02,670] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 27%|██▋       | 877/3218 [30:09<1:06:56,  1.72s/it]
[2024-05-14 12:38:05,237] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 876
[2024-05-14 12:38:05,238] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:38:05,238] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 12:38:06,077] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 877
[2024-05-14 12:38:06,077] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 27%|██▋       | 879/3218 [30:11<55:44,  1.43s/it]
[2024-05-14 12:38:07,409] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 878
[2024-05-14 12:38:07,410] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 27%|██▋       | 880/3218 [30:13<55:49,  1.43s/it]
[2024-05-14 12:38:08,852] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 879
[2024-05-14 12:38:08,852] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:38:08,853] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25

 27%|██▋       | 881/3218 [30:15<1:08:08,  1.75s/it]
[2024-05-14 12:38:11,306] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 880
[2024-05-14 12:38:11,306] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 27%|██▋       | 882/3218 [30:17<1:05:59,  1.69s/it]
[2024-05-14 12:38:12,878] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 881
[2024-05-14 12:38:12,879] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 27%|██▋       | 883/3218 [30:19<1:10:17,  1.81s/it]
[2024-05-14 12:38:14,963] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 882
[2024-05-14 12:38:14,963] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 27%|██▋       | 884/3218 [30:21<1:09:36,  1.79s/it]
[2024-05-14 12:38:16,668] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 883
[2024-05-14 12:38:16,669] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:38:16,669] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 12:38:18,246] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 884
[2024-05-14 12:38:18,247] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 28%|██▊       | 885/3218 [30:22<1:06:12,  1.70s/it]
[2024-05-14 12:38:20,105] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 885
[2024-05-14 12:38:20,105] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 28%|██▊       | 887/3218 [30:25<1:04:25,  1.66s/it]
[2024-05-14 12:38:21,565] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 886
[2024-05-14 12:38:21,566] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 28%|██▊       | 888/3218 [30:27<1:00:12,  1.55s/it]
[2024-05-14 12:38:22,832] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 887
[2024-05-14 12:38:22,832] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:38:22,833] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 12:38:24,246] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 888
[2024-05-14 12:38:24,246] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 28%|██▊       | 889/3218 [30:28<58:44,  1.51s/it]
[2024-05-14 12:38:25,966] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 889
[2024-05-14 12:38:25,966] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:38:25,966] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25

 28%|██▊       | 890/3218 [30:30<1:01:14,  1.58s/it]
[2024-05-14 12:38:28,178] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 890
[2024-05-14 12:38:28,179] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 28%|██▊       | 892/3218 [30:34<1:06:20,  1.71s/it]
[2024-05-14 12:38:29,826] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 891
[2024-05-14 12:38:29,827] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 28%|██▊       | 893/3218 [30:36<1:08:39,  1.77s/it]
[2024-05-14 12:38:31,675] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 892
[2024-05-14 12:38:31,676] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 28%|██▊       | 894/3218 [30:37<1:06:08,  1.71s/it]
[2024-05-14 12:38:33,214] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 893
[2024-05-14 12:38:33,214] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 28%|██▊       | 895/3218 [30:39<1:05:37,  1.70s/it]
[2024-05-14 12:38:34,915] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 894
[2024-05-14 12:38:34,916] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 28%|██▊       | 896/3218 [30:40<1:04:01,  1.65s/it]
[2024-05-14 12:38:36,496] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 895
[2024-05-14 12:38:36,496] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:38:36,497] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 12:38:38,136] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 896
[2024-05-14 12:38:38,137] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 28%|██▊       | 898/3218 [30:43<1:00:07,  1.55s/it]
[2024-05-14 12:38:39,529] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 897
[2024-05-14 12:38:39,530] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 28%|██▊       | 899/3218 [30:45<56:04,  1.45s/it]
[2024-05-14 12:38:40,724] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 898
[2024-05-14 12:38:40,725] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:38:40,725] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 12:38:42,023] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 899
[2024-05-14 12:38:42,023] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:38:42,024] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
 28%|██▊       | 900/3218 [30:46<54:33,  1.41s/it][INFO|trainer.py:3166] 2024-05-14 12:38:42,213 >> ***** Running Evaluation *****
[INFO|trainer.py:3168] 2024-05-14 12:38:42,213 >>   Num examples = 521
[INFO|trainer.py:3171] 2024-05-14 12:38:42,213 >>   Batch size = 8






 97%|█████████▋| 32/33 [00:11<00:00,  2.78it/s]
{'eval_loss': nan, 'eval_runtime': 12.4203, 'eval_samples_per_second': 41.947, 'eval_steps_per_second': 2.657, 'epoch': 0.28}
[2024-05-14 12:38:55,716] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 900
[2024-05-14 12:38:55,717] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 28%|██▊       | 902/3218 [31:01<2:35:11,  4.02s/it]
[2024-05-14 12:38:57,211] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 901
[2024-05-14 12:38:57,212] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 28%|██▊       | 904/3218 [31:04<1:40:48,  2.61s/it]
[2024-05-14 12:38:58,505] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 902
[2024-05-14 12:38:58,505] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:38:58,506] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 12:38:59,708] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 903
[2024-05-14 12:38:59,708] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 28%|██▊       | 905/3218 [31:05<1:28:11,  2.29s/it]
[2024-05-14 12:39:01,254] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 904
[2024-05-14 12:39:01,255] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 28%|██▊       | 907/3218 [31:08<1:05:56,  1.71s/it]
[2024-05-14 12:39:02,587] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 905
[2024-05-14 12:39:02,588] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:39:02,588] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 12:39:03,617] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 906
[2024-05-14 12:39:03,618] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 28%|██▊       | 908/3218 [31:09<1:02:52,  1.63s/it]
[2024-05-14 12:39:05,064] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 907
[2024-05-14 12:39:05,065] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:39:05,065] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 12:39:06,365] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 908
[2024-05-14 12:39:06,365] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 28%|██▊       | 910/3218 [31:11<53:55,  1.40s/it]
[2024-05-14 12:39:07,478] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 909
[2024-05-14 12:39:07,479] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:39:07,479] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25

 28%|██▊       | 911/3218 [31:13<56:18,  1.46s/it]
[2024-05-14 12:39:09,071] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 910
[2024-05-14 12:39:09,072] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 28%|██▊       | 913/3218 [31:15<48:58,  1.27s/it]
[2024-05-14 12:39:10,694] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 911
[2024-05-14 12:39:10,695] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:39:10,695] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 12:39:11,592] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 912
[2024-05-14 12:39:11,592] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 28%|██▊       | 914/3218 [31:17<53:35,  1.40s/it]
[2024-05-14 12:39:13,087] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 913
[2024-05-14 12:39:13,088] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 28%|██▊       | 915/3218 [31:19<57:57,  1.51s/it]
[2024-05-14 12:39:14,864] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 914
[2024-05-14 12:39:14,865] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:39:14,865] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 12:39:16,359] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 915
[2024-05-14 12:39:16,359] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 28%|██▊       | 916/3218 [31:20<58:10,  1.52s/it]
[2024-05-14 12:39:18,074] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 916
[2024-05-14 12:39:18,074] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 29%|██▊       | 918/3218 [31:23<58:54,  1.54s/it]
[2024-05-14 12:39:19,536] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 917
[2024-05-14 12:39:19,537] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 29%|██▊       | 919/3218 [31:25<1:04:31,  1.68s/it]
[2024-05-14 12:39:21,529] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 918
[2024-05-14 12:39:21,530] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 29%|██▊       | 920/3218 [31:27<1:04:15,  1.68s/it]
[2024-05-14 12:39:23,224] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 919
[2024-05-14 12:39:23,225] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:39:23,225] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25

 29%|██▊       | 921/3218 [31:29<1:01:29,  1.61s/it]
[2024-05-14 12:39:24,691] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 920
[2024-05-14 12:39:24,692] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:39:24,692] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 12:39:26,087] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 921
[2024-05-14 12:39:26,088] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 29%|██▊       | 923/3218 [31:31<55:31,  1.45s/it]
[2024-05-14 12:39:27,293] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 922
[2024-05-14 12:39:27,294] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 29%|██▊       | 924/3218 [31:33<57:40,  1.51s/it]
[2024-05-14 12:39:28,933] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 923
[2024-05-14 12:39:28,934] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:39:28,934] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 12:39:30,274] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 924
[2024-05-14 12:39:30,275] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 29%|██▉       | 926/3218 [31:36<55:23,  1.45s/it]
[2024-05-14 12:39:31,707] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 925
[2024-05-14 12:39:31,707] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 29%|██▉       | 927/3218 [31:37<54:14,  1.42s/it]
[2024-05-14 12:39:33,073] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 926
[2024-05-14 12:39:33,074] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 29%|██▉       | 928/3218 [31:39<57:50,  1.52s/it]
[2024-05-14 12:39:34,768] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 927
[2024-05-14 12:39:34,768] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:39:34,769] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 12:39:36,161] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 928
[2024-05-14 12:39:36,166] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 29%|██▉       | 930/3218 [31:41<53:25,  1.40s/it]
[2024-05-14 12:39:37,445] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 929
[2024-05-14 12:39:37,445] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:39:37,445] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25

 29%|██▉       | 932/3218 [31:44<48:24,  1.27s/it]
[2024-05-14 12:39:38,775] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 930
[2024-05-14 12:39:38,776] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:39:38,777] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 12:39:39,780] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 931
[2024-05-14 12:39:39,780] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 29%|██▉       | 933/3218 [31:45<49:59,  1.31s/it]
[2024-05-14 12:39:41,193] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 932
[2024-05-14 12:39:41,194] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 29%|██▉       | 935/3218 [31:48<47:34,  1.25s/it]
[2024-05-14 12:39:42,477] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 933
[2024-05-14 12:39:42,478] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:39:42,478] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 12:39:43,581] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 934
[2024-05-14 12:39:43,582] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 29%|██▉       | 937/3218 [31:50<42:59,  1.13s/it]
[2024-05-14 12:39:44,823] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 935
[2024-05-14 12:39:44,824] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:39:44,824] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 12:39:45,729] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 936
[2024-05-14 12:39:45,730] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 29%|██▉       | 938/3218 [31:51<41:32,  1.09s/it]
[2024-05-14 12:39:46,715] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 937
[2024-05-14 12:39:46,716] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:39:46,716] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 12:39:48,314] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 938
[2024-05-14 12:39:48,314] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 29%|██▉       | 940/3218 [31:53<43:51,  1.16s/it]
[2024-05-14 12:39:49,276] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 939
[2024-05-14 12:39:49,276] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:39:49,277] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
{'loss': 0.0, 'learning_rate': 2e-05, 'epoch': 0.29}
[2024-05-14 12:39:50,370] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 940
[2024-05-14 12:39:50,371] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 29%|██▉       | 942/3218 [31:55<42:13,  1.11s/it]
[2024-05-14 12:39:51,420] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 941
[2024-05-14 12:39:51,420] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:39:51,420] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 12:39:52,309] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 942
[2024-05-14 12:39:52,309] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 29%|██▉       | 944/3218 [31:57<39:06,  1.03s/it]
[2024-05-14 12:39:53,313] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 943
[2024-05-14 12:39:53,314] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:39:53,314] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 12:39:54,399] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 944
[2024-05-14 12:39:54,399] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 29%|██▉       | 945/3218 [31:58<39:59,  1.06s/it]
[2024-05-14 12:39:56,164] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 945
[2024-05-14 12:39:56,165] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 29%|██▉       | 946/3218 [32:00<47:08,  1.24s/it]
[2024-05-14 12:39:58,074] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 946
[2024-05-14 12:39:58,075] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 29%|██▉       | 948/3218 [32:03<47:08,  1.25s/it]
[2024-05-14 12:39:58,991] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 947
[2024-05-14 12:39:58,991] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:39:58,991] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 12:40:00,196] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 948
[2024-05-14 12:40:00,197] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 29%|██▉       | 949/3218 [32:04<49:03,  1.30s/it]
[2024-05-14 12:40:01,959] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 949
[2024-05-14 12:40:01,959] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:40:01,960] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25

 30%|██▉       | 951/3218 [32:08<56:43,  1.50s/it]
[2024-05-14 12:40:03,668] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 950
[2024-05-14 12:40:03,669] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 30%|██▉       | 952/3218 [32:09<54:29,  1.44s/it]
[2024-05-14 12:40:04,997] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 951
[2024-05-14 12:40:04,998] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:40:05,000] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 12:40:06,437] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 952
[2024-05-14 12:40:06,437] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 30%|██▉       | 953/3218 [32:10<55:10,  1.46s/it]
[2024-05-14 12:40:08,235] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 953
[2024-05-14 12:40:08,235] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 30%|██▉       | 955/3218 [32:14<57:51,  1.53s/it]
[2024-05-14 12:40:09,734] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 954
[2024-05-14 12:40:09,734] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 30%|██▉       | 956/3218 [32:15<55:14,  1.47s/it]
[2024-05-14 12:40:11,089] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 955
[2024-05-14 12:40:11,090] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 30%|██▉       | 957/3218 [32:17<57:11,  1.52s/it]
[2024-05-14 12:40:12,710] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 956
[2024-05-14 12:40:12,711] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:40:12,712] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 12:40:14,264] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 957
[2024-05-14 12:40:14,264] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 30%|██▉       | 959/3218 [32:20<57:20,  1.52s/it]
[2024-05-14 12:40:15,729] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 958
[2024-05-14 12:40:15,730] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 30%|██▉       | 960/3218 [32:21<53:52,  1.43s/it]
[2024-05-14 12:40:16,964] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 959
[2024-05-14 12:40:16,965] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:40:16,967] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
{'loss': 0.0, 'learning_rate': 2e-05, 'epoch': 0.3}
[2024-05-14 12:40:18,381] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 960
[2024-05-14 12:40:18,381] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 30%|██▉       | 961/3218 [32:22<53:31,  1.42s/it]
[2024-05-14 12:40:20,081] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 961
[2024-05-14 12:40:20,082] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 30%|██▉       | 963/3218 [32:25<53:07,  1.41s/it]
[2024-05-14 12:40:21,269] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 962
[2024-05-14 12:40:21,270] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 30%|██▉       | 964/3218 [32:27<54:48,  1.46s/it]
[2024-05-14 12:40:22,849] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 963
[2024-05-14 12:40:22,850] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:40:22,851] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 12:40:24,213] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 964
[2024-05-14 12:40:24,213] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 30%|███       | 966/3218 [32:29<50:12,  1.34s/it]
[2024-05-14 12:40:25,390] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 965
[2024-05-14 12:40:25,390] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 30%|███       | 967/3218 [32:31<54:35,  1.45s/it]
[2024-05-14 12:40:26,917] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 966
[2024-05-14 12:40:26,918] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:40:26,918] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 12:40:28,505] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 967
[2024-05-14 12:40:28,506] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 30%|███       | 969/3218 [32:33<50:02,  1.33s/it]
[2024-05-14 12:40:29,586] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 968
[2024-05-14 12:40:29,586] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 30%|███       | 970/3218 [32:35<48:27,  1.29s/it]
[2024-05-14 12:40:30,744] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 969
[2024-05-14 12:40:30,745] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:40:30,746] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
{'loss': 0.0, 'learning_rate': 2e-05, 'epoch': 0.3}
[2024-05-14 12:40:32,014] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 970
[2024-05-14 12:40:32,014] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 30%|███       | 972/3218 [32:37<48:22,  1.29s/it]
[2024-05-14 12:40:33,314] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 971
[2024-05-14 12:40:33,314] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:40:33,314] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 12:40:34,475] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 972
[2024-05-14 12:40:34,475] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 30%|███       | 974/3218 [32:40<47:16,  1.26s/it]
[2024-05-14 12:40:35,767] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 973
[2024-05-14 12:40:35,768] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 30%|███       | 976/3218 [32:42<43:38,  1.17s/it]
[2024-05-14 12:40:36,849] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 974
[2024-05-14 12:40:36,849] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:40:36,850] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 12:40:37,945] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 975
[2024-05-14 12:40:37,946] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 30%|███       | 977/3218 [32:43<43:23,  1.16s/it]
[2024-05-14 12:40:39,096] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 976
[2024-05-14 12:40:39,097] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:40:39,097] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 12:40:40,167] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 977
[2024-05-14 12:40:40,167] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 30%|███       | 979/3218 [32:45<42:24,  1.14s/it]
[2024-05-14 12:40:41,292] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 978
[2024-05-14 12:40:41,293] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:40:41,293] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 12:40:42,271] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 979
[2024-05-14 12:40:42,272] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:40:42,272] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25

 30%|███       | 981/3218 [32:48<44:13,  1.19s/it]
[2024-05-14 12:40:43,729] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 980
[2024-05-14 12:40:43,729] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 31%|███       | 982/3218 [32:50<53:08,  1.43s/it]
[2024-05-14 12:40:45,553] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 981
[2024-05-14 12:40:45,554] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:40:45,554] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 12:40:46,488] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 982
[2024-05-14 12:40:46,488] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 31%|███       | 984/3218 [32:52<48:00,  1.29s/it]
[2024-05-14 12:40:47,810] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 983
[2024-05-14 12:40:47,810] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 31%|███       | 985/3218 [32:54<56:10,  1.51s/it]
[2024-05-14 12:40:49,805] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 984
[2024-05-14 12:40:49,806] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 31%|███       | 986/3218 [32:55<57:03,  1.53s/it]
[2024-05-14 12:40:51,412] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 985
[2024-05-14 12:40:51,413] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 31%|███       | 987/3218 [32:57<59:56,  1.61s/it]
[2024-05-14 12:40:53,179] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 986
[2024-05-14 12:40:53,180] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:40:53,182] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 12:40:54,544] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 987
[2024-05-14 12:40:54,544] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 31%|███       | 989/3218 [33:00<55:58,  1.51s/it]
[2024-05-14 12:40:55,992] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 988
[2024-05-14 12:40:55,993] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:40:55,993] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 12:40:58,253] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 989
[2024-05-14 12:40:58,253] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:40:58,254] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25

 31%|███       | 990/3218 [33:02<1:04:23,  1.73s/it]
[2024-05-14 12:41:00,185] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 990
[2024-05-14 12:41:00,186] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 31%|███       | 992/3218 [33:06<1:03:39,  1.72s/it]
[2024-05-14 12:41:01,748] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 991
[2024-05-14 12:41:01,748] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:41:01,749] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 12:41:04,024] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 992
[2024-05-14 12:41:04,024] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 31%|███       | 994/3218 [33:09<1:05:46,  1.77s/it]
[2024-05-14 12:41:05,526] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 993
[2024-05-14 12:41:05,526] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 31%|███       | 995/3218 [33:11<1:00:58,  1.65s/it]
[2024-05-14 12:41:06,929] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 994
[2024-05-14 12:41:06,930] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:41:06,931] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 12:41:08,207] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 995
[2024-05-14 12:41:08,208] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 31%|███       | 997/3218 [33:14<56:14,  1.52s/it]
[2024-05-14 12:41:09,655] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 996
[2024-05-14 12:41:09,655] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 31%|███       | 998/3218 [33:15<53:52,  1.46s/it]
[2024-05-14 12:41:10,994] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 997
[2024-05-14 12:41:10,995] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:41:10,995] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 12:41:12,229] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 998
[2024-05-14 12:41:12,230] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
 31%|███       | 1000/3218 [33:17<50:04,  1.35s/it][INFO|trainer.py:3166] 2024-05-14 12:41:13,666 >> ***** Running Evaluation *****
[INFO|trainer.py:3168] 2024-05-14 12:41:13,666 >>   Num examples = 521
[INFO|trainer.py:3171] 2024-05-14 12:41:13,666 >>   Batch size = 8
  6%|▌         | 2/33 [00:00<00:03,  8.45it/s]
[2024-05-14 12:41:13,504] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 999
[2024-05-14 12:41:13,505] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:41:13,505] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25






 88%|████████▊ | 29/33 [00:10<00:01,  2.80it/s]
 31%|███       | 1000/3218 [33:30<50:04,  1.35s[INFO|trainer.py:2889] 2024-05-14 12:41:31,261 >> Saving model checkpoint to outputs/instruct_tuning/model/tmp-checkpoint-1000
[INFO|configuration_utils.py:483] 2024-05-14 12:41:31,264 >> Configuration saved in outputs/instruct_tuning/model/tmp-checkpoint-1000/config.json
[INFO|configuration_utils.py:594] 2024-05-14 12:41:31,266 >> Configuration saved in outputs/instruct_tuning/model/tmp-checkpoint-1000/generation_config.json
[INFO|modeling_utils.py:2390] 2024-05-14 12:42:06,318 >> The model is bigger than the maximum size per checkpoint (5GB) and is going to be split in 3 checkpoint shards. You can find where each parameters has been saved in the index located at outputs/instruct_tuning/model/tmp-checkpoint-1000/model.safetensors.index.json.
[INFO|tokenization_utils_base.py:2432] 2024-05-14 12:42:06,320 >> tokenizer config file saved in outputs/instruct_tuning/model/tmp-checkpoint-1000/tokenizer_config.json
[INFO|tokenization_utils_base.py:2441] 2024-05-14 12:42:06,321 >> Special tokens file saved in outputs/instruct_tuning/model/tmp-checkpoint-1000/special_tokens_map.json
/mounts/work/lyk/anaconda3/envs/torch2/lib/python3.8/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
[2024-05-14 12:42:07,208] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step1000 is about to be saved!
[2024-05-14 12:42:07,215] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: outputs/instruct_tuning/model/tmp-checkpoint-1000/global_step1000/mp_rank_00_model_states.pt
[2024-05-14 12:42:07,216] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving outputs/instruct_tuning/model/tmp-checkpoint-1000/global_step1000/mp_rank_00_model_states.pt...
[INFO|trainer.py:2979] 2024-05-14 12:43:14,793 >> Deleting older checkpoint [outputs/instruct_tuning/model/checkpoint-600] due to args.save_total_limit
[2024-05-14 12:43:14,774] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved outputs/instruct_tuning/model/tmp-checkpoint-1000/global_step1000/mp_rank_00_model_states.pt.
[2024-05-14 12:43:14,777] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step1000 is ready now!
 31%|███       | 1001/3218 [35:22<23:35:10, 38.30s/it]
[2024-05-14 12:43:17,985] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1000
[2024-05-14 12:43:17,986] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 31%|███       | 1002/3218 [35:24<16:50:02, 27.35s/it]
[2024-05-14 12:43:19,768] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1001
[2024-05-14 12:43:19,768] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 31%|███       | 1003/3218 [35:25<12:01:30, 19.54s/it]
[2024-05-14 12:43:21,096] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1002
[2024-05-14 12:43:21,096] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 31%|███       | 1005/3218 [35:27<6:16:14, 10.20s/it]
[2024-05-14 12:43:22,392] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1003
[2024-05-14 12:43:22,393] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:43:22,393] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 12:43:23,577] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1004
[2024-05-14 12:43:23,577] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 31%|███▏      | 1007/3218 [35:30<3:27:09,  5.62s/it]
[2024-05-14 12:43:24,797] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1005
[2024-05-14 12:43:24,798] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:43:24,798] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 12:43:26,015] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1006
[2024-05-14 12:43:26,015] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 31%|███▏      | 1008/3218 [35:31<2:38:32,  4.30s/it]
[2024-05-14 12:43:27,236] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1007
[2024-05-14 12:43:27,237] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 31%|███▏      | 1010/3218 [35:34<1:44:18,  2.83s/it]
[2024-05-14 12:43:28,958] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1008
[2024-05-14 12:43:28,959] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:43:28,959] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 12:43:30,168] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1009
[2024-05-14 12:43:30,168] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 31%|███▏      | 1011/3218 [35:35<1:25:25,  2.32s/it]
{'loss': 0.0, 'learning_rate': 2e-05, 'epoch': 0.31}
[2024-05-14 12:43:31,321] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1010
[2024-05-14 12:43:31,321] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 31%|███▏      | 1013/3218 [35:38<1:04:00,  1.74s/it]
[2024-05-14 12:43:32,553] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1011
[2024-05-14 12:43:32,554] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:43:32,554] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 12:43:33,712] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1012
[2024-05-14 12:43:33,713] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 32%|███▏      | 1015/3218 [35:40<50:02,  1.36s/it]
[2024-05-14 12:43:34,819] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1013
[2024-05-14 12:43:34,819] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:43:34,820] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 12:43:35,775] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1014
[2024-05-14 12:43:35,775] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 32%|███▏      | 1016/3218 [35:42<56:04,  1.53s/it]
[2024-05-14 12:43:37,672] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1015
[2024-05-14 12:43:37,673] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 32%|███▏      | 1017/3218 [35:44<1:06:02,  1.80s/it]
[2024-05-14 12:43:40,043] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1016
[2024-05-14 12:43:40,043] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 32%|███▏      | 1019/3218 [35:46<51:29,  1.40s/it]
[2024-05-14 12:43:40,846] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1017
[2024-05-14 12:43:40,846] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:43:40,847] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 12:43:41,997] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1018
[2024-05-14 12:43:41,997] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 32%|███▏      | 1020/3218 [35:48<53:55,  1.47s/it]
[2024-05-14 12:43:43,585] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1019
[2024-05-14 12:43:43,586] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:43:43,587] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25

 32%|███▏      | 1021/3218 [35:50<1:02:27,  1.71s/it]
[2024-05-14 12:43:45,829] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1020
[2024-05-14 12:43:45,829] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 32%|███▏      | 1022/3218 [35:51<1:00:08,  1.64s/it]
[2024-05-14 12:43:47,341] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1021
[2024-05-14 12:43:47,342] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 32%|███▏      | 1023/3218 [35:53<59:36,  1.63s/it]
[2024-05-14 12:43:48,910] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1022
[2024-05-14 12:43:48,911] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 32%|███▏      | 1024/3218 [35:55<1:02:09,  1.70s/it]
[2024-05-14 12:43:50,820] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1023
[2024-05-14 12:43:50,821] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:43:50,821] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 12:43:52,237] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1024
[2024-05-14 12:43:52,238] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 32%|███▏      | 1026/3218 [35:58<57:22,  1.57s/it]
[2024-05-14 12:43:53,726] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1025
[2024-05-14 12:43:53,727] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 32%|███▏      | 1027/3218 [35:59<55:02,  1.51s/it]
[2024-05-14 12:43:55,106] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1026
[2024-05-14 12:43:55,107] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 32%|███▏      | 1029/3218 [36:02<55:45,  1.53s/it]
[2024-05-14 12:43:56,426] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1027
[2024-05-14 12:43:56,427] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:43:56,428] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 12:43:58,095] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1028
[2024-05-14 12:43:58,096] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 32%|███▏      | 1030/3218 [36:04<58:27,  1.60s/it]
[2024-05-14 12:43:59,884] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1029
[2024-05-14 12:43:59,884] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:43:59,885] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25

 32%|███▏      | 1031/3218 [36:05<55:11,  1.51s/it]
[2024-05-14 12:44:01,178] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1030
[2024-05-14 12:44:01,179] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 32%|███▏      | 1032/3218 [36:06<53:11,  1.46s/it]
[2024-05-14 12:44:02,546] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1031
[2024-05-14 12:44:02,547] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:44:02,547] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 12:44:04,343] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1032
[2024-05-14 12:44:04,344] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 32%|███▏      | 1034/3218 [36:10<59:12,  1.63s/it]
[2024-05-14 12:44:06,096] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1033
[2024-05-14 12:44:06,097] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 32%|███▏      | 1035/3218 [36:12<1:00:36,  1.67s/it]
[2024-05-14 12:44:07,862] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1034
[2024-05-14 12:44:07,862] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 32%|███▏      | 1036/3218 [36:13<56:24,  1.55s/it]
[2024-05-14 12:44:09,172] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1035
[2024-05-14 12:44:09,173] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 32%|███▏      | 1038/3218 [36:16<51:55,  1.43s/it]
[2024-05-14 12:44:10,589] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1036
[2024-05-14 12:44:10,589] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:44:10,589] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 12:44:11,852] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1037
[2024-05-14 12:44:11,853] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 32%|███▏      | 1039/3218 [36:17<49:01,  1.35s/it]
[2024-05-14 12:44:13,020] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1038
[2024-05-14 12:44:13,020] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:44:13,022] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 12:44:14,293] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1039
[2024-05-14 12:44:14,294] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 32%|███▏      | 1041/3218 [36:20<49:57,  1.38s/it]
{'loss': 0.0, 'learning_rate': 2e-05, 'epoch': 0.32}
[2024-05-14 12:44:15,800] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1040
[2024-05-14 12:44:15,800] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 32%|███▏      | 1043/3218 [36:22<45:57,  1.27s/it]
[2024-05-14 12:44:16,905] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1041
[2024-05-14 12:44:16,905] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:44:16,905] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 12:44:18,087] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1042
[2024-05-14 12:44:18,087] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 32%|███▏      | 1044/3218 [36:23<45:05,  1.24s/it]
[2024-05-14 12:44:19,284] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1043
[2024-05-14 12:44:19,284] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 33%|███▎      | 1046/3218 [36:26<45:17,  1.25s/it]
[2024-05-14 12:44:20,590] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1044
[2024-05-14 12:44:20,590] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:44:20,591] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 12:44:21,801] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1045
[2024-05-14 12:44:21,801] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 33%|███▎      | 1048/3218 [36:28<41:10,  1.14s/it]
[2024-05-14 12:44:22,991] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1046
[2024-05-14 12:44:22,991] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:44:22,992] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 12:44:23,964] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1047
[2024-05-14 12:44:23,964] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 33%|███▎      | 1049/3218 [36:29<42:30,  1.18s/it]
[2024-05-14 12:44:25,188] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1048
[2024-05-14 12:44:25,188] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 33%|███▎      | 1050/3218 [36:30<43:12,  1.20s/it]
[2024-05-14 12:44:26,460] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1049
[2024-05-14 12:44:26,461] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:44:26,461] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25

 33%|███▎      | 1051/3218 [36:33<57:55,  1.60s/it]
[2024-05-14 12:44:29,011] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1050
[2024-05-14 12:44:29,012] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 33%|███▎      | 1053/3218 [36:35<48:30,  1.34s/it]
[2024-05-14 12:44:30,616] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1051
[2024-05-14 12:44:30,616] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:44:30,617] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 12:44:31,514] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1052
[2024-05-14 12:44:31,515] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 33%|███▎      | 1054/3218 [36:37<51:33,  1.43s/it]
[2024-05-14 12:44:32,962] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1053
[2024-05-14 12:44:32,963] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 33%|███▎      | 1055/3218 [36:39<55:40,  1.54s/it]
[2024-05-14 12:44:34,715] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1054
[2024-05-14 12:44:34,715] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 33%|███▎      | 1056/3218 [36:41<1:00:58,  1.69s/it]
[2024-05-14 12:44:36,824] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1055
[2024-05-14 12:44:36,825] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 33%|███▎      | 1057/3218 [36:43<1:06:42,  1.85s/it]
[2024-05-14 12:44:38,968] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1056
[2024-05-14 12:44:38,968] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:44:38,968] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 12:44:40,418] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1057
[2024-05-14 12:44:40,419] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 33%|███▎      | 1059/3218 [36:46<59:51,  1.66s/it]
[2024-05-14 12:44:41,910] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1058
[2024-05-14 12:44:41,910] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 33%|███▎      | 1060/3218 [36:47<59:08,  1.64s/it]
[2024-05-14 12:44:43,563] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1059
[2024-05-14 12:44:43,564] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:44:43,564] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25

 33%|███▎      | 1061/3218 [36:49<57:41,  1.60s/it]
[2024-05-14 12:44:45,103] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1060
[2024-05-14 12:44:45,103] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 33%|███▎      | 1062/3218 [36:51<59:59,  1.67s/it]
[2024-05-14 12:44:46,890] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1061
[2024-05-14 12:44:46,891] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:44:46,891] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 12:44:48,422] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1062
[2024-05-14 12:44:48,423] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 33%|███▎      | 1064/3218 [36:54<53:57,  1.50s/it]
[2024-05-14 12:44:49,623] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1063
[2024-05-14 12:44:49,624] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 33%|███▎      | 1065/3218 [36:55<54:24,  1.52s/it]
[2024-05-14 12:44:51,177] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1064
[2024-05-14 12:44:51,177] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 33%|███▎      | 1066/3218 [36:57<1:01:31,  1.72s/it]
[2024-05-14 12:44:53,355] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1065
[2024-05-14 12:44:53,356] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 33%|███▎      | 1068/3218 [37:00<52:10,  1.46s/it]
[2024-05-14 12:44:54,743] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1066
[2024-05-14 12:44:54,744] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:44:54,745] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 12:44:55,831] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1067
[2024-05-14 12:44:55,832] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 33%|███▎      | 1069/3218 [37:01<52:41,  1.47s/it]
[2024-05-14 12:44:57,297] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1068
[2024-05-14 12:44:57,297] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 33%|███▎      | 1071/3218 [37:04<48:52,  1.37s/it]
[2024-05-14 12:44:58,582] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1069
[2024-05-14 12:44:58,583] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:44:58,585] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
{'loss': 0.0, 'learning_rate': 2e-05, 'epoch': 0.33}
[2024-05-14 12:44:59,847] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1070
[2024-05-14 12:44:59,847] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 33%|███▎      | 1072/3218 [37:06<57:32,  1.61s/it]
[2024-05-14 12:45:02,013] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1071
[2024-05-14 12:45:02,013] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 33%|███▎      | 1073/3218 [37:07<53:21,  1.49s/it]
[2024-05-14 12:45:03,264] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1072
[2024-05-14 12:45:03,265] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 33%|███▎      | 1075/3218 [37:10<50:10,  1.40s/it]
[2024-05-14 12:45:04,513] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1073
[2024-05-14 12:45:04,514] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:45:04,514] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 12:45:05,853] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1074
[2024-05-14 12:45:05,854] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 33%|███▎      | 1076/3218 [37:11<48:37,  1.36s/it]
[2024-05-14 12:45:07,105] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1075
[2024-05-14 12:45:07,105] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 33%|███▎      | 1078/3218 [37:14<50:39,  1.42s/it]
[2024-05-14 12:45:08,909] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1076
[2024-05-14 12:45:08,909] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:45:08,909] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 12:45:10,168] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1077
[2024-05-14 12:45:10,168] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 34%|███▎      | 1079/3218 [37:15<48:27,  1.36s/it]
[2024-05-14 12:45:11,428] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1078
[2024-05-14 12:45:11,428] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 34%|███▎      | 1081/3218 [37:18<44:48,  1.26s/it]
[2024-05-14 12:45:12,541] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1079
[2024-05-14 12:45:12,542] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:45:12,542] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
{'loss': 0.0, 'learning_rate': 2e-05, 'epoch': 0.34}
[2024-05-14 12:45:13,722] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1080
[2024-05-14 12:45:13,722] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 34%|███▎      | 1083/3218 [37:20<42:32,  1.20s/it]
[2024-05-14 12:45:14,896] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1081
[2024-05-14 12:45:14,897] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:45:14,897] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 12:45:16,018] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1082
[2024-05-14 12:45:16,018] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 34%|███▎      | 1085/3218 [37:22<39:38,  1.12s/it]
[2024-05-14 12:45:17,136] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1083
[2024-05-14 12:45:17,137] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:45:17,137] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 12:45:18,131] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1084
[2024-05-14 12:45:18,131] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 34%|███▎      | 1086/3218 [37:23<39:33,  1.11s/it]
[2024-05-14 12:45:19,250] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1085
[2024-05-14 12:45:19,251] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 34%|███▍      | 1088/3218 [37:25<38:35,  1.09s/it]
[2024-05-14 12:45:20,655] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1086
[2024-05-14 12:45:20,656] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:45:20,656] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 12:45:21,642] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1087
[2024-05-14 12:45:21,642] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 34%|███▍      | 1089/3218 [37:27<42:28,  1.20s/it]
[2024-05-14 12:45:22,873] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1088
[2024-05-14 12:45:22,873] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:45:22,874] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 12:45:24,382] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1089
[2024-05-14 12:45:24,383] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 34%|███▍      | 1091/3218 [37:30<48:14,  1.36s/it]
{'loss': 0.0, 'learning_rate': 2e-05, 'epoch': 0.34}
[2024-05-14 12:45:25,917] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1090
[2024-05-14 12:45:25,918] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 34%|███▍      | 1092/3218 [37:31<49:30,  1.40s/it]
[2024-05-14 12:45:27,415] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1091
[2024-05-14 12:45:27,415] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 34%|███▍      | 1093/3218 [37:33<50:41,  1.43s/it]
[2024-05-14 12:45:28,936] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1092
[2024-05-14 12:45:28,937] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:45:28,937] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 12:45:30,508] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1093
[2024-05-14 12:45:30,509] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 34%|███▍      | 1095/3218 [37:36<54:10,  1.53s/it]
[2024-05-14 12:45:32,129] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1094
[2024-05-14 12:45:32,130] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 34%|███▍      | 1096/3218 [37:38<59:52,  1.69s/it]
[2024-05-14 12:45:34,162] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1095
[2024-05-14 12:45:34,163] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 34%|███▍      | 1097/3218 [37:40<56:51,  1.61s/it]
[2024-05-14 12:45:35,692] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1096
[2024-05-14 12:45:35,693] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 34%|███▍      | 1098/3218 [37:41<59:10,  1.67s/it]
[2024-05-14 12:45:37,431] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1097
[2024-05-14 12:45:37,432] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 34%|███▍      | 1099/3218 [37:43<1:01:07,  1.73s/it]
[2024-05-14 12:45:39,310] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1098
[2024-05-14 12:45:39,311] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
 34%|███▍      | 1100/3218 [37:45<58:19,  1.65s/it][INFO|trainer.py:3166] 2024-05-14 12:45:41,026 >> ***** Running Evaluation *****
[INFO|trainer.py:3168] 2024-05-14 12:45:41,026 >>   Num examples = 521
[INFO|trainer.py:3171] 2024-05-14 12:45:41,026 >>   Batch size = 8
 12%|█▏        | 4/33 [00:01<00:10,  2.69it/s]
[2024-05-14 12:45:40,801] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1099
[2024-05-14 12:45:40,802] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:45:40,813] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25






 97%|█████████▋| 32/33 [00:11<00:00,  2.78it/s]

 34%|███▍      | 1102/3218 [38:00<2:27:20,  4.18s/it]
[2024-05-14 12:45:54,786] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1100
[2024-05-14 12:45:54,786] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:45:54,786] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 12:45:56,236] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1101
[2024-05-14 12:45:56,236] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 34%|███▍      | 1103/3218 [38:02<2:03:23,  3.50s/it]
[2024-05-14 12:45:58,153] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1102
[2024-05-14 12:45:58,154] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 34%|███▍      | 1104/3218 [38:04<1:44:08,  2.96s/it]
[2024-05-14 12:45:59,782] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1103
[2024-05-14 12:45:59,782] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 34%|███▍      | 1105/3218 [38:05<1:27:11,  2.48s/it]
[2024-05-14 12:46:01,184] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1104
[2024-05-14 12:46:01,185] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:46:01,185] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 12:46:02,494] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1105
[2024-05-14 12:46:02,495] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 34%|███▍      | 1107/3218 [38:08<1:04:34,  1.84s/it]
[2024-05-14 12:46:03,662] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1106
[2024-05-14 12:46:03,662] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 34%|███▍      | 1109/3218 [38:10<53:14,  1.51s/it]
[2024-05-14 12:46:04,922] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1107
[2024-05-14 12:46:04,923] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:46:04,923] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 12:46:06,125] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1108
[2024-05-14 12:46:06,125] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 34%|███▍      | 1110/3218 [38:11<51:09,  1.46s/it]
[2024-05-14 12:46:07,417] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1109
[2024-05-14 12:46:07,418] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:46:07,418] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
{'loss': 0.0, 'learning_rate': 2e-05, 'epoch': 0.34}
[2024-05-14 12:46:08,527] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1110
[2024-05-14 12:46:08,527] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 35%|███▍      | 1112/3218 [38:14<52:36,  1.50s/it]
[2024-05-14 12:46:10,301] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1111
[2024-05-14 12:46:10,301] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 35%|███▍      | 1113/3218 [38:16<50:56,  1.45s/it]
[2024-05-14 12:46:11,704] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1112
[2024-05-14 12:46:11,704] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 35%|███▍      | 1115/3218 [38:18<46:32,  1.33s/it]
[2024-05-14 12:46:12,913] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1113
[2024-05-14 12:46:12,913] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:46:12,913] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 12:46:14,186] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1114
[2024-05-14 12:46:14,186] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 35%|███▍      | 1117/3218 [38:20<41:45,  1.19s/it]
[2024-05-14 12:46:15,288] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1115
[2024-05-14 12:46:15,288] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:46:15,288] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 12:46:16,331] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1116
[2024-05-14 12:46:16,331] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 35%|███▍      | 1118/3218 [38:21<40:01,  1.14s/it]
[2024-05-14 12:46:17,377] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1117
[2024-05-14 12:46:17,378] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 35%|███▍      | 1120/3218 [38:24<39:46,  1.14s/it]
[2024-05-14 12:46:18,770] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1118
[2024-05-14 12:46:18,770] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:46:18,771] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 12:46:19,718] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1119
[2024-05-14 12:46:19,718] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:46:19,718] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25

 35%|███▍      | 1122/3218 [38:26<42:28,  1.22s/it]
[2024-05-14 12:46:20,847] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1120
[2024-05-14 12:46:20,847] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:46:20,848] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 12:46:22,200] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1121
[2024-05-14 12:46:22,201] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 35%|███▍      | 1124/3218 [38:28<39:27,  1.13s/it]
[2024-05-14 12:46:23,061] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1122
[2024-05-14 12:46:23,062] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:46:23,062] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 12:46:24,225] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1123
[2024-05-14 12:46:24,226] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 35%|███▍      | 1125/3218 [38:30<43:04,  1.23s/it]
[2024-05-14 12:46:25,702] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1124
[2024-05-14 12:46:25,702] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 35%|███▍      | 1126/3218 [38:31<45:14,  1.30s/it]
[2024-05-14 12:46:27,102] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1125
[2024-05-14 12:46:27,103] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:46:27,106] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 12:46:28,537] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1126
[2024-05-14 12:46:28,537] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 35%|███▌      | 1128/3218 [38:34<48:30,  1.39s/it]
[2024-05-14 12:46:30,065] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1127
[2024-05-14 12:46:30,065] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 35%|███▌      | 1129/3218 [38:35<48:01,  1.38s/it]
[2024-05-14 12:46:31,412] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1128
[2024-05-14 12:46:31,412] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 35%|███▌      | 1131/3218 [38:38<47:14,  1.36s/it]
[2024-05-14 12:46:32,805] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1129
[2024-05-14 12:46:32,806] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:46:32,806] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
{'loss': 0.0, 'learning_rate': 2e-05, 'epoch': 0.35}
[2024-05-14 12:46:34,127] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1130
[2024-05-14 12:46:34,127] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 35%|███▌      | 1132/3218 [38:39<45:23,  1.31s/it]
[2024-05-14 12:46:35,316] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1131
[2024-05-14 12:46:35,316] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:46:35,317] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 12:46:36,601] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1132
[2024-05-14 12:46:36,601] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 35%|███▌      | 1134/3218 [38:42<46:51,  1.35s/it]
[2024-05-14 12:46:38,040] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1133
[2024-05-14 12:46:38,040] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 35%|███▌      | 1135/3218 [38:44<52:10,  1.50s/it]
[2024-05-14 12:46:39,922] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1134
[2024-05-14 12:46:39,923] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 35%|███▌      | 1136/3218 [38:45<53:13,  1.53s/it]
[2024-05-14 12:46:41,485] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1135
[2024-05-14 12:46:41,486] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 35%|███▌      | 1138/3218 [38:48<46:46,  1.35s/it]
[2024-05-14 12:46:42,864] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1136
[2024-05-14 12:46:42,865] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:46:42,866] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 12:46:43,945] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1137
[2024-05-14 12:46:43,945] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 35%|███▌      | 1139/3218 [38:49<44:47,  1.29s/it]
[2024-05-14 12:46:45,087] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1138
[2024-05-14 12:46:45,087] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 35%|███▌      | 1141/3218 [38:52<49:50,  1.44s/it]
[2024-05-14 12:46:46,955] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1139
[2024-05-14 12:46:46,955] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:46:46,956] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
{'loss': 0.0, 'learning_rate': 2e-05, 'epoch': 0.35}
[2024-05-14 12:46:48,294] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1140
[2024-05-14 12:46:48,294] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 35%|███▌      | 1142/3218 [38:54<49:44,  1.44s/it]
[2024-05-14 12:46:49,774] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1141
[2024-05-14 12:46:49,775] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 36%|███▌      | 1144/3218 [38:56<44:09,  1.28s/it]
[2024-05-14 12:46:50,940] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1142
[2024-05-14 12:46:50,941] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:46:50,941] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 12:46:52,038] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1143
[2024-05-14 12:46:52,039] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 36%|███▌      | 1145/3218 [38:58<48:01,  1.39s/it]
[2024-05-14 12:46:53,620] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1144
[2024-05-14 12:46:53,620] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 36%|███▌      | 1147/3218 [39:00<44:22,  1.29s/it]
[2024-05-14 12:46:54,861] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1145
[2024-05-14 12:46:54,862] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:46:54,862] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 12:46:56,037] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1146
[2024-05-14 12:46:56,038] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 36%|███▌      | 1149/3218 [39:02<42:18,  1.23s/it]
[2024-05-14 12:46:57,080] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1147
[2024-05-14 12:46:57,081] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:46:57,081] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 12:46:58,325] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1148
[2024-05-14 12:46:58,326] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 36%|███▌      | 1151/3218 [39:04<39:38,  1.15s/it]
[2024-05-14 12:46:59,310] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1149
[2024-05-14 12:46:59,310] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:46:59,310] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
{'loss': 0.0, 'learning_rate': 2e-05, 'epoch': 0.36}
[2024-05-14 12:47:00,436] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1150
[2024-05-14 12:47:00,436] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 36%|███▌      | 1152/3218 [39:06<42:45,  1.24s/it]
[2024-05-14 12:47:01,871] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1151
[2024-05-14 12:47:01,872] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 36%|███▌      | 1154/3218 [39:08<40:08,  1.17s/it]
[2024-05-14 12:47:02,940] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1152
[2024-05-14 12:47:02,941] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:47:02,941] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 12:47:04,089] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1153
[2024-05-14 12:47:04,090] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 36%|███▌      | 1155/3218 [39:09<40:13,  1.17s/it]
[2024-05-14 12:47:05,270] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1154
[2024-05-14 12:47:05,271] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 36%|███▌      | 1156/3218 [39:11<48:44,  1.42s/it]
[2024-05-14 12:47:07,298] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1155
[2024-05-14 12:47:07,299] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 36%|███▌      | 1158/3218 [39:14<47:38,  1.39s/it]
[2024-05-14 12:47:09,405] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1156
[2024-05-14 12:47:09,405] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:47:09,406] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 12:47:10,360] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1157
[2024-05-14 12:47:10,360] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 36%|███▌      | 1159/3218 [39:16<51:28,  1.50s/it]
[2024-05-14 12:47:11,956] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1158
[2024-05-14 12:47:11,957] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 36%|███▌      | 1160/3218 [39:18<52:51,  1.54s/it]
[2024-05-14 12:47:13,542] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1159
[2024-05-14 12:47:13,543] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:47:13,546] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25

 36%|███▌      | 1161/3218 [39:19<53:48,  1.57s/it]
[2024-05-14 12:47:15,275] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1160
[2024-05-14 12:47:15,276] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 36%|███▌      | 1163/3218 [39:22<52:41,  1.54s/it]
[2024-05-14 12:47:16,795] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1161
[2024-05-14 12:47:16,796] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:47:16,796] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 12:47:18,292] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1162
[2024-05-14 12:47:18,292] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 36%|███▌      | 1164/3218 [39:24<52:02,  1.52s/it]
[2024-05-14 12:47:19,716] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1163
[2024-05-14 12:47:19,716] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 36%|███▌      | 1165/3218 [39:25<53:50,  1.57s/it]
[2024-05-14 12:47:21,419] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1164
[2024-05-14 12:47:21,420] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:47:21,420] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 12:47:22,707] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1165
[2024-05-14 12:47:22,708] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 36%|███▋      | 1167/3218 [39:28<47:41,  1.40s/it]
[2024-05-14 12:47:23,936] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1166
[2024-05-14 12:47:23,937] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 36%|███▋      | 1168/3218 [39:29<46:19,  1.36s/it]
[2024-05-14 12:47:25,146] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1167
[2024-05-14 12:47:25,147] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 36%|███▋      | 1169/3218 [39:31<55:28,  1.62s/it]
[2024-05-14 12:47:27,354] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1168
[2024-05-14 12:47:27,354] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 36%|███▋      | 1171/3218 [39:34<51:52,  1.52s/it]
[2024-05-14 12:47:28,821] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1169
[2024-05-14 12:47:28,822] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:47:28,822] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
{'loss': 0.0, 'learning_rate': 2e-05, 'epoch': 0.36}
[2024-05-14 12:47:30,251] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1170
[2024-05-14 12:47:30,251] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 36%|███▋      | 1172/3218 [39:36<53:31,  1.57s/it]
[2024-05-14 12:47:31,933] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1171
[2024-05-14 12:47:31,933] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 36%|███▋      | 1173/3218 [39:37<49:29,  1.45s/it]
[2024-05-14 12:47:33,142] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1172
[2024-05-14 12:47:33,143] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:47:33,143] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 12:47:34,651] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1173
[2024-05-14 12:47:34,651] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 37%|███▋      | 1175/3218 [39:40<48:56,  1.44s/it]
[2024-05-14 12:47:35,994] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1174
[2024-05-14 12:47:35,995] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 37%|███▋      | 1177/3218 [39:42<41:29,  1.22s/it]
[2024-05-14 12:47:37,128] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1175
[2024-05-14 12:47:37,129] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:47:37,129] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 12:47:38,059] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1176
[2024-05-14 12:47:38,060] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 37%|███▋      | 1179/3218 [39:44<41:11,  1.21s/it]
[2024-05-14 12:47:39,285] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1177
[2024-05-14 12:47:39,286] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:47:39,286] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 12:47:40,494] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1178
[2024-05-14 12:47:40,495] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 37%|███▋      | 1180/3218 [39:46<41:20,  1.22s/it]
[2024-05-14 12:47:41,704] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1179
[2024-05-14 12:47:41,705] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:47:41,705] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
{'loss': 0.0, 'learning_rate': 2e-05, 'epoch': 0.37}
[2024-05-14 12:47:42,801] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1180
[2024-05-14 12:47:42,802] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 37%|███▋      | 1182/3218 [39:48<40:48,  1.20s/it]
[2024-05-14 12:47:44,067] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1181
[2024-05-14 12:47:44,067] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 37%|███▋      | 1184/3218 [39:50<42:19,  1.25s/it]
[2024-05-14 12:47:45,086] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1182
[2024-05-14 12:47:45,087] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:47:45,087] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 12:47:46,529] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1183
[2024-05-14 12:47:46,530] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 37%|███▋      | 1186/3218 [39:52<36:48,  1.09s/it]
[2024-05-14 12:47:47,615] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1184
[2024-05-14 12:47:47,615] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:47:47,615] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 12:47:48,469] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1185
[2024-05-14 12:47:48,470] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 37%|███▋      | 1188/3218 [39:55<36:36,  1.08s/it]
[2024-05-14 12:47:49,579] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1186
[2024-05-14 12:47:49,580] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:47:49,580] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 12:47:50,644] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1187
[2024-05-14 12:47:50,645] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 37%|███▋      | 1189/3218 [39:56<36:57,  1.09s/it]
[2024-05-14 12:47:51,775] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1188
[2024-05-14 12:47:51,775] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:47:51,776] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 12:47:52,837] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1189
[2024-05-14 12:47:52,837] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 37%|███▋      | 1191/3218 [39:58<41:17,  1.22s/it]
{'loss': 0.0, 'learning_rate': 2e-05, 'epoch': 0.37}
[2024-05-14 12:47:54,385] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1190
[2024-05-14 12:47:54,385] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 37%|███▋      | 1193/3218 [40:00<37:39,  1.12s/it]
[2024-05-14 12:47:55,842] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1191
[2024-05-14 12:47:55,843] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:47:55,843] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 12:47:56,709] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1192
[2024-05-14 12:47:56,710] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 37%|███▋      | 1194/3218 [40:02<39:13,  1.16s/it]
[2024-05-14 12:47:57,803] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1193
[2024-05-14 12:47:57,803] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 37%|███▋      | 1195/3218 [40:03<42:06,  1.25s/it]
[2024-05-14 12:47:59,244] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1194
[2024-05-14 12:47:59,245] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 37%|███▋      | 1196/3218 [40:06<54:08,  1.61s/it]
[2024-05-14 12:48:01,675] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1195
[2024-05-14 12:48:01,676] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 37%|███▋      | 1197/3218 [40:07<54:27,  1.62s/it]
[2024-05-14 12:48:03,343] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1196
[2024-05-14 12:48:03,344] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:48:03,345] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 12:48:04,873] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1197
[2024-05-14 12:48:04,873] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 37%|███▋      | 1199/3218 [40:10<52:01,  1.55s/it]
[2024-05-14 12:48:06,294] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1198
[2024-05-14 12:48:06,294] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
 37%|███▋      | 1200/3218 [40:12<58:49,  1.75s/it][INFO|trainer.py:3166] 2024-05-14 12:48:08,720 >> ***** Running Evaluation *****
[INFO|trainer.py:3168] 2024-05-14 12:48:08,720 >>   Num examples = 521
[INFO|trainer.py:3171] 2024-05-14 12:48:08,721 >>   Batch size = 8
  0%|          | 0/33 [00:00<?, ?it/s]
[2024-05-14 12:48:08,511] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1199
[2024-05-14 12:48:08,511] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:48:08,511] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25







100%|██████████| 33/33 [00:11<00:00,  2.31it/s]
 37%|███▋      | 1200/3218 [40:25<58:49,  1.75s[INFO|trainer.py:2889] 2024-05-14 12:48:26,326 >> Saving model checkpoint to outputs/instruct_tuning/model/tmp-checkpoint-1200
[INFO|configuration_utils.py:483] 2024-05-14 12:48:26,328 >> Configuration saved in outputs/instruct_tuning/model/tmp-checkpoint-1200/config.json
[INFO|configuration_utils.py:594] 2024-05-14 12:48:26,329 >> Configuration saved in outputs/instruct_tuning/model/tmp-checkpoint-1200/generation_config.json
[INFO|modeling_utils.py:2390] 2024-05-14 12:48:57,637 >> The model is bigger than the maximum size per checkpoint (5GB) and is going to be split in 3 checkpoint shards. You can find where each parameters has been saved in the index located at outputs/instruct_tuning/model/tmp-checkpoint-1200/model.safetensors.index.json.
[INFO|tokenization_utils_base.py:2432] 2024-05-14 12:48:57,639 >> tokenizer config file saved in outputs/instruct_tuning/model/tmp-checkpoint-1200/tokenizer_config.json
[INFO|tokenization_utils_base.py:2441] 2024-05-14 12:48:57,639 >> Special tokens file saved in outputs/instruct_tuning/model/tmp-checkpoint-1200/special_tokens_map.json
/mounts/work/lyk/anaconda3/envs/torch2/lib/python3.8/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
[2024-05-14 12:48:58,456] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step1200 is about to be saved!
[2024-05-14 12:48:58,463] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: outputs/instruct_tuning/model/tmp-checkpoint-1200/global_step1200/mp_rank_00_model_states.pt
[2024-05-14 12:48:58,463] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving outputs/instruct_tuning/model/tmp-checkpoint-1200/global_step1200/mp_rank_00_model_states.pt...
[INFO|trainer.py:2979] 2024-05-14 12:50:17,763 >> Deleting older checkpoint [outputs/instruct_tuning/model/checkpoint-800] due to args.save_total_limit
[2024-05-14 12:50:17,748] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved outputs/instruct_tuning/model/tmp-checkpoint-1200/global_step1200/mp_rank_00_model_states.pt.
[2024-05-14 12:50:17,750] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step1200 is ready now!
 37%|███▋      | 1202/3218 [42:26<16:17:54, 29.10s/it]
[2024-05-14 12:50:21,083] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1200
[2024-05-14 12:50:21,084] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:50:21,084] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 12:50:22,488] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1201
[2024-05-14 12:50:22,488] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 37%|███▋      | 1203/3218 [42:28<11:40:03, 20.85s/it]
[2024-05-14 12:50:23,994] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1202
[2024-05-14 12:50:23,995] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 37%|███▋      | 1205/3218 [42:30<6:04:03, 10.85s/it]
[2024-05-14 12:50:25,222] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1203
[2024-05-14 12:50:25,223] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:50:25,223] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 12:50:26,549] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1204
[2024-05-14 12:50:26,550] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 37%|███▋      | 1206/3218 [42:32<4:27:23,  7.97s/it]
[2024-05-14 12:50:27,769] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1205
[2024-05-14 12:50:27,769] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 38%|███▊      | 1207/3218 [42:33<3:22:55,  6.05s/it]
[2024-05-14 12:50:29,369] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1206
[2024-05-14 12:50:29,370] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:50:29,370] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 12:50:30,958] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1207
[2024-05-14 12:50:30,959] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 38%|███▊      | 1209/3218 [42:36<2:05:38,  3.75s/it]
[2024-05-14 12:50:32,332] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1208
[2024-05-14 12:50:32,333] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 38%|███▊      | 1210/3218 [42:38<1:42:37,  3.07s/it]
[2024-05-14 12:50:33,886] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1209
[2024-05-14 12:50:33,887] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:50:33,887] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25

 38%|███▊      | 1212/3218 [42:41<1:15:16,  2.25s/it]
[2024-05-14 12:50:35,164] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1210
[2024-05-14 12:50:35,164] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:50:35,165] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 12:50:36,707] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1211
[2024-05-14 12:50:36,708] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 38%|███▊      | 1213/3218 [42:42<1:05:28,  1.96s/it]
[2024-05-14 12:50:38,033] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1212
[2024-05-14 12:50:38,034] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 38%|███▊      | 1215/3218 [42:44<53:12,  1.59s/it]
[2024-05-14 12:50:39,096] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1213
[2024-05-14 12:50:39,097] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:50:39,097] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 12:50:40,445] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1214
[2024-05-14 12:50:40,446] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 38%|███▊      | 1217/3218 [42:47<45:33,  1.37s/it]
[2024-05-14 12:50:41,727] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1215
[2024-05-14 12:50:41,728] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:50:41,728] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 12:50:42,799] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1216
[2024-05-14 12:50:42,799] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 38%|███▊      | 1218/3218 [42:48<45:57,  1.38s/it]
[2024-05-14 12:50:44,209] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1217
[2024-05-14 12:50:44,210] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 38%|███▊      | 1220/3218 [42:50<39:59,  1.20s/it]
[2024-05-14 12:50:45,113] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1218
[2024-05-14 12:50:45,114] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:50:45,114] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 12:50:46,237] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1219
[2024-05-14 12:50:46,237] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:50:46,238] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25

 38%|███▊      | 1222/3218 [42:53<41:52,  1.26s/it]
[2024-05-14 12:50:47,301] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1220
[2024-05-14 12:50:47,302] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:50:47,303] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 12:50:48,763] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1221
[2024-05-14 12:50:48,764] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 38%|███▊      | 1223/3218 [42:54<38:18,  1.15s/it]
[2024-05-14 12:50:49,684] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1222
[2024-05-14 12:50:49,685] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:50:49,687] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 12:50:50,999] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1223
[2024-05-14 12:50:51,000] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 38%|███▊      | 1225/3218 [42:56<37:10,  1.12s/it]
[2024-05-14 12:50:51,985] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1224
[2024-05-14 12:50:51,986] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 38%|███▊      | 1226/3218 [42:57<40:58,  1.23s/it]
[2024-05-14 12:50:53,498] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1225
[2024-05-14 12:50:53,499] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 38%|███▊      | 1228/3218 [43:00<37:53,  1.14s/it]
[2024-05-14 12:50:55,215] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1226
[2024-05-14 12:50:55,216] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:50:55,216] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 12:50:55,932] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1227
[2024-05-14 12:50:55,932] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 38%|███▊      | 1229/3218 [43:01<41:36,  1.26s/it]
[2024-05-14 12:50:57,263] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1228
[2024-05-14 12:50:57,264] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:50:57,266] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 12:50:59,031] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1229
[2024-05-14 12:50:59,032] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 38%|███▊      | 1230/3218 [43:03<46:44,  1.41s/it]

 38%|███▊      | 1231/3218 [43:05<54:25,  1.64s/it]
[2024-05-14 12:51:01,204] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1230
[2024-05-14 12:51:01,204] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:51:01,206] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25

 38%|███▊      | 1233/3218 [43:09<54:50,  1.66s/it]
[2024-05-14 12:51:03,089] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:51:03,090] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 12:51:04,620] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1232
[2024-05-14 12:51:04,620] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 38%|███▊      | 1234/3218 [43:11<58:57,  1.78s/it]
[2024-05-14 12:51:06,616] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1233
[2024-05-14 12:51:06,617] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 38%|███▊      | 1235/3218 [43:12<57:11,  1.73s/it]
[2024-05-14 12:51:08,326] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1234
[2024-05-14 12:51:08,327] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 38%|███▊      | 1236/3218 [43:14<53:23,  1.62s/it]
[2024-05-14 12:51:09,692] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1235
[2024-05-14 12:51:09,693] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 38%|███▊      | 1237/3218 [43:15<52:26,  1.59s/it]
[2024-05-14 12:51:11,197] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1236
[2024-05-14 12:51:11,198] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:51:11,198] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 12:51:13,005] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1237
[2024-05-14 12:51:13,005] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 39%|███▊      | 1239/3218 [43:19<54:06,  1.64s/it]
[2024-05-14 12:51:14,594] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1238
[2024-05-14 12:51:14,594] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 39%|███▊      | 1240/3218 [43:20<51:23,  1.56s/it]
[2024-05-14 12:51:15,997] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1239
[2024-05-14 12:51:15,998] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:51:15,999] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25

 39%|███▊      | 1242/3218 [43:23<48:54,  1.49s/it]
[2024-05-14 12:51:17,490] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1240
[2024-05-14 12:51:17,491] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:51:17,491] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 12:51:18,857] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1241
[2024-05-14 12:51:18,857] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 39%|███▊      | 1243/3218 [43:24<49:32,  1.51s/it]
[2024-05-14 12:51:20,418] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1242
[2024-05-14 12:51:20,418] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 39%|███▊      | 1244/3218 [43:26<47:02,  1.43s/it]
[2024-05-14 12:51:21,672] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1243
[2024-05-14 12:51:21,673] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:51:21,673] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 12:51:22,973] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1244
[2024-05-14 12:51:22,973] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 39%|███▊      | 1246/3218 [43:28<43:04,  1.31s/it]
[2024-05-14 12:51:24,128] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1245
[2024-05-14 12:51:24,129] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 39%|███▉      | 1247/3218 [43:30<44:54,  1.37s/it]
[2024-05-14 12:51:25,538] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1246
[2024-05-14 12:51:25,539] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:51:25,539] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 12:51:26,929] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1247
[2024-05-14 12:51:26,929] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 39%|███▉      | 1249/3218 [43:32<45:18,  1.38s/it]
[2024-05-14 12:51:28,289] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1248
[2024-05-14 12:51:28,290] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 39%|███▉      | 1250/3218 [43:34<46:17,  1.41s/it]
[2024-05-14 12:51:29,796] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1249
[2024-05-14 12:51:29,796] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:51:29,797] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25

 39%|███▉      | 1252/3218 [43:36<45:01,  1.37s/it]
[2024-05-14 12:51:31,253] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1250
[2024-05-14 12:51:31,253] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:51:31,254] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 12:51:32,537] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1251
[2024-05-14 12:51:32,538] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 39%|███▉      | 1253/3218 [43:38<43:05,  1.32s/it]
[2024-05-14 12:51:33,726] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1252
[2024-05-14 12:51:33,727] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:51:33,727] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 12:51:34,918] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1253
[2024-05-14 12:51:34,919] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 39%|███▉      | 1255/3218 [43:40<40:01,  1.22s/it]
[2024-05-14 12:51:36,008] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1254
[2024-05-14 12:51:36,008] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:51:36,010] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 12:51:37,088] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1255
[2024-05-14 12:51:37,088] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 39%|███▉      | 1257/3218 [43:42<39:12,  1.20s/it]
[2024-05-14 12:51:38,351] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1256
[2024-05-14 12:51:38,351] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 39%|███▉      | 1259/3218 [43:45<39:48,  1.22s/it]
[2024-05-14 12:51:39,788] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1257
[2024-05-14 12:51:39,788] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:51:39,788] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 12:51:40,890] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1258
[2024-05-14 12:51:40,890] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 39%|███▉      | 1260/3218 [43:46<36:26,  1.12s/it]
[2024-05-14 12:51:41,812] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1259
[2024-05-14 12:51:41,813] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:51:41,813] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25

 39%|███▉      | 1262/3218 [43:49<43:47,  1.34s/it]
[2024-05-14 12:51:43,247] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1260
[2024-05-14 12:51:43,248] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:51:43,248] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 12:51:44,887] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1261
[2024-05-14 12:51:44,888] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 39%|███▉      | 1263/3218 [43:49<36:15,  1.11s/it]
[2024-05-14 12:51:45,613] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1262
[2024-05-14 12:51:45,614] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 39%|███▉      | 1264/3218 [43:51<44:33,  1.37s/it]
[2024-05-14 12:51:47,343] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1263
[2024-05-14 12:51:47,344] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:51:47,344] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 12:51:49,067] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1264
[2024-05-14 12:51:49,067] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 39%|███▉      | 1266/3218 [43:55<48:24,  1.49s/it]
[2024-05-14 12:51:50,601] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1265
[2024-05-14 12:51:50,601] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 39%|███▉      | 1267/3218 [43:56<49:25,  1.52s/it]
[2024-05-14 12:51:52,231] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1266
[2024-05-14 12:51:52,231] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 39%|███▉      | 1268/3218 [43:57<46:53,  1.44s/it]
[2024-05-14 12:51:53,480] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1267
[2024-05-14 12:51:53,481] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:51:53,482] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 12:51:55,144] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1268
[2024-05-14 12:51:55,144] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25


 39%|███▉      | 1271/3218 [44:03<52:58,  1.63s/it]
[2024-05-14 12:51:57,341] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1269
[2024-05-14 12:51:57,342] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:51:57,348] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
{'loss': 0.0, 'learning_rate': 2e-05, 'epoch': 0.39}
[2024-05-14 12:51:58,786] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1270
[2024-05-14 12:51:58,787] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 40%|███▉      | 1272/3218 [44:04<50:30,  1.56s/it]
[2024-05-14 12:52:00,150] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1271
[2024-05-14 12:52:00,150] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 40%|███▉      | 1273/3218 [44:06<49:15,  1.52s/it]
[2024-05-14 12:52:01,571] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1272
[2024-05-14 12:52:01,571] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:52:01,572] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 12:52:03,064] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1273
[2024-05-14 12:52:03,065] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 40%|███▉      | 1275/3218 [44:09<52:21,  1.62s/it]
[2024-05-14 12:52:04,873] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1274
[2024-05-14 12:52:04,873] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 40%|███▉      | 1276/3218 [44:10<51:42,  1.60s/it]
[2024-05-14 12:52:06,480] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1275
[2024-05-14 12:52:06,481] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 40%|███▉      | 1278/3218 [44:13<45:38,  1.41s/it]
[2024-05-14 12:52:07,658] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1276
[2024-05-14 12:52:07,659] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:52:07,659] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 12:52:08,949] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1277
[2024-05-14 12:52:08,949] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 40%|███▉      | 1279/3218 [44:15<47:56,  1.48s/it]
[2024-05-14 12:52:10,536] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1278
[2024-05-14 12:52:10,536] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 40%|███▉      | 1280/3218 [44:16<47:12,  1.46s/it]
[2024-05-14 12:52:11,978] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1279
[2024-05-14 12:52:11,978] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:52:11,979] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25

 40%|███▉      | 1282/3218 [44:19<46:16,  1.43s/it]
[2024-05-14 12:52:13,321] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1280
[2024-05-14 12:52:13,321] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:52:13,321] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 12:52:14,767] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1281
[2024-05-14 12:52:14,768] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 40%|███▉      | 1283/3218 [44:20<45:21,  1.41s/it]
[2024-05-14 12:52:16,143] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1282
[2024-05-14 12:52:16,143] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 40%|███▉      | 1285/3218 [44:23<43:17,  1.34s/it]
[2024-05-14 12:52:17,498] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1283
[2024-05-14 12:52:17,499] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:52:17,499] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 12:52:18,733] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1284
[2024-05-14 12:52:18,733] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 40%|███▉      | 1286/3218 [44:24<43:11,  1.34s/it]
[2024-05-14 12:52:20,053] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1285
[2024-05-14 12:52:20,054] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 40%|████      | 1288/3218 [44:26<39:16,  1.22s/it]
[2024-05-14 12:52:21,374] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1286
[2024-05-14 12:52:21,374] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:52:21,375] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 12:52:22,357] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1287
[2024-05-14 12:52:22,358] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 40%|████      | 1290/3218 [44:29<38:01,  1.18s/it]
[2024-05-14 12:52:23,551] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1288
[2024-05-14 12:52:23,551] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:52:23,551] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 12:52:24,674] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1289
[2024-05-14 12:52:24,674] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:52:24,674] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25

 40%|████      | 1291/3218 [44:30<39:23,  1.23s/it]
[2024-05-14 12:52:26,000] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1290
[2024-05-14 12:52:26,000] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:52:26,001] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 12:52:27,051] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1291
[2024-05-14 12:52:27,051] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 40%|████      | 1293/3218 [44:32<36:16,  1.13s/it]
[2024-05-14 12:52:28,101] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1292
[2024-05-14 12:52:28,101] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:52:28,102] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 12:52:29,157] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1293
[2024-05-14 12:52:29,157] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 40%|████      | 1295/3218 [44:34<32:35,  1.02s/it]
[2024-05-14 12:52:29,983] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1294
[2024-05-14 12:52:29,983] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 40%|████      | 1296/3218 [44:35<37:54,  1.18s/it]
[2024-05-14 12:52:31,538] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1295
[2024-05-14 12:52:31,539] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 40%|████      | 1298/3218 [44:38<38:53,  1.22s/it]
[2024-05-14 12:52:33,534] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1296
[2024-05-14 12:52:33,534] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:52:33,535] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 12:52:34,386] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1297
[2024-05-14 12:52:34,387] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 40%|████      | 1299/3218 [44:39<39:13,  1.23s/it]
[2024-05-14 12:52:35,517] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1298
[2024-05-14 12:52:35,518] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
 40%|████      | 1300/3218 [44:41<46:16,  1.45s/it][INFO|trainer.py:3166] 2024-05-14 12:52:37,614 >> ***** Running Evaluation *****
[INFO|trainer.py:3168] 2024-05-14 12:52:37,614 >>   Num examples = 521
[INFO|trainer.py:3171] 2024-05-14 12:52:37,614 >>   Batch size = 8
 12%|█▏        | 4/33 [00:01<00:10,  2.69it/s]
[2024-05-14 12:52:37,432] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1299
[2024-05-14 12:52:37,433] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:52:37,434] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25






 97%|█████████▋| 32/33 [00:11<00:00,  2.78it/s]
{'eval_loss': nan, 'eval_runtime': 12.4047, 'eval_samples_per_second': 42.0, 'eval_steps_per_second': 2.66, 'epoch': 0.4}
[2024-05-14 12:52:51,087] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1300
[2024-05-14 12:52:51,088] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 40%|████      | 1301/3218 [44:55<2:43:51,  5.13s/it]
[2024-05-14 12:52:53,036] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1301
[2024-05-14 12:52:53,037] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 40%|████      | 1303/3218 [44:59<1:48:42,  3.41s/it]
[2024-05-14 12:52:54,581] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1302
[2024-05-14 12:52:54,582] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 41%|████      | 1304/3218 [45:00<1:28:41,  2.78s/it]
[2024-05-14 12:52:56,003] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1303
[2024-05-14 12:52:56,003] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:52:56,004] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 12:52:57,362] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1304
[2024-05-14 12:52:57,362] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 41%|████      | 1306/3218 [45:03<1:05:56,  2.07s/it]
[2024-05-14 12:52:58,817] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1305
[2024-05-14 12:52:58,817] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 41%|████      | 1307/3218 [45:05<1:03:47,  2.00s/it]
[2024-05-14 12:53:00,566] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1306
[2024-05-14 12:53:00,567] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 41%|████      | 1308/3218 [45:06<57:21,  1.80s/it]
[2024-05-14 12:53:01,967] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1307
[2024-05-14 12:53:01,967] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:53:01,968] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25

 41%|████      | 1309/3218 [45:07<54:07,  1.70s/it]
[2024-05-14 12:53:03,386] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:53:03,386] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 12:53:05,350] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1309
[2024-05-14 12:53:05,350] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 41%|████      | 1310/3218 [45:09<56:09,  1.77s/it]
{'loss': 0.0, 'learning_rate': 2e-05, 'epoch': 0.41}
[2024-05-14 12:53:07,361] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1310
[2024-05-14 12:53:07,362] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 41%|████      | 1311/3218 [45:11<58:57,  1.86s/it]
[2024-05-14 12:53:09,155] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1311
[2024-05-14 12:53:09,156] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 41%|████      | 1313/3218 [45:15<55:03,  1.73s/it]
[2024-05-14 12:53:10,685] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1312
[2024-05-14 12:53:10,685] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 41%|████      | 1314/3218 [45:16<51:41,  1.63s/it]
[2024-05-14 12:53:12,083] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1313
[2024-05-14 12:53:12,084] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:53:12,084] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 12:53:13,277] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1314
[2024-05-14 12:53:13,277] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 41%|████      | 1316/3218 [45:18<45:14,  1.43s/it]
[2024-05-14 12:53:14,520] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1315
[2024-05-14 12:53:14,521] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 41%|████      | 1317/3218 [45:20<47:16,  1.49s/it]
[2024-05-14 12:53:16,181] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1316
[2024-05-14 12:53:16,182] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 41%|████      | 1318/3218 [45:21<45:01,  1.42s/it]
[2024-05-14 12:53:17,464] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1317
[2024-05-14 12:53:17,464] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:53:17,464] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 12:53:19,153] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1318
[2024-05-14 12:53:19,154] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 41%|████      | 1320/3218 [45:24<44:15,  1.40s/it]
[2024-05-14 12:53:20,308] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1319
[2024-05-14 12:53:20,309] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:53:20,309] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25

 41%|████      | 1322/3218 [45:27<40:43,  1.29s/it]
[2024-05-14 12:53:21,475] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1320
[2024-05-14 12:53:21,476] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:53:21,476] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 12:53:22,664] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1321
[2024-05-14 12:53:22,664] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 41%|████      | 1324/3218 [45:29<37:24,  1.19s/it]
[2024-05-14 12:53:23,833] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1322
[2024-05-14 12:53:23,834] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:53:23,834] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 12:53:24,855] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1323
[2024-05-14 12:53:24,855] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 41%|████      | 1326/3218 [45:31<35:34,  1.13s/it]
[2024-05-14 12:53:25,996] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1324
[2024-05-14 12:53:25,997] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:53:25,997] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 12:53:27,051] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1325
[2024-05-14 12:53:27,051] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 41%|████      | 1327/3218 [45:32<33:35,  1.07s/it]
[2024-05-14 12:53:27,982] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1326
[2024-05-14 12:53:27,982] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:53:27,982] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 12:53:29,215] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1327
[2024-05-14 12:53:29,215] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 41%|████▏     | 1329/3218 [45:34<33:18,  1.06s/it]
[2024-05-14 12:53:30,145] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1328
[2024-05-14 12:53:30,145] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:53:30,145] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 12:53:31,307] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1329
[2024-05-14 12:53:31,308] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:53:31,308] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25

 41%|████▏     | 1330/3218 [45:35<33:59,  1.08s/it]
[2024-05-14 12:53:33,276] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1330
[2024-05-14 12:53:33,276] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 41%|████▏     | 1332/3218 [45:39<44:05,  1.40s/it]
[2024-05-14 12:53:34,796] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1331
[2024-05-14 12:53:34,797] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 41%|████▏     | 1334/3218 [45:41<40:15,  1.28s/it]
[2024-05-14 12:53:35,776] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1332
[2024-05-14 12:53:35,776] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:53:35,776] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 12:53:37,026] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1333
[2024-05-14 12:53:37,026] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 41%|████▏     | 1335/3218 [45:43<43:52,  1.40s/it]
[2024-05-14 12:53:38,672] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1334
[2024-05-14 12:53:38,673] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 42%|████▏     | 1336/3218 [45:44<47:27,  1.51s/it]
[2024-05-14 12:53:40,486] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1335
[2024-05-14 12:53:40,487] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 42%|████▏     | 1337/3218 [45:46<47:54,  1.53s/it]
[2024-05-14 12:53:42,032] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1336
[2024-05-14 12:53:42,032] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 42%|████▏     | 1338/3218 [45:48<50:13,  1.60s/it]
[2024-05-14 12:53:43,781] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1337
[2024-05-14 12:53:43,781] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:53:43,791] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 12:53:45,334] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1338
[2024-05-14 12:53:45,335] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 42%|████▏     | 1340/3218 [45:51<50:11,  1.60s/it]
[2024-05-14 12:53:46,964] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1339
[2024-05-14 12:53:46,965] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:53:46,965] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25

 42%|████▏     | 1341/3218 [45:52<48:03,  1.54s/it]
[2024-05-14 12:53:48,410] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1340
[2024-05-14 12:53:48,411] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 42%|████▏     | 1342/3218 [45:54<48:54,  1.56s/it]
[2024-05-14 12:53:49,937] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1341
[2024-05-14 12:53:49,938] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 42%|████▏     | 1344/3218 [45:57<46:44,  1.50s/it]
[2024-05-14 12:53:51,591] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1342
[2024-05-14 12:53:51,591] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:53:51,591] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 12:53:52,893] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1343
[2024-05-14 12:53:52,893] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 42%|████▏     | 1345/3218 [45:58<45:53,  1.47s/it]
[2024-05-14 12:53:54,317] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1344
[2024-05-14 12:53:54,318] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 42%|████▏     | 1346/3218 [46:00<45:42,  1.47s/it]
[2024-05-14 12:53:55,783] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1345
[2024-05-14 12:53:55,784] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:53:55,785] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 12:53:57,118] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1346
[2024-05-14 12:53:57,119] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 42%|████▏     | 1348/3218 [46:02<44:24,  1.42s/it]
[2024-05-14 12:53:58,542] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1347
[2024-05-14 12:53:58,543] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 42%|████▏     | 1349/3218 [46:04<43:28,  1.40s/it]
[2024-05-14 12:53:59,875] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1348
[2024-05-14 12:53:59,875] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:53:59,875] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 12:54:01,440] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1349
[2024-05-14 12:54:01,440] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 42%|████▏     | 1351/3218 [46:07<44:59,  1.45s/it]
{'loss': 0.0, 'learning_rate': 2e-05, 'epoch': 0.42}
[2024-05-14 12:54:02,894] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1350
[2024-05-14 12:54:02,894] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 42%|████▏     | 1352/3218 [46:08<45:37,  1.47s/it]
[2024-05-14 12:54:04,419] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1351
[2024-05-14 12:54:04,420] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:54:04,420] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 12:54:05,537] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1352
[2024-05-14 12:54:05,537] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 42%|████▏     | 1354/3218 [46:11<40:20,  1.30s/it]
[2024-05-14 12:54:06,693] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1353
[2024-05-14 12:54:06,694] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 42%|████▏     | 1356/3218 [46:13<37:17,  1.20s/it]
[2024-05-14 12:54:07,945] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1354
[2024-05-14 12:54:07,946] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:54:07,948] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 12:54:08,953] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1355
[2024-05-14 12:54:08,954] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 42%|████▏     | 1357/3218 [46:14<38:57,  1.26s/it]
[2024-05-14 12:54:10,314] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1356
[2024-05-14 12:54:10,315] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:54:10,315] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 12:54:11,505] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1357
[2024-05-14 12:54:11,506] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 42%|████▏     | 1359/3218 [46:17<37:37,  1.21s/it]
[2024-05-14 12:54:12,618] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1358
[2024-05-14 12:54:12,619] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 42%|████▏     | 1361/3218 [46:19<37:27,  1.21s/it]
[2024-05-14 12:54:13,765] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1359
[2024-05-14 12:54:13,766] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:54:13,766] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
{'loss': 0.0, 'learning_rate': 2e-05, 'epoch': 0.42}
[2024-05-14 12:54:15,034] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1360
[2024-05-14 12:54:15,035] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 42%|████▏     | 1362/3218 [46:20<36:24,  1.18s/it]
[2024-05-14 12:54:16,113] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1361
[2024-05-14 12:54:16,113] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:54:16,113] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 12:54:17,260] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1362
[2024-05-14 12:54:17,261] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 42%|████▏     | 1364/3218 [46:22<36:01,  1.17s/it]
[2024-05-14 12:54:18,445] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1363
[2024-05-14 12:54:18,446] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:54:18,446] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 12:54:19,456] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1364
[2024-05-14 12:54:19,456] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 42%|████▏     | 1365/3218 [46:23<34:09,  1.11s/it]
[2024-05-14 12:54:21,439] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1365
[2024-05-14 12:54:21,440] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 42%|████▏     | 1366/3218 [46:25<42:29,  1.38s/it]
[2024-05-14 12:54:23,577] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1366
[2024-05-14 12:54:23,578] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 43%|████▎     | 1368/3218 [46:28<40:26,  1.31s/it]
[2024-05-14 12:54:24,379] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1367
[2024-05-14 12:54:24,379] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 43%|████▎     | 1369/3218 [46:30<42:11,  1.37s/it]
[2024-05-14 12:54:25,711] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1368
[2024-05-14 12:54:25,712] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:54:25,712] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 12:54:27,559] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1369
[2024-05-14 12:54:27,559] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 43%|████▎     | 1370/3218 [46:32<47:23,  1.54s/it]
{'loss': 0.0, 'learning_rate': 2e-05, 'epoch': 0.43}
[2024-05-14 12:54:29,582] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1370
[2024-05-14 12:54:29,582] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 43%|████▎     | 1372/3218 [46:35<49:35,  1.61s/it]
[2024-05-14 12:54:31,136] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1371
[2024-05-14 12:54:31,136] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:54:31,137] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 12:54:33,300] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1372
[2024-05-14 12:54:33,301] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 43%|████▎     | 1374/3218 [46:39<54:04,  1.76s/it]
[2024-05-14 12:54:35,024] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1373
[2024-05-14 12:54:35,024] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 43%|████▎     | 1375/3218 [46:40<51:28,  1.68s/it]
[2024-05-14 12:54:36,502] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1374
[2024-05-14 12:54:36,503] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 43%|████▎     | 1376/3218 [46:42<48:35,  1.58s/it]
[2024-05-14 12:54:37,857] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1375
[2024-05-14 12:54:37,858] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:54:37,860] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 12:54:39,478] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1376
[2024-05-14 12:54:39,479] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 43%|████▎     | 1378/3218 [46:45<47:11,  1.54s/it]
[2024-05-14 12:54:40,890] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1377
[2024-05-14 12:54:40,890] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 43%|████▎     | 1379/3218 [46:47<53:29,  1.75s/it]
[2024-05-14 12:54:43,057] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1378
[2024-05-14 12:54:43,058] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 43%|████▎     | 1380/3218 [46:48<50:49,  1.66s/it]
[2024-05-14 12:54:44,554] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1379
[2024-05-14 12:54:44,555] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:54:44,561] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25

 43%|████▎     | 1381/3218 [46:50<48:52,  1.60s/it]
[2024-05-14 12:54:46,028] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1380
[2024-05-14 12:54:46,028] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:54:46,028] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 12:54:47,322] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1381
[2024-05-14 12:54:47,323] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 43%|████▎     | 1383/3218 [46:52<43:19,  1.42s/it]
[2024-05-14 12:54:48,533] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1382
[2024-05-14 12:54:48,534] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 43%|████▎     | 1384/3218 [46:54<44:12,  1.45s/it]
[2024-05-14 12:54:50,017] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1383
[2024-05-14 12:54:50,018] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:54:50,018] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 12:54:51,293] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1384
[2024-05-14 12:54:51,294] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 43%|████▎     | 1386/3218 [46:57<41:43,  1.37s/it]
[2024-05-14 12:54:52,594] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1385
[2024-05-14 12:54:52,594] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 43%|████▎     | 1387/3218 [46:58<44:46,  1.47s/it]
[2024-05-14 12:54:54,276] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1386
[2024-05-14 12:54:54,276] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 43%|████▎     | 1389/3218 [47:01<42:26,  1.39s/it]
[2024-05-14 12:54:55,732] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1387
[2024-05-14 12:54:55,733] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:54:55,740] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 12:54:56,960] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1388
[2024-05-14 12:54:56,961] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 43%|████▎     | 1390/3218 [47:02<39:31,  1.30s/it]
[2024-05-14 12:54:58,088] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1389
[2024-05-14 12:54:58,088] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:54:58,089] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
{'loss': 0.0, 'learning_rate': 2e-05, 'epoch': 0.43}
[2024-05-14 12:54:59,320] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1390
[2024-05-14 12:54:59,320] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 43%|████▎     | 1392/3218 [47:05<40:15,  1.32s/it]
[2024-05-14 12:55:00,684] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1391
[2024-05-14 12:55:00,684] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 43%|████▎     | 1393/3218 [47:06<39:04,  1.28s/it]
[2024-05-14 12:55:01,887] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1392
[2024-05-14 12:55:01,887] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:55:01,888] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 12:55:03,279] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1393
[2024-05-14 12:55:03,280] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 43%|████▎     | 1395/3218 [47:08<39:33,  1.30s/it]
[2024-05-14 12:55:04,597] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1394
[2024-05-14 12:55:04,597] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 43%|████▎     | 1396/3218 [47:10<41:44,  1.37s/it]
[2024-05-14 12:55:06,050] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1395
[2024-05-14 12:55:06,051] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:55:06,052] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 12:55:07,226] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1396
[2024-05-14 12:55:07,226] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

 43%|████▎     | 1398/3218 [47:12<38:18,  1.26s/it]
[2024-05-14 12:55:08,406] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1397
[2024-05-14 12:55:08,406] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:55:08,407] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-14 12:55:09,607] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1398
[2024-05-14 12:55:09,607] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
 44%|████▎     | 1400/3218 [47:14<34:15,  1.13s/it][INFO|trainer.py:3166] 2024-05-14 12:55:10,690 >> ***** Running Evaluation *****
[INFO|trainer.py:3168] 2024-05-14 12:55:10,691 >>   Num examples = 521
[INFO|trainer.py:3171] 2024-05-14 12:55:10,691 >>   Batch size = 8
[2024-05-14 12:55:10,520] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1399
[2024-05-14 12:55:10,520] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-14 12:55:10,521] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
{'loss': 0.0, 'learning_rate': 2e-05, 'epoch': 0.44}




