  0%|          | 0/3218 [00:00<?, ?it/s]/mounts/work/lyk/anaconda3/envs/torch2/lib/python3.8/site-packages/deepspeed/runtime/utils.py:274: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/conda/conda-bld/pytorch_1699449229234/work/torch/csrc/tensor/python_tensor.cpp:83.)
  overflow_gpu = get_accelerator().ByteTensor([overflow])
  0%|          | 1/3218 [00:01<1:21:45,  1.52s/it]
[2024-05-13 14:23:47,195] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 0
[2024-05-13 14:23:47,195] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2024-05-13 14:23:47,197] [INFO] [replace_operation.py:192:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0

  0%|          | 2/3218 [00:03<1:31:13,  1.70s/it]
[2024-05-13 14:23:48,903] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1
[2024-05-13 14:23:48,903] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2024-05-13 14:23:48,904] [INFO] [replace_operation.py:192:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
[2024-05-13 14:23:49,848] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 2
[2024-05-13 14:23:49,848] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0

  0%|          | 4/3218 [00:05<1:06:25,  1.24s/it]
[2024-05-13 14:23:50,813] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 3
[2024-05-13 14:23:50,814] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 8192.0 to 4096.0
[2024-05-13 14:23:50,815] [INFO] [replace_operation.py:192:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 8192.0, reducing to 4096.0
[2024-05-13 14:23:52,507] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 4
[2024-05-13 14:23:52,508] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 4096.0 to 2048.0

  0%|          | 5/3218 [00:06<1:14:55,  1.40s/it]
[2024-05-13 14:23:54,400] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 5
[2024-05-13 14:23:54,400] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 2048.0 to 1024.0

  0%|          | 6/3218 [00:08<1:23:58,  1.57s/it]
[2024-05-13 14:23:56,034] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 6
[2024-05-13 14:23:56,035] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 1024.0 to 512.0

  0%|          | 7/3218 [00:10<1:25:11,  1.59s/it]
[2024-05-13 14:23:58,211] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 7
[2024-05-13 14:23:58,212] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 512.0 to 256.0

  0%|          | 9/3218 [00:13<1:27:01,  1.63s/it]
[2024-05-13 14:23:59,514] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 8
[2024-05-13 14:23:59,515] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 256.0 to 128.0

  0%|          | 10/3218 [00:15<1:26:00,  1.61s/it]
[2024-05-13 14:24:01,068] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 9
[2024-05-13 14:24:01,069] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 128.0 to 64.0
[2024-05-13 14:24:01,082] [INFO] [replace_operation.py:192:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 128.0, reducing to 64.0

  0%|          | 11/3218 [00:17<1:32:32,  1.73s/it]
[2024-05-13 14:24:03,074] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 10
[2024-05-13 14:24:03,074] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 64.0 to 32.0
[2024-05-13 14:24:03,076] [INFO] [replace_operation.py:192:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 64.0, reducing to 32.0
[2024-05-13 14:24:04,720] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 11
[2024-05-13 14:24:04,720] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 32.0 to 16.0


  0%|          | 13/3218 [00:21<1:37:09,  1.82s/it]
[2024-05-13 14:24:06,792] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 12
[2024-05-13 14:24:06,792] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 16.0 to 8.0
[2024-05-13 14:24:06,793] [INFO] [replace_operation.py:192:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 16.0, reducing to 8.0
[2024-05-13 14:24:08,587] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 13
[2024-05-13 14:24:08,587] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 8.0 to 4.0

  0%|          | 14/3218 [00:23<1:36:24,  1.81s/it]
[2024-05-13 14:24:10,469] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 14
[2024-05-13 14:24:10,469] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 4.0 to 2.0

  0%|          | 15/3218 [00:24<1:37:30,  1.83s/it]
[2024-05-13 14:24:12,108] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 15
[2024-05-13 14:24:12,109] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 2.0 to 1.0

  1%|          | 17/3218 [00:27<1:29:06,  1.67s/it]
[2024-05-13 14:24:13,541] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 16
[2024-05-13 14:24:13,542] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 1.0 to 0.5

  1%|          | 18/3218 [00:29<1:25:51,  1.61s/it]
[2024-05-13 14:24:15,016] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 17
[2024-05-13 14:24:15,016] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.5 to 0.25
[2024-05-13 14:24:15,016] [INFO] [replace_operation.py:192:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.5, reducing to 0.25
[2024-05-13 14:24:16,571] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 18
[2024-05-13 14:24:16,572] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  1%|          | 20/3218 [00:32<1:20:15,  1.51s/it]
[2024-05-13 14:24:17,879] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 19
[2024-05-13 14:24:17,879] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-13 14:24:17,880] [INFO] [replace_operation.py:192:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25

  1%|          | 21/3218 [00:33<1:21:04,  1.52s/it]
[2024-05-13 14:24:19,443] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 20
[2024-05-13 14:24:19,443] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  1%|          | 22/3218 [00:35<1:18:54,  1.48s/it]
[2024-05-13 14:24:20,834] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 21
[2024-05-13 14:24:20,834] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-13 14:24:20,836] [INFO] [replace_operation.py:192:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-13 14:24:22,155] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 22
[2024-05-13 14:24:22,155] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  1%|          | 24/3218 [00:38<1:17:50,  1.46s/it]
[2024-05-13 14:24:23,677] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 23
[2024-05-13 14:24:23,677] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  1%|          | 25/3218 [00:39<1:20:55,  1.52s/it]
[2024-05-13 14:24:25,354] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 24
[2024-05-13 14:24:25,355] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-13 14:24:25,355] [INFO] [replace_operation.py:192:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-13 14:24:26,677] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 25
[2024-05-13 14:24:26,677] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  1%|          | 26/3218 [00:41<1:17:45,  1.46s/it]
[2024-05-13 14:24:28,026] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 26
[2024-05-13 14:24:28,027] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  1%|          | 28/3218 [00:43<1:13:06,  1.38s/it]
[2024-05-13 14:24:29,254] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 27
[2024-05-13 14:24:29,255] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-13 14:24:29,255] [INFO] [replace_operation.py:192:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-13 14:24:30,431] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 28
[2024-05-13 14:24:30,432] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  1%|          | 30/3218 [00:46<1:09:07,  1.30s/it]
[2024-05-13 14:24:31,719] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 29
[2024-05-13 14:24:31,720] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-13 14:24:31,720] [INFO] [replace_operation.py:192:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25

  1%|          | 31/3218 [00:47<1:09:04,  1.30s/it]
[2024-05-13 14:24:33,009] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 30
[2024-05-13 14:24:33,009] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-13 14:24:33,010] [INFO] [replace_operation.py:192:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-13 14:24:34,055] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 31
[2024-05-13 14:24:34,055] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  1%|          | 33/3218 [00:49<1:01:36,  1.16s/it]
[2024-05-13 14:24:35,069] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 32
[2024-05-13 14:24:35,069] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-13 14:24:35,070] [INFO] [replace_operation.py:192:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-13 14:24:36,163] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 33
[2024-05-13 14:24:36,163] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  1%|          | 35/3218 [00:51<57:46,  1.09s/it]
[2024-05-13 14:24:37,256] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 34
[2024-05-13 14:24:37,257] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  1%|          | 36/3218 [00:53<1:19:05,  1.49s/it]
[2024-05-13 14:24:39,496] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 35
[2024-05-13 14:24:39,497] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  1%|          | 37/3218 [00:56<1:27:45,  1.66s/it]
[2024-05-13 14:24:41,377] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 36
[2024-05-13 14:24:41,378] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-13 14:24:41,378] [INFO] [replace_operation.py:192:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-13 14:24:42,744] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 37
[2024-05-13 14:24:42,745] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  1%|          | 38/3218 [00:56<1:14:50,  1.41s/it]
[2024-05-13 14:24:44,375] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 38
[2024-05-13 14:24:44,376] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  1%|          | 39/3218 [00:59<1:27:01,  1.64s/it]
[2024-05-13 14:24:46,618] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 39
[2024-05-13 14:24:46,618] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  1%|          | 40/3218 [01:01<1:37:11,  1.84s/it]
{'loss': 0.0, 'learning_rate': 2e-05, 'epoch': 0.01}
[2024-05-13 14:24:48,728] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 40
[2024-05-13 14:24:48,728] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25


  1%|▏         | 42/3218 [01:05<1:50:12,  2.08s/it]
[2024-05-13 14:24:51,145] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 41
[2024-05-13 14:24:51,145] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-13 14:24:51,146] [INFO] [replace_operation.py:192:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-13 14:24:54,047] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 42
[2024-05-13 14:24:54,048] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  1%|▏         | 43/3218 [01:08<2:04:19,  2.35s/it]
[2024-05-13 14:24:56,345] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 43
[2024-05-13 14:24:56,346] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  1%|▏         | 44/3218 [01:11<2:01:29,  2.30s/it]
[2024-05-13 14:24:58,181] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 44
[2024-05-13 14:24:58,181] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  1%|▏         | 45/3218 [01:12<1:53:59,  2.16s/it]
[2024-05-13 14:25:00,556] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 45
[2024-05-13 14:25:00,556] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  1%|▏         | 46/3218 [01:15<1:58:26,  2.24s/it]
[2024-05-13 14:25:02,658] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 46
[2024-05-13 14:25:02,658] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  1%|▏         | 47/3218 [01:17<1:56:00,  2.20s/it]
[2024-05-13 14:25:04,533] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 47
[2024-05-13 14:25:04,534] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  1%|▏         | 48/3218 [01:19<1:50:23,  2.09s/it]
[2024-05-13 14:25:06,244] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 48
[2024-05-13 14:25:06,245] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  2%|▏         | 49/3218 [01:20<1:44:19,  1.98s/it]
[2024-05-13 14:25:08,140] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 49
[2024-05-13 14:25:08,140] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-13 14:25:08,140] [INFO] [replace_operation.py:192:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25

  2%|▏         | 50/3218 [01:22<1:42:56,  1.95s/it]
[2024-05-13 14:25:09,946] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 50
[2024-05-13 14:25:09,947] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  2%|▏         | 52/3218 [01:26<1:36:38,  1.83s/it]
[2024-05-13 14:25:11,645] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 51
[2024-05-13 14:25:11,646] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  2%|▏         | 53/3218 [01:28<1:36:55,  1.84s/it]
[2024-05-13 14:25:13,463] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 52
[2024-05-13 14:25:13,463] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  2%|▏         | 54/3218 [01:30<1:38:12,  1.86s/it]
[2024-05-13 14:25:15,367] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 53
[2024-05-13 14:25:15,367] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  2%|▏         | 55/3218 [01:31<1:34:00,  1.78s/it]
[2024-05-13 14:25:16,990] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 54
[2024-05-13 14:25:16,990] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-13 14:25:16,990] [INFO] [replace_operation.py:192:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-13 14:25:18,386] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 55
[2024-05-13 14:25:18,387] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  2%|▏         | 56/3218 [01:33<1:28:05,  1.67s/it]
[2024-05-13 14:25:19,880] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 56
[2024-05-13 14:25:19,881] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  2%|▏         | 58/3218 [01:36<1:23:49,  1.59s/it]
[2024-05-13 14:25:21,403] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 57
[2024-05-13 14:25:21,403] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  2%|▏         | 59/3218 [01:37<1:24:27,  1.60s/it]
[2024-05-13 14:25:23,067] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 58
[2024-05-13 14:25:23,067] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-13 14:25:23,067] [INFO] [replace_operation.py:192:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-13 14:25:24,515] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 59
[2024-05-13 14:25:24,516] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  2%|▏         | 60/3218 [01:39<1:22:01,  1.56s/it]
{'loss': 0.0, 'learning_rate': 2e-05, 'epoch': 0.02}
[2024-05-13 14:25:25,917] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 60
[2024-05-13 14:25:25,918] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  2%|▏         | 62/3218 [01:42<1:20:30,  1.53s/it]
[2024-05-13 14:25:27,500] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 61
[2024-05-13 14:25:27,501] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  2%|▏         | 63/3218 [01:43<1:20:05,  1.52s/it]
[2024-05-13 14:25:29,011] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 62
[2024-05-13 14:25:29,011] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-13 14:25:29,011] [INFO] [replace_operation.py:192:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-13 14:25:30,523] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 63
[2024-05-13 14:25:30,523] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  2%|▏         | 64/3218 [01:45<1:19:35,  1.51s/it]
[2024-05-13 14:25:31,894] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 64
[2024-05-13 14:25:31,894] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  2%|▏         | 66/3218 [01:48<1:18:29,  1.49s/it]
[2024-05-13 14:25:33,436] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 65
[2024-05-13 14:25:33,436] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-13 14:25:33,436] [INFO] [replace_operation.py:192:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-13 14:25:34,746] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 66
[2024-05-13 14:25:34,747] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  2%|▏         | 67/3218 [01:49<1:15:24,  1.44s/it]
[2024-05-13 14:25:36,263] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 67
[2024-05-13 14:25:36,263] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  2%|▏         | 69/3218 [01:52<1:13:25,  1.40s/it]
[2024-05-13 14:25:37,526] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 68
[2024-05-13 14:25:37,527] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-13 14:25:37,527] [INFO] [replace_operation.py:192:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-13 14:25:38,764] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 69
[2024-05-13 14:25:38,764] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  2%|▏         | 70/3218 [01:53<1:08:00,  1.30s/it]
{'loss': 0.0, 'learning_rate': 2e-05, 'epoch': 0.02}
[2024-05-13 14:25:40,799] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 70
[2024-05-13 14:25:40,799] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  2%|▏         | 71/3218 [01:55<1:20:53,  1.54s/it]
[2024-05-13 14:25:42,899] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 71
[2024-05-13 14:25:42,900] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  2%|▏         | 73/3218 [01:58<1:12:57,  1.39s/it]
[2024-05-13 14:25:44,005] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 72
[2024-05-13 14:25:44,006] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  2%|▏         | 74/3218 [01:59<1:17:32,  1.48s/it]
[2024-05-13 14:25:45,166] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 73
[2024-05-13 14:25:45,167] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  2%|▏         | 75/3218 [02:01<1:28:17,  1.69s/it]
[2024-05-13 14:25:47,298] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 74
[2024-05-13 14:25:47,299] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  2%|▏         | 76/3218 [02:04<1:37:35,  1.86s/it]
[2024-05-13 14:25:49,567] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 75
[2024-05-13 14:25:49,568] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  2%|▏         | 77/3218 [02:06<1:38:45,  1.89s/it]
[2024-05-13 14:25:51,536] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 76
[2024-05-13 14:25:51,537] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  2%|▏         | 78/3218 [02:08<1:38:45,  1.89s/it]
[2024-05-13 14:25:53,413] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 77
[2024-05-13 14:25:53,413] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  2%|▏         | 79/3218 [02:10<1:45:07,  2.01s/it]
[2024-05-13 14:25:55,685] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 78
[2024-05-13 14:25:55,686] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  2%|▏         | 80/3218 [02:12<1:42:48,  1.97s/it]
[2024-05-13 14:25:57,584] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 79
[2024-05-13 14:25:57,584] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-13 14:25:57,584] [INFO] [replace_operation.py:192:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25

  3%|▎         | 81/3218 [02:14<1:41:00,  1.93s/it]
[2024-05-13 14:25:59,429] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 80
[2024-05-13 14:25:59,430] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  3%|▎         | 82/3218 [02:16<1:42:25,  1.96s/it]
[2024-05-13 14:26:01,436] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 81
[2024-05-13 14:26:01,437] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  3%|▎         | 83/3218 [02:17<1:41:02,  1.93s/it]
[2024-05-13 14:26:03,303] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 82
[2024-05-13 14:26:03,304] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-13 14:26:03,304] [INFO] [replace_operation.py:192:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-13 14:26:05,020] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 83
[2024-05-13 14:26:05,021] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  3%|▎         | 84/3218 [02:19<1:37:44,  1.87s/it]
[2024-05-13 14:26:06,680] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 84
[2024-05-13 14:26:06,681] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  3%|▎         | 85/3218 [02:21<1:33:50,  1.80s/it]
[2024-05-13 14:26:08,237] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 85
[2024-05-13 14:26:08,238] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  3%|▎         | 86/3218 [02:22<1:29:50,  1.72s/it]
[2024-05-13 14:26:10,083] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 86
[2024-05-13 14:26:10,084] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  3%|▎         | 88/3218 [02:26<1:30:14,  1.73s/it]
[2024-05-13 14:26:11,741] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 87
[2024-05-13 14:26:11,741] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  3%|▎         | 89/3218 [02:27<1:27:54,  1.69s/it]
[2024-05-13 14:26:13,347] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 88
[2024-05-13 14:26:13,348] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-13 14:26:13,350] [INFO] [replace_operation.py:192:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-13 14:26:14,955] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 89
[2024-05-13 14:26:14,956] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  3%|▎         | 90/3218 [02:29<1:26:36,  1.66s/it]
{'loss': 0.0, 'learning_rate': 2e-05, 'epoch': 0.03}
[2024-05-13 14:26:16,488] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 90
[2024-05-13 14:26:16,488] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  3%|▎         | 91/3218 [02:31<1:24:48,  1.63s/it]
[2024-05-13 14:26:18,158] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 91
[2024-05-13 14:26:18,158] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  3%|▎         | 93/3218 [02:34<1:22:32,  1.58s/it]
[2024-05-13 14:26:19,586] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 92
[2024-05-13 14:26:19,586] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-13 14:26:19,587] [INFO] [replace_operation.py:192:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-13 14:26:21,088] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 93
[2024-05-13 14:26:21,089] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  3%|▎         | 94/3218 [02:35<1:20:54,  1.55s/it]
[2024-05-13 14:26:22,612] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 94
[2024-05-13 14:26:22,612] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  3%|▎         | 95/3218 [02:37<1:20:14,  1.54s/it]
[2024-05-13 14:26:24,286] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 95
[2024-05-13 14:26:24,287] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  3%|▎         | 97/3218 [02:40<1:21:35,  1.57s/it]
[2024-05-13 14:26:25,809] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 96
[2024-05-13 14:26:25,810] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  3%|▎         | 98/3218 [02:42<1:21:36,  1.57s/it]
[2024-05-13 14:26:27,400] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 97
[2024-05-13 14:26:27,401] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-13 14:26:27,405] [INFO] [replace_operation.py:192:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-13 14:26:28,781] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 98
[2024-05-13 14:26:28,781] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  3%|▎         | 99/3218 [02:43<1:18:33,  1.51s/it]
[2024-05-13 14:26:30,108] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 99
[2024-05-13 14:26:30,108] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-13 14:26:30,110] [INFO] [replace_operation.py:192:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
  3%|▎         | 100/3218 [02:44<1:15:44,  1.46s/it][INFO|trainer.py:3166] 2024-05-13 14:26:30,636 >> ***** Running Evaluation *****
[INFO|trainer.py:3168] 2024-05-13 14:26:30,636 >>   Num examples = 521
[INFO|trainer.py:3171] 2024-05-13 14:26:30,636 >>   Batch size = 8






 97%|█████████▋| 32/33 [00:11<00:00,  2.71it/s]
{'eval_loss': nan, 'eval_runtime': 12.6023, 'eval_samples_per_second': 41.342, 'eval_steps_per_second': 2.619, 'epoch': 0.03}
[2024-05-13 14:26:44,150] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 100
[2024-05-13 14:26:44,150] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  3%|▎         | 102/3218 [03:00<3:37:07,  4.18s/it]
[2024-05-13 14:26:45,864] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 101
[2024-05-13 14:26:45,864] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  3%|▎         | 103/3218 [03:01<2:51:57,  3.31s/it]
[2024-05-13 14:26:47,169] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 102
[2024-05-13 14:26:47,169] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-13 14:26:47,170] [INFO] [replace_operation.py:192:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-13 14:26:48,857] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 103
[2024-05-13 14:26:48,857] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  3%|▎         | 104/3218 [03:03<2:26:23,  2.82s/it]
[2024-05-13 14:26:50,103] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 104
[2024-05-13 14:26:50,104] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  3%|▎         | 106/3218 [03:06<1:48:15,  2.09s/it]
[2024-05-13 14:26:51,679] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 105
[2024-05-13 14:26:51,680] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  3%|▎         | 107/3218 [03:08<1:51:05,  2.14s/it]
[2024-05-13 14:26:53,144] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 106
[2024-05-13 14:26:53,145] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-13 14:26:53,145] [INFO] [replace_operation.py:192:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-13 14:26:54,878] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 107
[2024-05-13 14:26:54,879] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  3%|▎         | 108/3218 [03:08<1:26:50,  1.68s/it]
[2024-05-13 14:26:56,064] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 108
[2024-05-13 14:26:56,065] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  3%|▎         | 109/3218 [03:10<1:27:40,  1.69s/it]
[2024-05-13 14:26:58,254] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 109
[2024-05-13 14:26:58,254] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-13 14:26:58,256] [INFO] [replace_operation.py:192:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25

  3%|▎         | 110/3218 [03:12<1:36:13,  1.86s/it]
[2024-05-13 14:27:00,522] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 110
[2024-05-13 14:27:00,522] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  3%|▎         | 111/3218 [03:15<1:41:57,  1.97s/it]
[2024-05-13 14:27:02,651] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 111
[2024-05-13 14:27:02,652] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  3%|▎         | 112/3218 [03:17<1:44:26,  2.02s/it]
[2024-05-13 14:27:04,603] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 112
[2024-05-13 14:27:04,604] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  4%|▎         | 113/3218 [03:19<1:43:13,  1.99s/it]
[2024-05-13 14:27:06,568] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 113
[2024-05-13 14:27:06,568] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
