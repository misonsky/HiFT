  0%|          | 0/6436 [00:00<?, ?it/s]/mounts/work/lyk/anaconda3/envs/torch2/lib/python3.8/site-packages/deepspeed/runtime/utils.py:274: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/conda/conda-bld/pytorch_1699449229234/work/torch/csrc/tensor/python_tensor.cpp:83.)
  overflow_gpu = get_accelerator().ByteTensor([overflow])
Traceback (most recent call last):
  File "examples/instruct_tuning.py", line 552, in <module>
    main()
  File "examples/instruct_tuning.py", line 525, in main
    train_result = trainer.train(resume_from_checkpoint=checkpoint)
  File "/mounts/work/lyk/anaconda3/envs/torch2/lib/python3.8/site-packages/transformers/trainer.py", line 1537, in train
    return inner_training_loop(
  File "/nfs/datx/lyk/HiFT/hift/trainer.py", line 1033, in _inner_training_loop
    tr_loss_step = self.training_step(model, inputs)
  File "/nfs/datx/lyk/HiFT/hift/trainer.py", line 722, in training_step
    return super().training_step(model, inputs)
  File "/mounts/work/lyk/anaconda3/envs/torch2/lib/python3.8/site-packages/transformers/trainer.py", line 2744, in training_step
    self.accelerator.backward(loss)
  File "/mounts/work/lyk/anaconda3/envs/torch2/lib/python3.8/site-packages/accelerate/accelerator.py", line 1899, in backward
    self.deepspeed_engine_wrapped.backward(loss, **kwargs)
  File "/mounts/work/lyk/anaconda3/envs/torch2/lib/python3.8/site-packages/accelerate/utils/deepspeed.py", line 176, in backward
    self.engine.step()
  File "/mounts/work/lyk/anaconda3/envs/torch2/lib/python3.8/site-packages/deepspeed/runtime/engine.py", line 2148, in step
    self._take_model_step(lr_kwargs)
  File "/mounts/work/lyk/anaconda3/envs/torch2/lib/python3.8/site-packages/deepspeed/runtime/engine.py", line 2054, in _take_model_step
    self.optimizer.step()
  File "/nfs/datx/lyk/HiFT/hift/optimizers/replace_operation.py", line 252, in step
    self.optimizer.step()
  File "/mounts/work/lyk/anaconda3/envs/torch2/lib/python3.8/site-packages/torch/optim/lr_scheduler.py", line 68, in wrapper
    return wrapped(*args, **kwargs)
  File "/nfs/datx/lyk/HiFT/hift/optimizers/optimizer.py", line 388, in wrapper
    out = func(*args, **kwargs)
  File "/mounts/work/lyk/anaconda3/envs/torch2/lib/python3.8/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/nfs/datx/lyk/HiFT/hift/optimizers/optimizer.py", line 1197, in step
    self.update_step(group, p, gindex, pindex)
  File "/mounts/work/lyk/anaconda3/envs/torch2/lib/python3.8/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/nfs/datx/lyk/HiFT/hift/optimizers/optimizer.py", line 1448, in update_step
    F.optimizer_update_32bit(
  File "/mounts/work/lyk/anaconda3/envs/torch2/lib/python3.8/site-packages/bitsandbytes/functional.py", line 1584, in optimizer_update_32bit
    optim_func = str2optimizer32bit[optimizer_name][0]
KeyError: 'lamb'
[2024-05-14 01:31:09,093] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 0
[2024-05-14 01:31:09,094] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2024-05-14 01:31:09,095] [INFO] [replace_operation.py:232:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0