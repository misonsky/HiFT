  0%|          | 0/3218 [00:00<?, ?it/s]/mounts/work/lyk/anaconda3/envs/torch2/lib/python3.8/site-packages/deepspeed/runtime/utils.py:274: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/conda/conda-bld/pytorch_1699449229234/work/torch/csrc/tensor/python_tensor.cpp:83.)
  overflow_gpu = get_accelerator().ByteTensor([overflow])
[2024-05-13 13:44:53,535] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 0
[2024-05-13 13:44:53,535] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2024-05-13 13:44:53,646] [INFO] [replace_operation.py:192:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
{'loss': 0.0, 'learning_rate': 1e-06, 'epoch': 0.0}
  0%|          | 1/3218 [00:02<2:06:23,  2.36s/it]
[2024-05-13 13:44:55,603] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 1
[2024-05-13 13:44:55,603] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2024-05-13 13:44:55,604] [INFO] [replace_operation.py:192:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
[2024-05-13 13:44:57,010] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 2
[2024-05-13 13:44:57,010] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0

  0%|          | 3/3218 [00:04<1:17:57,  1.45s/it]
[2024-05-13 13:44:57,947] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 3
[2024-05-13 13:44:57,948] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 8192.0 to 4096.0

  0%|          | 4/3218 [00:06<1:20:31,  1.50s/it]
[2024-05-13 13:44:59,983] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 4
[2024-05-13 13:44:59,983] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 4096.0 to 2048.0

  0%|          | 5/3218 [00:08<1:31:55,  1.72s/it]
[2024-05-13 13:45:02,301] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 5
[2024-05-13 13:45:02,301] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 2048.0 to 1024.0

  0%|          | 6/3218 [00:10<1:41:30,  1.90s/it]
[2024-05-13 13:45:04,049] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 6
[2024-05-13 13:45:04,049] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 1024.0 to 512.0

  0%|          | 7/3218 [00:12<1:39:31,  1.86s/it]
[2024-05-13 13:45:06,638] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 7
[2024-05-13 13:45:06,638] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 512.0 to 256.0

  0%|          | 8/3218 [00:15<1:51:17,  2.08s/it]
[2024-05-13 13:45:08,387] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 8
[2024-05-13 13:45:08,387] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 256.0 to 128.0

  0%|          | 9/3218 [00:16<1:46:07,  1.98s/it]
[2024-05-13 13:45:10,405] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 9
[2024-05-13 13:45:10,406] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 128.0 to 64.0

  0%|          | 10/3218 [00:19<1:47:08,  2.00s/it]

  0%|          | 11/3218 [00:21<2:00:40,  2.26s/it]
[2024-05-13 13:45:13,293] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 10
[2024-05-13 13:45:13,293] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 64.0 to 32.0

  0%|          | 12/3218 [00:23<1:55:17,  2.16s/it]
[2024-05-13 13:45:15,212] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 11
[2024-05-13 13:45:15,212] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 32.0 to 16.0

  0%|          | 13/3218 [00:25<1:55:44,  2.17s/it]
[2024-05-13 13:45:17,392] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 12
[2024-05-13 13:45:17,392] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 16.0 to 8.0

  0%|          | 14/3218 [00:28<1:55:02,  2.15s/it]
[2024-05-13 13:45:19,432] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 13
[2024-05-13 13:45:19,432] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 8.0 to 4.0
[2024-05-13 13:45:19,433] [INFO] [replace_operation.py:192:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 8.0, reducing to 4.0
[2024-05-13 13:45:21,691] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 14
[2024-05-13 13:45:21,691] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 4.0 to 2.0

  0%|          | 16/3218 [00:32<1:51:18,  2.09s/it]
[2024-05-13 13:45:23,640] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 15
[2024-05-13 13:45:23,640] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 2.0 to 1.0

  1%|          | 17/3218 [00:34<1:50:09,  2.06s/it]
[2024-05-13 13:45:25,516] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 16
[2024-05-13 13:45:25,517] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 1.0 to 0.5

  1%|          | 18/3218 [00:36<1:48:53,  2.04s/it]
[2024-05-13 13:45:27,506] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 17
[2024-05-13 13:45:27,506] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.5 to 0.25
[2024-05-13 13:45:27,507] [INFO] [replace_operation.py:192:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.5, reducing to 0.25
[2024-05-13 13:45:29,614] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 18
[2024-05-13 13:45:29,614] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  1%|          | 20/3218 [00:40<1:47:20,  2.01s/it]
[2024-05-13 13:45:31,461] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 19
[2024-05-13 13:45:31,462] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-13 13:45:31,462] [INFO] [replace_operation.py:192:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
{'loss': 0.0, 'learning_rate': 1e-06, 'epoch': 0.01}
[2024-05-13 13:45:33,595] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 20
[2024-05-13 13:45:33,595] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  1%|          | 22/3218 [00:44<1:46:44,  2.00s/it]
[2024-05-13 13:45:35,477] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 21
[2024-05-13 13:45:35,477] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  1%|          | 23/3218 [00:46<1:46:11,  1.99s/it]
[2024-05-13 13:45:37,383] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 22
[2024-05-13 13:45:37,383] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  1%|          | 24/3218 [00:48<1:46:15,  2.00s/it]
[2024-05-13 13:45:39,389] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 23
[2024-05-13 13:45:39,389] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-13 13:45:39,389] [INFO] [replace_operation.py:192:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-13 13:45:41,819] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 24
[2024-05-13 13:45:41,820] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  1%|          | 26/3218 [00:52<1:45:55,  1.99s/it]
[2024-05-13 13:45:43,500] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 25
[2024-05-13 13:45:43,500] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  1%|          | 27/3218 [00:53<1:40:07,  1.88s/it]
[2024-05-13 13:45:45,311] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 26
[2024-05-13 13:45:45,312] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
[2024-05-13 13:45:45,312] [INFO] [replace_operation.py:192:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 0.25, reducing to 0.25
[2024-05-13 13:45:46,852] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 27
[2024-05-13 13:45:46,853] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  1%|          | 28/3218 [00:55<1:35:22,  1.79s/it]
[2024-05-13 13:45:48,597] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 28
[2024-05-13 13:45:48,597] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  1%|          | 29/3218 [00:57<1:34:16,  1.77s/it]
[2024-05-13 13:45:50,433] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 29
[2024-05-13 13:45:50,434] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  1%|          | 30/3218 [00:59<1:35:42,  1.80s/it]
{'loss': 0.0, 'learning_rate': 1e-06, 'epoch': 0.01}
[2024-05-13 13:45:52,364] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 30
[2024-05-13 13:45:52,365] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25

  1%|          | 31/3218 [01:01<1:42:05,  1.92s/it]
[2024-05-13 13:45:54,296] [INFO] [unfused_optimizer.py:281:_update_scale] Grad overflow on iteration: 31
[2024-05-13 13:45:54,296] [INFO] [unfused_optimizer.py:282:_update_scale] Reducing dynamic loss scale from 0.25 to 0.25
